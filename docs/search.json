[
  {
    "objectID": "posts/model_fine_tuning.html",
    "href": "posts/model_fine_tuning.html",
    "title": "Model fine tuning",
    "section": "",
    "text": "ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ëŸ°íƒ€ì„ ì¬ì‹œì‘\n\n# pip install datasets==2.20.0 transformers==4.41.2 peft==0.10.0 evaluate==0.4.2 scikit-learn==1.4.2 accelerate -U\n\nì´ì „ chapterì—ì„œëŠ” ì „ì²´ì ìœ¼ë¡œ ë¯¸ì„¸ì¡°ì •ì„ í•˜ì§€ ì•Šê³  ëª¨ë¸ê³¼ ê° íƒœìŠ¤í¬ì˜ ëŒ€ëµì ì¸ êµ¬ì¡°ë§Œ í•™ìŠµí–ˆë‹¤.\nì•„ë¬´ë˜ë„ í•™ìŠµì„ í•˜ì§€ ì•Šê³  ë°”ë¡œ predictë¥¼ í•˜ë‹ˆ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šì•˜ë‹¤.\nì´ë²ˆ chapterì—ì„œëŠ” ë¯¸ì„¸ì¡°ì •ì„ í•´ë³´ëŠ” ì½”ë“œë¥¼ ë°°ì›Œë³¼ ê²ƒì´ë‹¤."
  },
  {
    "objectID": "posts/model_fine_tuning.html#model",
    "href": "posts/model_fine_tuning.html#model",
    "title": "Model fine tuning",
    "section": "2-1 Model",
    "text": "2-1 Model\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n\n\n\n/root/anaconda3/envs/asdf/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
  },
  {
    "objectID": "posts/model_fine_tuning.html#dataset",
    "href": "posts/model_fine_tuning.html#dataset",
    "title": "Model fine tuning",
    "section": "2-2. Dataset",
    "text": "2-2. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"sts\")\n\ndef process_data(batch):\n  result = tokenizer(\n      batch[\"sentence1\"],\n      text_pair=batch[\"sentence2\"],\n      max_length=128,\n      padding=\"max_length\",\n      truncation=True,\n      return_tensors=\"np\",\n  )\n  result[\"labels\"] = [x[\"binary-label\"] for x in batch[\"labels\"]]\n  return result\n\ntokenized_dataset = dataset.map(\n    process_data,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n)\ntokenized_dataset[\"train\"].column_names\n\n\n\n\n\n\n\n['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
  },
  {
    "objectID": "posts/model_fine_tuning.html#train",
    "href": "posts/model_fine_tuning.html#train",
    "title": "Model fine tuning",
    "section": "2-3 Train",
    "text": "2-3 Train\n\nfrom transformers import (\n    Trainer, # ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ í›ˆë ¨ ë„êµ¬\n    TrainingArguments, # í•™ìŠµì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì •ì„ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤\n    default_data_collator, # ì½œë ˆì´í„°\n    EarlyStoppingCallback # early stop í•¨ìˆ˜\n)\nimport evaluate \n\n\ndef custom_metrics(pred): # micro f1 score í‰ê°€ ì§€í‘œë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n  f1 = evaluate.load(\"f1\")\n  labels = pred.label_ids\n  preds = pred.predictions.argmax(-1)\n\n  return f1.compute(predictions=preds, references=labels, average=\"micro\")\n\ntraining_args = TrainingArguments( # í•™ìŠµ argument ì„¤ì •\n    per_device_train_batch_size=64, # í•™ìŠµí•  ë•Œ ë°°ì¹˜ í¬ê¸°\n    per_device_eval_batch_size=64, # í‰ê°€í•  ë•Œ ë°°ì¹˜ í¬ê¸°\n    learning_rate=5e-6,\n    max_grad_norm=1, # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€)\n    num_train_epochs=10, \n    evaluation_strategy=\"steps\", # ì¼ì • stepë§ˆë‹¤ ê²€ì¦ ì‹¤í–‰\n    logging_strategy=\"steps\", # ì¼ì • ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì €ì¥\n    logging_steps=100, # 100 stepë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥\n    logging_dir=\"data/logs\", # ë¡œê·¸ ì €ì¥ ê²½ë¡œ (í˜„ì¬ ì‹¤í–‰ì¤‘ì¸ í´ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ í˜„ì¬ í´ë”ë¥¼ ì˜ í™•ì¸í•´ì•¼í•¨)\n    save_strategy=\"steps\", # ì¼ì • stepë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n    save_steps=100, # 100 stepë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n    output_dir=\"data/ckpt\", # ë¡œê·¸ ì €ì¥ ê²½ë¡œ (í˜„ì¬ ì‹¤í–‰ì¤‘ì¸ í´ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ í˜„ì¬ í´ë”ë¥¼ ì˜ í™•ì¸í•´ì•¼í•¨)\n    load_best_model_at_end = True, # í•™ìŠµ ì¢…ë£Œ í›„ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ë¡œë“œ\n    report_to='tensorboard', # tensorboardì— í•™ìŠµ ë¡œê·¸ ê¸°ë¡\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=default_data_collator,\n    compute_metrics=custom_metrics,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)] # 2ë²ˆ ì—°ì†ìœ¼ë¡œ ê²€ì¦ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨\n)\n\ntrainer.train()\n\n/root/anaconda3/envs/asdf/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n\n\n\n\n    \n      \n      \n      [ 400/1830 04:27 &lt; 16:01, 1.49 it/s, Epoch 2/10]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\nF1\n\n\n\n\n100\n0.045700\n0.930365\n0.786127\n\n\n200\n0.083700\n0.718717\n0.805395\n\n\n300\n0.071400\n0.800480\n0.786127\n\n\n400\n0.060000\n0.828407\n0.799615\n\n\n\n\n\n\n\nTrainOutput(global_step=400, training_loss=0.06518504738807679, metrics={'train_runtime': 268.0555, 'train_samples_per_second': 435.283, 'train_steps_per_second': 6.827, 'total_flos': 1678122311086080.0, 'train_loss': 0.06518504738807679, 'epoch': 2.185792349726776})"
  },
  {
    "objectID": "posts/model_fine_tuning.html#predict",
    "href": "posts/model_fine_tuning.html#predict",
    "title": "Model fine tuning",
    "section": "2-4. Predict",
    "text": "2-4. Predict\në¯¸ì„¸ì¡°ì •ìœ¼ë¡œ 400 stepì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¡œ ì €ì¥í•œ ê²½ë¡œì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì™€ ê²€ì¦ ê²Œì´í„° ì¤‘ 10ê°œ ìƒ˜í”Œì„ ì…ë ¥í•´ ê°„ë‹¨í•œ ì¶”ë¡ ì„ ì§„í–‰í•œë‹¤.\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    BertForSequenceClassification,\n    DataCollatorWithPadding\n)\n\n# tokenizer, model\nmodel_name = \"data/ckpt/checkpoint-400\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name)\n\ncollator = DataCollatorWithPadding(tokenizer) # ì½œë ˆì´í„° ì„¤ì •\nbatch = collator([tokenized_dataset[\"validation\"][i] for i in range(10)])\n\n# inference\nwith torch.no_grad():\n  logits = model(**batch).logits\nlogits\n\ntensor([[-3.8785,  3.4420],\n        [ 3.2443, -2.9451],\n        [ 1.9444, -1.9935],\n        [-3.5267,  3.0033],\n        [ 0.2386, -0.7375],\n        [ 3.3531, -2.7962],\n        [-3.2954,  2.9353],\n        [ 3.8899, -3.2844],\n        [ 4.2035, -3.6925],\n        [ 4.1651, -3.5504]])"
  },
  {
    "objectID": "posts/model_fine_tuning.html#evaluate",
    "href": "posts/model_fine_tuning.html#evaluate",
    "title": "Model fine tuning",
    "section": "2-5. Evaluate",
    "text": "2-5. Evaluate\n\nimport evaluate\n\npred_labels = logits.argmax(dim=1).cpu().numpy()\ntrue_labels = batch[\"labels\"].numpy()\n\nf1 = evaluate.load(\"f1\")\nf1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")\n\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n\n\n{'f1': 1.0}\n\n\nâ€˜klue/bert-baseâ€™ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ AutoModelForSequenceClassification í—¤ë“œë¥¼ ë¶™í˜”ë‹¤.\nê·¸ í›„ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ê³  í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì˜ˆì¸¡ì„ í•˜ì˜€ìœ¼ë¯€ë¡œ ë¯¸ì„¸ì¡°ì •(fine tuning)ì´ë‹¤."
  },
  {
    "objectID": "posts/model_fine_tuning.html#model-1",
    "href": "posts/model_fine_tuning.html#model-1",
    "title": "Model fine tuning",
    "section": "3-1. model",
    "text": "3-1. model\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\nmodel_name = \"skt/kogpt2-base-v2\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    bos_token=\"&lt;/s&gt;\",\n    eos_token=\"&lt;/s&gt;\",\n    unk_token=\"&lt;unk&gt;\",\n    pad_token=\"&lt;pad&gt;\",\n    mask_token=\"&lt;mask&gt;\"\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)"
  },
  {
    "objectID": "posts/model_fine_tuning.html#dataset-1",
    "href": "posts/model_fine_tuning.html#dataset-1",
    "title": "Model fine tuning",
    "section": "3-2. Dataset",
    "text": "3-2. Dataset\n\nfrom datasets import load_dataset\n\n# ë°ì´í„°ì…‹ êµ¬ì¡°í™”\nsplit_dict = {\n    \"train\": \"train[:8000]\",\n    \"test\": \"train[8000:10000]\",\n    \"unused\": \"train[10000:]\",\n}\ndataset = load_dataset(\"heegyu/kowikitext\", split=split_dict)\ndel dataset[\"unused\"] # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” unused ë°ì´í„°ë¥¼ ì‚­ì œ\n\n# í† í°í™”\ntokenized_dataset = dataset.map(\n    lambda batch: tokenizer([f\"{ti}\\n{te}\" for ti, te in zip(batch[\"title\"], batch[\"text\"])]),\n    batched=True, # ë°°ì¹˜ë¡œ ë¬¶ì–´ì„œ ì²˜ë¦¬\n    num_proc=2, # 2ê°œ í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ì‹¤í–‰\n    remove_columns=dataset[\"train\"].column_names, # ê¸°ì¡´ì˜ 'title','text' ì»¬ëŸ¼ì„ ì‚­ì œ í›„ í† í°í™” ê²°ê³¼ë§Œ ë‚¨ê¹€\n)\n\n# ìµœëŒ€ ê¸¸ì´ë¡œ ê·¸ë£¹í™”\nmax_length = 512 \ndef group_texts(batched_sample):\n    sample = {k: v[0] for k, v in batched_sample.items()} # ë°ì´í„°ì…‹ì—ì„œ ê° keyì˜ ì²« ë²ˆì§¸ ê°’ë§Œ ê°€ì ¸ì˜¤ê¸°.\n\n    if sample[\"input_ids\"][-1] != tokenizer.eos_token_id: # ë§ˆì§€ë§‰ í† í°ì´ &lt;eos&gt;ê°€ ì•„ë‹ˆë¼ë©´ &lt;eos&gt; ì¶”ê°€.\n        for k in sample.keys():\n            sample[k].append(\n                tokenizer.eos_token_id if k == \"input_ids\" else sample[k][-1]\n                # sample['input_ids']ë¼ë©´ &lt;eos&gt; í† í°ì„ ì¶”ê°€. / sample['input_ids']ê°€ ì•„ë‹ˆë¼ë©´ ê¸°ì¡´ ê°’ ìœ ì§€.(sample[k][-1])\n            )\n\n    result = {\n        k: [v[i: i + max_length] for i in range(0, len(v), max_length)] # ë¬¸ì¥ì´ ê¸¸ë©´ ì—¬ëŸ¬ ê°œì˜ ìƒ˜í”Œë¡œ ë¶„í• \n        for k, v in sample.items()\n    }\n    return result\n\ngrouped_dataset = tokenized_dataset.map(\n    group_texts,\n    batched=True,\n    batch_size=1,\n    num_proc=2,\n)\ngrouped_dataset[\"train\"].column_names\n\n\n\n\n\n\n\n['input_ids', 'attention_mask']"
  },
  {
    "objectID": "posts/model_fine_tuning.html#fine-tuning",
    "href": "posts/model_fine_tuning.html#fine-tuning",
    "title": "Model fine tuning",
    "section": "3-3. Fine tuning",
    "text": "3-3. Fine tuning\n\nfrom transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=5e-6,\n    max_grad_norm=1,\n    num_train_epochs=3,\n    evaluation_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    logging_dir=\"data/logs\",\n    output_dir=\"data/ckpt\",\n    report_to=\"tensorboard\",\n)\n\ncollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=grouped_dataset[\"train\"],\n    eval_dataset=grouped_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=collator,\n)\n\ntrainer.train()\ntrainer.save_model(\"data/model\")\n\n/root/anaconda3/envs/asdf/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [13776/13776 1:59:37, Epoch 3/3]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n500\n4.284300\n4.471501\n\n\n1000\n4.219000\n3.416660\n\n\n1500\n4.206900\n2.995688\n\n\n2000\n4.185800\n2.903164\n\n\n2500\n4.168300\n2.876508\n\n\n3000\n4.151000\n2.805254\n\n\n3500\n4.151500\n2.801232\n\n\n4000\n4.160500\n2.774476\n\n\n4500\n4.133300\n2.774085\n\n\n5000\n4.085200\n2.745091\n\n\n5500\n4.062300\n2.747127\n\n\n6000\n4.066100\n2.767898\n\n\n6500\n4.037000\n2.726495\n\n\n7000\n4.048100\n2.735216\n\n\n7500\n4.060500\n2.741515\n\n\n8000\n4.031300\n2.703305\n\n\n8500\n4.050100\n2.725095\n\n\n9000\n4.062700\n2.712202\n\n\n9500\n4.020300\n2.688988\n\n\n10000\n3.996800\n2.699451\n\n\n10500\n4.008100\n2.701477\n\n\n11000\n3.989600\n2.698209\n\n\n11500\n3.984000\n2.686813\n\n\n12000\n4.007600\n2.688273\n\n\n12500\n3.992500\n2.678961\n\n\n13000\n3.982600\n2.687267\n\n\n13500\n4.009200\n2.686797"
  },
  {
    "objectID": "posts/model_fine_tuning.html#predict-by-before-fine-tuning-model",
    "href": "posts/model_fine_tuning.html#predict-by-before-fine-tuning-model",
    "title": "Model fine tuning",
    "section": "3-4. Predict by before fine tuning model",
    "text": "3-4. Predict by before fine tuning model\n\nimport torch\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\n\n# ë¯¸ì„¸ì¡°ì • ì´ì „\norigin_name = \"skt/kogpt2-base-v2\"\norigin_tokenizer = AutoTokenizer.from_pretrained(\n    origin_name,\n    bos_token=\"&lt;/s&gt;\",\n    eos_token=\"&lt;/s&gt;\",\n    unk_token=\"&lt;unk&gt;\",\n    pad_token=\"&lt;pad&gt;\",\n    mask_token=\"&lt;mask&gt;\"\n)\norigin_model = GPT2LMHeadModel.from_pretrained(origin_name)\n\ninputs1 = origin_tokenizer(\n    \"ìš°ë¦¬ëŠ” ëˆ„êµ¬ë‚˜ í¬ë§ì„ ê°€ì§€ê³ \",\n    return_tensors=\"pt\"\n).to(origin_model.device)\noutputs1 = origin_model.generate(inputs1.input_ids, max_length=128, repetition_penalty=2.0)\nresult1 = origin_tokenizer.batch_decode(outputs1, skip_special_tokens=True)\nprint(result1[0])\n\n/root/anaconda3/envs/asdf/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nìš°ë¦¬ëŠ” ëˆ„êµ¬ë‚˜ í¬ë§ì„ ê°€ì§€ê³  ì‚´ì•„ê°ˆ ìˆ˜ ìˆëŠ” ì‚¬íšŒë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤\"ê³  ê°•ì¡°í–ˆë‹¤.\nì´ë‚  í–‰ì‚¬ì—ëŠ” ë°•ê·¼í˜œ ëŒ€í†µë ¹, í™©ìš°ì—¬ ìƒˆëˆ„ë¦¬ë‹¹ ëŒ€í‘œ ë“± ì—¬ê¶Œ ì§€ë„ë¶€ì™€ ê¹€ë¬´ì„± ì „ ëŒ€í‘œê°€ ì°¸ì„í•´ ì¶•ì‚¬ë¥¼ í–ˆë‹¤.\nê¹€ì˜ì‚¼ ì •ë¶€ ì‹œì ˆì¸ ì§€ë‚œ 2007ë…„ ëŒ€ì„  ë‹¹ì‹œ ì´ëª…ë°• í›„ë³´ì˜ ë‹¹ì„ ì„ ìœ„í•´ 'êµ­ë¯¼í†µí•©21'ì„ ì´ëŒì—ˆë˜ ì´ í›„ë³´ëŠ” \"ìš°ë¦¬ë‚˜ë¼ì—ì„œ ê°€ì¥ í° ë¬¸ì œëŠ” ê²½ì œ\"ë¼ë©° \"ì´ëª…ë°•ì€ ê²½ì œë¥¼ ì‚´ë¦¬ê³  ì„œë¯¼ì„ ìœ„í•œ ì •ì¹˜ë¥¼ í•˜ê² ë‹¤ê³  ì•½ì†í–ˆì§€ë§Œ í˜„ì‹¤ì€ ê·¸ë ‡ì§€ ëª»í–ˆë‹¤\"ë©° ì´ê°™ì´ ë§í–ˆë‹¤.\nê·¸ëŠ” ì´ì–´ \"ë‚˜ëŠ” ì§€ê¸ˆ ëŒ€í•œë¯¼êµ­ì„ ê±±ì •í•˜ê³  ìˆë‹¤, ê²½ì œê°€ ì–´ë µë‹¤ë©´ ìš°ë¦¬ ëª¨ë‘ í˜ì„ ëª¨ì•„ ìœ„ê¸°ë¥¼ ê·¹ë³µí•´ì•¼ í•œë‹¤ê³  ìƒê°í•œë‹¤"
  },
  {
    "objectID": "posts/model_fine_tuning.html#predict-by-after-fine-tuning-model",
    "href": "posts/model_fine_tuning.html#predict-by-after-fine-tuning-model",
    "title": "Model fine tuning",
    "section": "3-5. Predict by after fine tuning model",
    "text": "3-5. Predict by after fine tuning model\n\n# ë¯¸ì„¸ì¡°ì • ì´í›„\nfinetuned_name = \"data/model\"\nfinetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_name)\nfinetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_name)\n\ninputs2 = finetuned_tokenizer(\n    \"ìš°ë¦¬ëŠ” ëˆ„êµ¬ë‚˜ í¬ë§ì„ ê°€ì§€ê³ \",\n    return_tensors=\"pt\").to(finetuned_model.device)\noutputs2 = finetuned_model.generate(\n    inputs2.input_ids,\n    max_length=128,\n    repetition_penalty=2.0\n)\nresult2 = finetuned_tokenizer.batch_decode(outputs2, skip_special_tokens=True)\nprint(result2[0])\n\nìš°ë¦¬ëŠ” ëˆ„êµ¬ë‚˜ í¬ë§ì„ ê°€ì§€ê³  ì‚´ì•„ê°ˆ ìˆ˜ ìˆëŠ” ì‚¬íšŒë¥¼ ë§Œë“¤ìëŠ” ê²ƒì´ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ í¬ë§ì€ ê²°êµ­ ì¢Œì ˆë˜ê³  ë§ì•˜ë‹¤. ì´ ì ˆë§ì ì¸ ìƒí™© ì†ì—ì„œ, ì‚¬ëŒë“¤ì€ ìì‹ ë“¤ì˜ ì‚¶ì„ í¬ê¸°í•˜ê³  ë‹¤ë¥¸ ì‚¬ëŒë“¤ì˜ ì‚¶ìœ¼ë¡œ ëŒì•„ê°€ê³ ì í•˜ì˜€ë‹¤. ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ê·¸ë“¤ì€ ìì‹ ì˜ ì‚¶ì— ëŒ€í•œ ì±…ì„ì„ íšŒí”¼í•˜ê³  ìê¸° ìì‹ ì„ í¬ìƒí•˜ëŠ” ì„ íƒì„ í•˜ê²Œ ë˜ì—ˆë‹¤. ê·¸ë¦¬í•˜ì—¬ ë§ì€ ì‚¬ëŒë“¤ì´ ìì‚´ì„ ì„ íƒí•˜ê²Œ ë˜ì—ˆê³ , ì´ëŠ” ê³§ ìì‚´ë¡œ ì´ì–´ì§€ê²Œ ë˜ì—ˆë‹¤.\nì´ëŸ¬í•œ ìƒí™©ì— ëŒ€í•´ì„œ, í˜„ëŒ€ ì‚¬íšŒëŠ” ê°œì¸ì˜ ì¡´ì—„ì„±ì„ ì¡´ì¤‘í•˜ì§€ ì•ŠëŠ” ì‚¬íšŒë¼ê³  ë¹„íŒí•˜ì˜€ë‹¤. ë˜í•œ ê°œì¸ë“¤ì´ ìŠ¤ìŠ¤ë¡œ ëª©ìˆ¨ì„ ëŠëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê¸°ë„ í–ˆë‹¤. í•˜ì§€ë§Œ ê·¸ëŸ¬í•œ ê·¹ë‹¨ì ì¸ í–‰ë™ë“¤ì€ ì˜¤íˆë ¤ ì‚¬ëŒë“¤ì„ ì£½ìŒìœ¼ë¡œ ë‚´ëª°ê²Œ ë§Œë“œëŠ” ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•œ ê°œì¸ì´ ì‚¬ë§í•  ê²½ìš° ê°€ì¡±ë“¤ì˜ ë™ì˜ ì—†ì´ ê°•ì œë¡œ ì£½ìŒì„"
  },
  {
    "objectID": "posts/model_fine_tuning.html#model-2",
    "href": "posts/model_fine_tuning.html#model-2",
    "title": "Model fine tuning",
    "section": "4-1. model",
    "text": "4-1. model\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = 'hyunwoongko/kobart'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2."
  },
  {
    "objectID": "posts/model_fine_tuning.html#dataset-2",
    "href": "posts/model_fine_tuning.html#dataset-2",
    "title": "Model fine tuning",
    "section": "4-2. Dataset",
    "text": "4-2. Dataset\në²ˆì—­ ëª¨ë¸ì—ì„œëŠ” text_targetì´ë¼ëŠ” ì¸ìë¥¼ ì‚¬ìš©í•˜ê³  í•œêµ­ì–´ë¥¼ ì…ë ¥í–ˆì„ ë•Œ ì˜ì–´ë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ëª©ì ìœ¼ë¡œ text_target = batch['english']ë¥¼ ì‚¬ìš©í•œë‹¤\n\nfrom datasets import load_dataset\n\ndataset = load_dataset('msarmi9/korean-english-multitarget-ted-talks-task')\ntokenized_dataset = dataset.map(\n    lambda batch: (tokenizer(batch['korean'], text_target = batch['english'], max_length=128, truncation = True)\n                  ),batched = True, batch_size = 1000, num_proc = 2, remove_columns = dataset['train'].column_names)\n\ntokenized_dataset['train'].column_names\n\n\n\n\n\n\n\n\n\n\n['input_ids', 'attention_mask', 'labels']\n\n\ninput_ids : ì…ë ¥ ë¬¸ì¥ì´ í† í°í™” ë˜ì–´ì„œ ì‚¬ì „ì— ë§¤í•‘ëœ ìˆ«ìë¡œ ë³€í™˜ëœë‹¤.\nattention mask: ì§„ì§œ ì…ë ¥ê³¼ íŒ¨ë”©ì„ êµ¬ë³„í•˜ë„ë¡ ë„ì™€ì¤€ë‹¤. input_idsì˜ ê¸¸ì´ê°€ ëª¨ë‘ ê°™ì•„ì•¼ ëª¨ë¸ì´ í•œ êº¼ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆê¸°ì— ë¬¸ì¥ì´ ì§§ì€ ê²½ìš°ì—” ë’¤ì— 0 ë˜ëŠ” [PAD]ë¥¼ ë„£ì–´ì„œ ê¸¸ì´ë¥¼ ë§ì¶˜ë‹¤. ì´ë•Œ, ì–´ë–¤ ë¶€ë¶„ì´ ì‹¤ì œ ë¬¸ì¥ì´ê³  ì–´ë–¤ ë¶€ë¶„ì´ íŒ¨ë”©ì¸ì§€ ì•Œë ¤ì£¼ëŠ” ê²Œ ë°”ë¡œ attention maskì´ë‹¤.\nlabels: ì •ë‹µ í† í° ì‹œí€€ìŠ¤ì´ë‹¤. Seq2Seq ëª¨ë¸ì—ì„œ ëª¨ë¸ì´ ì˜ˆì¸¡í•´ì•¼ í•  ëª©í‘œ ë¬¸ì¥ì„ ì˜ë¯¸í•œë‹¤. labelsë„ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ paddingì„ ë„£ì„ ìˆ˜ ìˆëŠ”ë° padding ë¶€ë¶„ì€ Loss ê³„ì‚°ì—ì„œ ë¬´ì‹œí•´ì•¼í•˜ë¯€ë¡œ ë³´í†µ -100ìœ¼ë¡œ ì±„ìš´ë‹¤.\nlabelsê°€ ìˆë‹¤ëŠ” ê±´ ì •ë‹µì´ ìˆë‹¤ëŠ” ì˜ë¯¸!\në²ˆì—­ Taskì—ì„œ ì •ë‹µì€ ë²ˆì—­í•˜ê³ ì í•˜ëŠ” ì–¸ì–´ë¡œ ì“°ì—¬ì§„ ë¬¸ì¥ì´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì˜ì–´ ë¬¸ì¥ì´ë‹¤.\nì˜ì–´ ë¬¸ì¥ì€ Datasetì— ì´ë¯¸ í¬í•¨ë˜ì–´ìˆê³  í•™ìŠµì„ ìœ„í•´ Loss ê³„ì‚°ì„ í•  ë•Œ ì‚¬ìš©ëœë‹¤."
  },
  {
    "objectID": "posts/model_fine_tuning.html#fine-tuning-1",
    "href": "posts/model_fine_tuning.html#fine-tuning-1",
    "title": "Model fine tuning",
    "section": "4-3. fine tuning",
    "text": "4-3. fine tuning\n\nfrom transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    learning_rate=2e-5,\n    max_grad_norm=1,\n    num_train_epochs=2,\n    evaluation_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    logging_dir=\"data/logs\",\n    save_strategy=\"steps\",\n    save_steps=1000,\n    output_dir=\"data/ckpt\",\n    report_to=\"tensorboard\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n)\n\n\ntrainer.train()\ntrainer.save_model(\"data/model\")\n\n- GPU ì‚¬ìš©í•  ë•Œ í•™ìŠµ ì‹œê°„ì€ 2ì‹œê°„ì´ ì¡°ê¸ˆ ë„˜ê¸°ì— í•™ìŠµì€ ìƒëµí–ˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/model_fine_tuning.html#predict-1",
    "href": "posts/model_fine_tuning.html#predict-1",
    "title": "Model fine tuning",
    "section": "4-4. predict",
    "text": "4-4. predict\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    GenerationConfig\n)\n\nmodel_name = \"data/model\" # ë¯¸ì„¸ì¡°ì •í•œ model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ncollator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"max_length\",\n    max_length=512,\n)\nbatch = collator([tokenized_dataset[\"test\"][i] for i in range(2)])\n\noutputs = model.generate(batch[\"input_ids\"], max_length=128, do_sample=False)\nresult = tokenizer.batch_decode(outputs, skip_special_tokens=True)\norigin = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\nprint(f\"ì›ë³¸ : {origin[0]} -&gt; ì˜ì–´ : {result[0]}\")\nprint(f\"ì›ë³¸ : {origin[1]} -&gt; ì˜ì–´ : {result[1]}\")"
  },
  {
    "objectID": "posts/Text_Classification.html",
    "href": "posts/Text_Classification.html",
    "title": "Text Classification Fine Tuning",
    "section": "",
    "text": "- colabì—ì„œ ì‹¤ìŠµí•˜ê¸¸ ë°”ëë‹ˆë‹¤.\n# !git clone https://github.com/rickiepark/nlp-with-transformers.git\n# %cd nlp-with-transformers\n# from install import *\n# install_requirements(chapter=2)"
  },
  {
    "objectID": "posts/Text_Classification.html#data-loading-emotion-encoding",
    "href": "posts/Text_Classification.html#data-loading-emotion-encoding",
    "title": "Text Classification Fine Tuning",
    "section": "1. Data loading & Emotion encoding",
    "text": "1. Data loading & Emotion encoding\n\n# í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê¸°\nfrom huggingface_hub import list_datasets\nfrom datasets import load_dataset\nfrom datasets import ClassLabel\n\nemotions = load_dataset(\"emotion\")\n\nfrom transformers import AutoTokenizer\nemotions['train'].features['label'] = ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])\nmodel_ckpt = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n\nemotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n\n- ì½”ë“œ ì„¤ëª… 1. emotion ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤. 2. emotion ë°ì´í„°ì—ì„œ trainì— ìˆëŠ” ë ˆì´ë¸”ì„ 6ê°œì˜ ê°ì •ìœ¼ë¡œ í• ë‹¹í•´ì¤€ë‹¤. 3. modelì„ ì„¤ì •í•˜ê³  tokenizerë„ ëª¨ë¸ì— ë§ê²Œ ë¶ˆëŸ¬ì˜¨ë‹¤. 4. tokenize í•¨ìˆ˜ë¥¼ ì„ ì–¸í•˜ê³  ë¬¸ì¥ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ paddingê³¼ truncationì„ Trueë¡œ ì„¤ì •í•œë‹¤. 5. emotionì„ í† í¬ë‚˜ì´ì§• í•œë‹¤."
  },
  {
    "objectID": "posts/Text_Classification.html#text-tokenizing",
    "href": "posts/Text_Classification.html#text-tokenizing",
    "title": "Text Classification Fine Tuning",
    "section": "Text tokenizing",
    "text": "Text tokenizing\n\nfrom transformers import AutoModel\nimport torch\nfrom sklearn.metrics import accuracy_score, f1_score\n\ntext = \"this is a test\"\ninputs = tokenizer(text, return_tensors=\"pt\")\ninputs['input_ids'].size()\n\n- ì½”ë“œ ì„¤ëª… 1. ì„ì˜ì˜ í…ŒìŠ¤íŠ¸ textë¥¼ ìƒì„± í›„ í† í¬ë‚˜ì´ì§•ì„ í•´ì¤€ë‹¤. 2. tokenizerê°€ ë°˜í™˜í•˜ëŠ” ë°ì´í„°ë¥¼ PyTorch í…ì„œ(torch.Tensor) í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ì„œ return_tensors=â€œptâ€ë¥¼ ì„¤ì •í•œë‹¤."
  },
  {
    "objectID": "posts/Text_Classification.html#hf-login",
    "href": "posts/Text_Classification.html#hf-login",
    "title": "Text Classification Fine Tuning",
    "section": "3. HF login",
    "text": "3. HF login\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
    "objectID": "posts/Text_Classification.html#model",
    "href": "posts/Text_Classification.html#model",
    "title": "Text Classification Fine Tuning",
    "section": "4. model",
    "text": "4. model\n\nfrom transformers import AutoModelForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_labels = 6\nmodel_ckpt = \"distilbert-base-uncased\"\n\n# distilbert-base-uncasedê°€ ë°”ë””ì´ê³  AutoModelForSequenceClassificationê°€ í—¤ë“œì´ë‹¤.\n# num_labelì´ 6ì´ë¯€ë¡œ 6ê°œì˜ ê°ì • í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” í—¤ë“œ í•˜ë‚˜ê°€ ì¶”ê°€ëœ ê²ƒì´ë‹¤.\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, num_labels=num_labels)\n         .to(device))\n\n- ì½”ë“œ ì„¤ëª… 1. GPUë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ device ì„¤ì •. 2. labelì˜ ê°œìˆ˜ëŠ” ìœ„ì—ì„œ í• ë‹¹í•œ ëŒ€ë¡œ 6ê°œì´ê³  modelë„ ì„ ì–¸í•´ì¤€ë‹¤. 3. ì—¬ê¸°ì„œ distilbert-base-uncasedì€ ë°”ë””ì´ê³  AutoModelForSequenceClassificationì€ í—¤ë“œì´ë‹¤. ì‚¬ì „í•™ìŠµëœ bertëª¨ë¸ì— ê°ì • í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•´ì„œ í—¤ë“œë¥¼ ì¶”ê°€í–ˆë‹¤."
  },
  {
    "objectID": "posts/Text_Classification.html#learning",
    "href": "posts/Text_Classification.html#learning",
    "title": "Text Classification Fine Tuning",
    "section": "5. Learning",
    "text": "5. Learning\n\nfrom transformers import Trainer, TrainingArguments\n\nbatch_size = 64\nlogging_steps = len(emotions_encoded[\"train\"]) // batch_size\nmodel_name = f\"{model_ckpt}-finetuned-emotion\"\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=2,\n                                  learning_rate=2e-5,\n                                  per_device_train_batch_size=batch_size,\n                                  per_device_eval_batch_size=batch_size,\n                                  weight_decay=0.01,\n                                  evaluation_strategy=\"epoch\",\n                                  disable_tqdm=False,\n                                  logging_steps=logging_steps,\n                                  push_to_hub=True,\n                                  save_strategy=\"epoch\",\n                                  load_best_model_at_end=True,\n                                  log_level=\"error\",\n                                  report_to=\"none\")\n\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n\ntrainer = Trainer(model=model, args=training_args,\n                  compute_metrics=compute_metrics,\n                  train_dataset=emotions_encoded[\"train\"],\n                  eval_dataset=emotions_encoded[\"validation\"],\n                  tokenizer=tokenizer)\ntrainer.train()\n\n- ì½”ë“œ ì„¤ëª… 1. training argumentë¥¼ ì„¤ì •í•´ì¤€ë‹¤. 2. í•™ìŠµì„ í•˜ê³  ê²°ê³¼ë¥¼ ë³´ë‹ˆ Loss, Accuracy, F1 ë“¤ì´ ì „ë¶€ í–¥ìƒëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰ Fine tuningì´ ì˜ ì´ë£¨ì–´ ì¡Œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "posts/Text_Classification.html#prediction",
    "href": "posts/Text_Classification.html#prediction",
    "title": "Text Classification Fine Tuning",
    "section": "6. Prediction",
    "text": "6. Prediction\n\noutput = trainer.predict(emotions_encoded[\"validation\"])\noutput.metrics\n\n\nimport numpy as np\nyy = np.argmax(output.predictions,axis=1)\nyy"
  },
  {
    "objectID": "posts/Text_Classification.html#error-analyze",
    "href": "posts/Text_Classification.html#error-analyze",
    "title": "Text Classification Fine Tuning",
    "section": "7. Error analyze",
    "text": "7. Error analyze\n\nfrom torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    # ëª¨ë“  ì…ë ¥ í…ì„œë¥¼ ëª¨ë¸ê³¼ ê°™ì€ ì¥ì¹˜ë¡œ ì´ë™í•©ë‹ˆë‹¤.\n    inputs = {k:v.to(device) for k,v in batch.items()\n              if k in tokenizer.model_input_names}\n\n    with torch.no_grad(): # ì—­ì „íŒŒë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í‰ê°€ ë‹¨ê³„ì´ë¯€ë¡œ)\n        output = model(**inputs) # ì…ë ¥ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì „ë‹¬\n        pred_label = torch.argmax(output.logits, axis=-1) # ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ í´ë˜ìŠ¤ ì„ íƒ\n        loss = cross_entropy(output.logits, batch[\"label\"].to(device), # loss ê³„ì‚°\n                             reduction=\"none\") # í‰ê· ì„ ë‚´ì§€ ì•Šê³  ê°œë³„ ìƒ˜í”Œì˜ ì†ì‹¤ì„ ë°˜í™˜\n\n    return {\"loss\": loss.cpu().numpy(), # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™ ë° numpy ë°°ì—´ë¡œ ë³€í™˜ # PyTorch í…ì„œëŠ” datasetì—ì„œ ë‹¤ë£¨ê¸° ì–´ë µë‹¤.\n            \"predicted_label\": pred_label.cpu().numpy()}\n\n\n# ë°ì´í„°ì…‹ì„ ë‹¤ì‹œ íŒŒì´í† ì¹˜ í…ì„œë¡œ ë³€í™˜\nemotions_encoded.set_format(\"torch\",\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n# ì†ì‹¤ ê°’ì„ ê³„ì‚°\nemotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n    forward_pass_with_label, batched=True, batch_size=16)\n\n- ì½”ë“œ ì„¤ëª… 1. ëª¨ë“  ì…ë ¥ í…ì„œê°€ ëª¨ë¸ê³¼ ê°™ì•„ì•¼ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ê¸°ì— ê°™ì€ ì¥ì¹˜ë¡œ ì´ë™ 2. ì…ë ¥ ë°ì´í„°ë¥¼ **inputsìœ¼ë¡œ ëª¨ë¸ì— ì „ë‹¬ í›„ ê°€ì¥ ë†’ì€ logitsê°’ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•œë‹¤. 3. ì´ì œ lossë¥¼ ê³„ì‚°í•˜ê³  í‰ê· ì„ ë‚´ì§€ ì•ŠëŠ” ì´ìœ ëŠ” labelë§ˆë‹¤ lossê°’ì˜ í¸ì°¨ê°€ ìˆëŠ” ê²ƒì„ í™•ì¸í•˜ê¸° ìœ„í•´ í‰ê· ì„ ë‚´ì§€ ì•ŠëŠ”ë‹¤. 4. ê²°ê³¼ë¥¼ numpyë¡œ ë³€í™˜. (datasets.map() í•¨ìˆ˜ëŠ” PyTorch í…ì„œ ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ë‚˜ NumPy ë°°ì—´ì„ ë°˜í™˜í•´ì•¼ í•¨.) 5. ì†ì‹¤ê°’ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ PyTorch í…ì„œë¡œ ì „í™˜í•œë‹¤. (batchí˜•íƒœë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ)"
  },
  {
    "objectID": "posts/Text_Classification.html#int---str-ë³€í™˜",
    "href": "posts/Text_Classification.html#int---str-ë³€í™˜",
    "title": "Text Classification Fine Tuning",
    "section": "8. int -> str ë³€í™˜",
    "text": "8. int -&gt; str ë³€í™˜\n\ndef label_int2str(row):\n    return emotions[\"train\"].features[\"label\"].int2str(row)\n\nemotions_encoded.set_format(\"pandas\")\ncols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\ndf_test = emotions_encoded[\"validation\"][:][cols]\ndf_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\ndf_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n                              .apply(label_int2str))\n\n\ndf_test.sort_values(\"loss\", ascending=False).head(10)\n\n\ndf_test.sort_values(\"loss\", ascending=True).head(10)\n\n- ì½”ë“œ ì„¤ëª… 1. labelì— ìˆëŠ” intí˜• ê°’ë“¤ì„ ì‚¬ëŒì´ ì•Œì•„ë³´ê¸° ì‰½ê²Œ strí˜•íƒœë¡œ ë°”ê¿”ì¤€ë‹¤. 2. ê²°ê³¼ë¥¼ ì‚´í´ë³´ë©´ sadness ë ˆì´ë¸”ë“¤ì€ lossë„ ì ê³  ì˜ ë§ì¶”ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "posts/Text_Classification.html#save-model-publish",
    "href": "posts/Text_Classification.html#save-model-publish",
    "title": "Text Classification Fine Tuning",
    "section": "9. Save model & Publish",
    "text": "9. Save model & Publish\n\ntrainer.push_to_hub(commit_message=\"Training completed!\")\n\n\nfrom transformers import pipeline\n\nmodel_id = \"SangJinCha/distilbert-base-uncased-finetuned-emotion\"\nclassifier = pipeline(\"text-classification\", model=model_id)\n\nì´ì œ ëª¨ë¸ì— hugging face ì‚¬ìš©ì ì´ë¦„ì„ ë¶™í˜€ì„œ push í•´ì£¼ë©´ ëœë‹¤."
  },
  {
    "objectID": "posts/Based_on_Decoder.html",
    "href": "posts/Based_on_Decoder.html",
    "title": "Base on Decoder models",
    "section": "",
    "text": "ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ ë¬¸ì¥ ì•ë¶€ë¶„ ì¼ë¶€ë§Œì„ ì…ë ¥ë°›ì•„ ì´ë¥¼ ì´ì–´ì„œ ì‘ì„±í•˜ëŠ” í˜•íƒœì´ë©° ì´ë¥¼ ìì—°ì–´ ìƒì„±ì´ë¼ê³ ë„ ë§í•œë‹¤.\n\n\në””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ ì…€í”„ ì–´í…ì…˜, ì¸ì½”ë” ì°¸ì¡° ì–´í…ì…˜, FFNNìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê¸°ì¡´ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”ì—ì„œ ì¸ì½”ë” ì°¸ì¡° ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ë‘ ë‹¨ê³„ë¡œ êµ¬ì„±ëœë‹¤.\nì´ë¥¼ ì—¬ëŸ¬ ê°œ ë ˆì´ì–´ë¡œ ì¸µì¸µì´ ìŒ“ì€ í˜•íƒœì˜ ëª¨ë¸ì´ ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì´ë‹¤.\nì¸ì½”ë” ê¸°ë°˜ ëª¨ë¸ì˜ ì£¼ìš” taskëŠ” ì™„ì„±ëœ ë¬¸ì¥ì„ ë¶„ì„í•˜ëŠ” ê²ƒì´ê¸°ì— ì´ë¯¸ ì™„ì„±ëœ ë¬¸ì¥ì´ inputìœ¼ë¡œ ì…ë ¥ëœë‹¤.\ní•˜ì§€ë§Œ ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì˜ ì£¼ìš” taskëŠ” ë¯¸ì™„ì„±ì˜ ë¬¸ì¥ì„ ì´ì–´ì„œ ì‘ì„±í•˜ëŠ” ìƒì„±íƒœìŠ¤í¬ì´ë‹¤.\në¯¸ì™„ì„± ìƒíƒœë¡œ ì‘ì„±ì¤‘ì¸ ë¬¸ì¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•˜ë©° ì§ì ‘ ì´ì–´ ë‚˜ê°€ì•¼ í•˜ê¸°ì— ë¬¸ì¥ ì¼ë¶€ë§Œìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¨ë°©í–¥ í˜•íƒœë¡œ ë¶„ì„í•œë‹¤.\n- ë‹¨ë°©í–¥ í˜•íƒœ?\në””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ ë’¤ìª½ ë‹¨ì–´ê°€ ì•ìª½ ë‹¨ì–´ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ì—†ë‹¤â€¦ë” ê¹Šì€ ì´í•´ë¥¼ ìœ„í•´ BERTì™€ GPTë¥¼ ë¹„êµí•´ë³´ì.\n\n\n\n\n\nBERTëŠ” Transformerì˜ ì¸ì½”ë” ë¸”ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì´ë‹¤.\ní•µì‹¬ íŠ¹ì§•ì€ ì–‘ë°©í–¥(Bidirectional)ìœ¼ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•œë‹¤ëŠ” ì ì´ë‹¤. ì¦‰, ëª¨ë“  ë‹¨ì–´ê°€ ì•ìª½ + ë’¤ìª½ì˜ ë‹¨ì–´ë¥¼ ë™ì‹œì— ì°¸ê³ í•  ìˆ˜ ìˆìŒ.\nì˜ˆë¥¼ ë“¤ì–´, ë¬¸ì¥ì´ \"The cat sat on the mat\"ë¼ë©´, \"cat\"ì€ \"The\"ë„ ë³´ê³  \"sat\"ë„ ë³´ë©´ì„œ ë¬¸ë§¥ì„ ì´í•´í•  ìˆ˜ ìˆìŒ(???). ì¦‰, ì²« ë²ˆì§¸ ë‹¨ì–´ë„ ë§ˆì§€ë§‰ ë‹¨ì–´ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒ\nBERTëŠ” ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ í•œ ë²ˆì— ì…ë ¥ë°›ì•„ ì „ì²´ ë¬¸ë§¥ì„ í•™ìŠµí•˜ê¸°ì— ì²« ë²ˆì§¸ ë‹¨ì–´ì˜ ë²¡í„°ê°€ ë§ˆì§€ë§‰ ë‹¨ì–´ ë²¡í„°ì—ê¹Œì§€ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤.\nì¦‰, BERTëŠ” ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ê°€ ì„œë¡œ ì˜í–¥ì„ ì£¼ê³ ë°›ì„ ìˆ˜ ìˆëŠ” êµ¬ì¡°ë‹¤.\n- \"cat\"ì€ \"The\"ë„ ë³´ê³  \"sat\"ë„ ë³´ë©´ì„œ ë¬¸ë§¥ì„ ì´í•´í•  ìˆ˜ ìˆìŒ (???)\nQ. catì´ \"The\"ë„ ë³´ê³  \"sat\"ë„ ë³´ë©´ì„œ ë¬¸ë§¥ì„ ì´í•´í•œë‹¤ê³ ? ê·¸ëƒ¥ í•˜ë‚˜ì˜ ë‹¨ì–´ ì•„ë‹Œê°€? A. catë§Œ ë³¸ë‹¤ë©´ ë‹¹ì—°íˆ ê³ ì–‘ì´ë¼ê³  ìƒê°í•˜ì§€ë§Œ ì–´ë–¤ ë¬¸ì¥ì—ì„œëŠ” ë‹¤ë¥¸ ì˜ë¯¸ í˜¹ì€ ì´ë¦„ë“± í•¨ì¶•í•˜ê³  ìˆëŠ” ë‚´ìš©ì€ ê³ ì–‘ì´ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ self-attention ë©”ì»¤ë‹ˆì¦˜ì„ ì ìš©í•˜ë©´ sat, the, on ë“±ì„ ë³´ë©´ì„œ catì˜ ë‚´í¬ ì˜ë¯¸ë¥¼ ë” í™•ì‹¤í•˜ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ catì´ ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ ë³´ë©° ë¬¸ë§¥ì„ ì´í•´í•œë‹¤ê³  í‘œí˜„í•œ ê²ƒì´ë‹¤.\n\n\n\nGPTëŠ” Transformerì˜ ë””ì½”ë”(Decoder) ë¸”ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì´ë‹¤.\ní•µì‹¬ íŠ¹ì§•ì€ ë‹¨ë°©í–¥(Unidirectional)ìœ¼ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•œë‹¤ëŠ” ì ì´ë‹¤. Self-Attentionì„ ë‹¨ë°©í–¥ìœ¼ë¡œ ìˆ˜í–‰í•˜ë¯€ë¡œ í˜„ì¬ ë‹¨ì–´ë³´ë‹¤ ì•ìª½(ì´ì „)ì— ìˆëŠ” ë‹¨ì–´ë“¤ë§Œ ì°¸ê³ í•  ìˆ˜ ìˆë‹¤.\nì˜ˆë¥¼ ë“¤ì–´, \"The cat sat on the mat\"ì—ì„œ \"mat\"ì„ ì˜ˆì¸¡í•  ë•Œ \"The cat sat on the\"ê¹Œì§€ë§Œ ë³´ê³  \"mat\"ì„ ê²°ì •í•´ì•¼ í•¨.\në°˜ëŒ€ë¡œ \"mat\"ì´ \"sat\"ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ëŠ” ì—†ìŒ. ì¦‰, ë¯¸ë˜ ì •ë³´ëŠ” ë³¼ ìˆ˜ ì—†ìŒ\nGPTëŠ” ìƒì„± ëª¨ë¸ì´ê¸° ë•Œë¬¸ì—, ë¬¸ì¥ì„ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ë‹¨ì–´ì”© ìƒì„±í•´ì•¼ í•œë‹¤. ë§Œì•½ ì˜¤ë¥¸ìª½(ë’¤ìª½)ì˜ ë‹¨ì–´ë¥¼ ì•ˆë‹¤ë©´ ì´ë¯¸ ì •ë‹µì„ ì•Œê³  ìˆê¸°ì— cheatingìœ¼ë¡œ ê°„ì£¼ëœë‹¤.\në”°ë¼ì„œ ì²« ë²ˆì§¸ ë‹¨ì–´ëŠ” ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ì „í˜€ ëª¨ë¦„.\nì¦‰, GPTëŠ” ì•ìª½ ë‹¨ì–´ë“¤ì´ ë’¤ìª½ ë‹¨ì–´ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆì§€ë§Œ, ë°˜ëŒ€ë¡œ ë’¤ìª½ ë‹¨ì–´ê°€ ì•ìª½ ë‹¨ì–´ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ì—†ìŒ!"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#ê¸°ë³¸-êµ¬ì¡°",
    "href": "posts/Based_on_Decoder.html#ê¸°ë³¸-êµ¬ì¡°",
    "title": "Base on Decoder models",
    "section": "",
    "text": "ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ ì…€í”„ ì–´í…ì…˜, ì¸ì½”ë” ì°¸ì¡° ì–´í…ì…˜, FFNNìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê¸°ì¡´ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”ì—ì„œ ì¸ì½”ë” ì°¸ì¡° ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ë‘ ë‹¨ê³„ë¡œ êµ¬ì„±ëœë‹¤.\nì´ë¥¼ ì—¬ëŸ¬ ê°œ ë ˆì´ì–´ë¡œ ì¸µì¸µì´ ìŒ“ì€ í˜•íƒœì˜ ëª¨ë¸ì´ ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì´ë‹¤.\nì¸ì½”ë” ê¸°ë°˜ ëª¨ë¸ì˜ ì£¼ìš” taskëŠ” ì™„ì„±ëœ ë¬¸ì¥ì„ ë¶„ì„í•˜ëŠ” ê²ƒì´ê¸°ì— ì´ë¯¸ ì™„ì„±ëœ ë¬¸ì¥ì´ inputìœ¼ë¡œ ì…ë ¥ëœë‹¤.\ní•˜ì§€ë§Œ ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì˜ ì£¼ìš” taskëŠ” ë¯¸ì™„ì„±ì˜ ë¬¸ì¥ì„ ì´ì–´ì„œ ì‘ì„±í•˜ëŠ” ìƒì„±íƒœìŠ¤í¬ì´ë‹¤.\në¯¸ì™„ì„± ìƒíƒœë¡œ ì‘ì„±ì¤‘ì¸ ë¬¸ì¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•˜ë©° ì§ì ‘ ì´ì–´ ë‚˜ê°€ì•¼ í•˜ê¸°ì— ë¬¸ì¥ ì¼ë¶€ë§Œìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¨ë°©í–¥ í˜•íƒœë¡œ ë¶„ì„í•œë‹¤.\n- ë‹¨ë°©í–¥ í˜•íƒœ?\në””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ ë’¤ìª½ ë‹¨ì–´ê°€ ì•ìª½ ë‹¨ì–´ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ì—†ë‹¤â€¦ë” ê¹Šì€ ì´í•´ë¥¼ ìœ„í•´ BERTì™€ GPTë¥¼ ë¹„êµí•´ë³´ì."
  },
  {
    "objectID": "posts/Based_on_Decoder.html#bert-vs-gpt",
    "href": "posts/Based_on_Decoder.html#bert-vs-gpt",
    "title": "Base on Decoder models",
    "section": "",
    "text": "BERTëŠ” Transformerì˜ ì¸ì½”ë” ë¸”ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì´ë‹¤.\ní•µì‹¬ íŠ¹ì§•ì€ ì–‘ë°©í–¥(Bidirectional)ìœ¼ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•œë‹¤ëŠ” ì ì´ë‹¤. ì¦‰, ëª¨ë“  ë‹¨ì–´ê°€ ì•ìª½ + ë’¤ìª½ì˜ ë‹¨ì–´ë¥¼ ë™ì‹œì— ì°¸ê³ í•  ìˆ˜ ìˆìŒ.\nì˜ˆë¥¼ ë“¤ì–´, ë¬¸ì¥ì´ \"The cat sat on the mat\"ë¼ë©´, \"cat\"ì€ \"The\"ë„ ë³´ê³  \"sat\"ë„ ë³´ë©´ì„œ ë¬¸ë§¥ì„ ì´í•´í•  ìˆ˜ ìˆìŒ(???). ì¦‰, ì²« ë²ˆì§¸ ë‹¨ì–´ë„ ë§ˆì§€ë§‰ ë‹¨ì–´ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒ\nBERTëŠ” ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ í•œ ë²ˆì— ì…ë ¥ë°›ì•„ ì „ì²´ ë¬¸ë§¥ì„ í•™ìŠµí•˜ê¸°ì— ì²« ë²ˆì§¸ ë‹¨ì–´ì˜ ë²¡í„°ê°€ ë§ˆì§€ë§‰ ë‹¨ì–´ ë²¡í„°ì—ê¹Œì§€ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤.\nì¦‰, BERTëŠ” ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ê°€ ì„œë¡œ ì˜í–¥ì„ ì£¼ê³ ë°›ì„ ìˆ˜ ìˆëŠ” êµ¬ì¡°ë‹¤.\n- \"cat\"ì€ \"The\"ë„ ë³´ê³  \"sat\"ë„ ë³´ë©´ì„œ ë¬¸ë§¥ì„ ì´í•´í•  ìˆ˜ ìˆìŒ (???)\nQ. catì´ \"The\"ë„ ë³´ê³  \"sat\"ë„ ë³´ë©´ì„œ ë¬¸ë§¥ì„ ì´í•´í•œë‹¤ê³ ? ê·¸ëƒ¥ í•˜ë‚˜ì˜ ë‹¨ì–´ ì•„ë‹Œê°€? A. catë§Œ ë³¸ë‹¤ë©´ ë‹¹ì—°íˆ ê³ ì–‘ì´ë¼ê³  ìƒê°í•˜ì§€ë§Œ ì–´ë–¤ ë¬¸ì¥ì—ì„œëŠ” ë‹¤ë¥¸ ì˜ë¯¸ í˜¹ì€ ì´ë¦„ë“± í•¨ì¶•í•˜ê³  ìˆëŠ” ë‚´ìš©ì€ ê³ ì–‘ì´ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ self-attention ë©”ì»¤ë‹ˆì¦˜ì„ ì ìš©í•˜ë©´ sat, the, on ë“±ì„ ë³´ë©´ì„œ catì˜ ë‚´í¬ ì˜ë¯¸ë¥¼ ë” í™•ì‹¤í•˜ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ catì´ ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ ë³´ë©° ë¬¸ë§¥ì„ ì´í•´í•œë‹¤ê³  í‘œí˜„í•œ ê²ƒì´ë‹¤.\n\n\n\nGPTëŠ” Transformerì˜ ë””ì½”ë”(Decoder) ë¸”ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì´ë‹¤.\ní•µì‹¬ íŠ¹ì§•ì€ ë‹¨ë°©í–¥(Unidirectional)ìœ¼ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•œë‹¤ëŠ” ì ì´ë‹¤. Self-Attentionì„ ë‹¨ë°©í–¥ìœ¼ë¡œ ìˆ˜í–‰í•˜ë¯€ë¡œ í˜„ì¬ ë‹¨ì–´ë³´ë‹¤ ì•ìª½(ì´ì „)ì— ìˆëŠ” ë‹¨ì–´ë“¤ë§Œ ì°¸ê³ í•  ìˆ˜ ìˆë‹¤.\nì˜ˆë¥¼ ë“¤ì–´, \"The cat sat on the mat\"ì—ì„œ \"mat\"ì„ ì˜ˆì¸¡í•  ë•Œ \"The cat sat on the\"ê¹Œì§€ë§Œ ë³´ê³  \"mat\"ì„ ê²°ì •í•´ì•¼ í•¨.\në°˜ëŒ€ë¡œ \"mat\"ì´ \"sat\"ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ëŠ” ì—†ìŒ. ì¦‰, ë¯¸ë˜ ì •ë³´ëŠ” ë³¼ ìˆ˜ ì—†ìŒ\nGPTëŠ” ìƒì„± ëª¨ë¸ì´ê¸° ë•Œë¬¸ì—, ë¬¸ì¥ì„ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ë‹¨ì–´ì”© ìƒì„±í•´ì•¼ í•œë‹¤. ë§Œì•½ ì˜¤ë¥¸ìª½(ë’¤ìª½)ì˜ ë‹¨ì–´ë¥¼ ì•ˆë‹¤ë©´ ì´ë¯¸ ì •ë‹µì„ ì•Œê³  ìˆê¸°ì— cheatingìœ¼ë¡œ ê°„ì£¼ëœë‹¤.\në”°ë¼ì„œ ì²« ë²ˆì§¸ ë‹¨ì–´ëŠ” ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ì „í˜€ ëª¨ë¦„.\nì¦‰, GPTëŠ” ì•ìª½ ë‹¨ì–´ë“¤ì´ ë’¤ìª½ ë‹¨ì–´ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆì§€ë§Œ, ë°˜ëŒ€ë¡œ ë’¤ìª½ ë‹¨ì–´ê°€ ì•ìª½ ë‹¨ì–´ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ì—†ìŒ!"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#model",
    "href": "posts/Based_on_Decoder.html#model",
    "title": "Base on Decoder models",
    "section": "2-1. model",
    "text": "2-1. model\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\nmodel_name = \"skt/kogpt2-base-v2\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    bos_token=\"&lt;/s&gt;\",\n    eos_token=\"&lt;/s&gt;\",\n    unk_token=\"&lt;unk&gt;\",\n    pad_token=\"&lt;pad&gt;\",\n    mask_token=\"&lt;mask&gt;\"\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel\n\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(51200, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n)\n\n\n- ëª¨ë¸ì„ ì˜ ì‚´í´ë³´ë©´ ì²« ì¤„ì˜ Embedding(51200, 768)ê³¼ ë§ˆì§€ë§‰ ì¤„ì˜ (lm_head): Linear(in_features=768, out_features=51200, bias=False)ì—ì„œ 51200ìœ¼ë¡œ ìˆ«ìê°€ ê°™ì€ë° ì´ê²Œ ìš°ì—°ì´ ì•„ë‹ˆë‹¤.\n! Embedding(vocab_size , Embedding_dim)ìœ¼ë¡œ ì´ë£¨ì–´ì§€ëŠ”ë° 51200ê°œì˜ ë‹¨ì–´ë¥¼ 768 ê¸¸ì´ì˜ ë²¡í„°ì—ì„œ ì„ë² ë”©ì„ í†µí•´ ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ í‘œí˜„í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. Embedding_dimì´ ì»¤ì§ˆ ìˆ˜ë¡ ì—°ì‚°ì€ ì¦ê°€í•˜ì§€ë§Œ ë” ì„¸ì‹¬í•˜ê²Œ ë‹¨ì–´ë“¤ë¼ë¦¬ì˜ ì°¨ì´ë¥¼ ë‘˜ ìˆ˜ ìˆë‹¤.\nìœ„ì—ì„œ ë§í–ˆë“¯ì´ ì¶œë ¥ classëŠ” vocab sizeì™€ ê°™ê¸°ì— ê·¸ë˜ì„œ 51200ì˜ ìˆ˜ê°€ ê²¹ì¹˜ëŠ” ê²ƒì´ë‹¤."
  },
  {
    "objectID": "posts/Based_on_Decoder.html#dataset",
    "href": "posts/Based_on_Decoder.html#dataset",
    "title": "Base on Decoder models",
    "section": "2-2. Dataset",
    "text": "2-2. Dataset\n- ìœ„í‚¤ ë°ì´í„°ë¼ì„œ ì–‘ì´ ë§¤ìš° ë§ë‹¤. ì¼ë¶€ë§Œ ë½‘ì•„ì„œ ì‚¬ìš©í•˜ì.\n\nfrom datasets import load_dataset\n\nsplit_dict = {\n    \"train\": \"train[:8000]\",\n    \"test\": \"train[8000:10000]\",\n    \"unused\": \"train[10000:]\",\n}\ndataset = load_dataset(\"heegyu/kowikitext\", split=split_dict)\ndel dataset[\"unused\"]\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'revid', 'url', 'title', 'text'],\n        num_rows: 8000\n    })\n    test: Dataset({\n        features: ['id', 'revid', 'url', 'title', 'text'],\n        num_rows: 2000\n    })\n})\n\n\n\ntokenized_dataset = dataset.map(\n    lambda batch: tokenizer([f\"{ti}\\n{te}\" for ti, te in zip(batch[\"title\"], batch[\"text\"])]),\n    batched=True, # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ì •\n    num_proc=2, # ë³‘ë ¬ ì²˜ë¦¬ ê°œìˆ˜ë¥¼ ì„¤ì • (CPU ì½”ì–´ 2ê°œ ì‚¬ìš©)\n    remove_columns=dataset[\"train\"].column_names, # ê¸°ì¡´ ë°ì´í„°ì…‹ì˜ ì»¬ëŸ¼ ì‚­ì œ -&gt; í† í°í™” ë°ì´í„°ë§Œ ë‚¨ê¹€\n)\ntokenized_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 8000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 2000\n    })\n})"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#preprocessing",
    "href": "posts/Based_on_Decoder.html#preprocessing",
    "title": "Base on Decoder models",
    "section": "2-3. Preprocessing",
    "text": "2-3. Preprocessing\n\nmax_length = 512 # ë¬¸ì¥ì˜ ìµœëŒ€ê¸¸ì´\ndef group_texts(batched_sample):\n    sample = {k: v[0] for k, v in batched_sample.items()}\n\n    if sample[\"input_ids\"][-1] != tokenizer.eos_token_id: # sample['input_ids']ì˜ ë§ˆì§€ë§‰ í† í°ì´ eos í† í°ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì‹¤í–‰ëœë‹¤.\n        for k in sample.keys():\n            sample[k].append(\n                tokenizer.eos_token_id if k == \"input_ids\" else sample[k][-1] # input_ids í‚¤ì¸ ê²½ìš°ì—ëŠ” eos_token_idë¥¼ ì¶”ê°€í•œë‹¤.\n            )\n\n    result = {k: [v[i: i + max_length] for i in range(0, len(v), max_length)] for k, v in sample.items()}\n    return result\n\ngrouped_dataset = tokenized_dataset.map(\n    group_texts,\n    batched=True, # ì…ë ¥ ë°ì´í„°ê°€ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì „ë‹¬ë¨\n    batch_size=1, # í•œ ë²ˆì— 1ê°œì˜ ë°ì´í„°ë§Œ ì²˜ë¦¬\n    num_proc=2,\n)\nprint(len(grouped_dataset[\"train\"][0][\"input_ids\"]))\nprint(grouped_dataset)\n\n512\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 18365\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 2400\n    })\n})\n\n\n- ë°ì´í„°ì— ì •ë‹µ ë°ì´í„°ì¸ labels ì¹¼ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. (ë””ì½”ë” ê¸°ë°˜ì´ë‹ˆê¹Œ)\në””ì½”ë” ê¸°ë°˜ ëª¨ë¸ ìƒì„±íƒœìŠ¤í¬ì—ì„œëŠ” labels ë°ì´í„°ë¥¼ ë”°ë¡œ ì‘ì„±í•˜ì§€ ì•Šê³  input_idsì— ëª¨ë‘ í¬í•¨í•œë‹¤.\n-&gt; ì…ë ¥ëœ ë¬¸ì¥ì„ í•œ ì¹¸ ì´ë™ì‹œí‚¨ ê°’ì„ ì •ë‹µìœ¼ë¡œ ì‚¬ìš©í•¨. (ê¸°ìˆ ì  ì´ìœ ë¡œ labels ì¹¼ëŸ¼ì€ ì¶”ê°€í•´ì•¼í•˜ê¸°ì— ì½œë ˆì´í„°ë¥¼ ì‚¬ìš©í•´ ì •ë‹µ labelì´ ì—†ëŠ” ê²ƒì„ í•´ê²°í•œë‹¤.)"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#collator",
    "href": "posts/Based_on_Decoder.html#collator",
    "title": "Base on Decoder models",
    "section": "2-4. Collator",
    "text": "2-4. Collator\nDataCollatorëŠ” ë°°ì¹˜ ë‚´ì—ì„œ sampleì„ íŒ¨ë”©í•˜ê±°ë‚˜, ë§ˆìŠ¤í‚¹ì„ ì ìš©í•˜ëŠ” ì—­í• ì„ í•œë‹¤.\n\nfrom transformers import DataCollatorForLanguageModeling\n\ncollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # mlml = False: ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ë§ì„ ì‚¬ìš©í•˜ì§€ ì•Šê² ë‹¤ëŠ” ì˜ë¯¸\n# mlm=Trueë¡œ ì„¤ì •í•œë‹¤ë©´ ì¼ë¶€ í† í°ì„ [MASK]ë¡œ ë³€í™˜í•˜ì—¬ ì›ë˜ ë‹¨ì–´ë¥¼ ë§ì¶”ë„ë¡ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ê²ƒì€ ì¸ì½”ë” ê¸°ë°˜ì˜ BERTê°€ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ë¯€ë¡œ ì§€ê¸ˆì€ Falseë¡œ ì„¤ì •í•œë‹¤.\nsample = collator([grouped_dataset[\"train\"][i] for i in range(1)])\n# ìœ„ì—ì„œ ë°ì´í„° ì „ì²˜ë¦¬í•  ë•Œ batch_size = 1ì´ë¯€ë¡œ range(1)ë¡œ ì„¤ì •í–ˆë‹¤. ë§Œì•½ batchë¥¼ ëŠ˜ë¦¬ê³ ì‹¶ë‹¤ë©´ range(???) ???ì„ ëŠ˜ë¦¬ë©´ ëœë‹¤."
  },
  {
    "objectID": "posts/Based_on_Decoder.html#evaluate",
    "href": "posts/Based_on_Decoder.html#evaluate",
    "title": "Base on Decoder models",
    "section": "2-5. Evaluate",
    "text": "2-5. Evaluate\në¬¸ì¥ ìƒì„± íƒœìŠ¤í¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµ ë„ì¤‘ í‰ê°€ë¥¼ ì§„í–‰í•˜ê¸° ì–´ë µë‹¤.\n\nì´ìœ \n\nì¼ë°˜ì ì¸ ëª¨ë¸ì€ ì…ë ¥ -&gt; ì¶œë ¥ 1ë²ˆì˜ ê³¼ì •ì´ë‹¤. í•˜ì§€ë§Œ ìƒì„±ëª¨ë¸ì€ í•œ ë¬¸ì¥ì„ ë§Œë“¤ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²ˆ ì¶”ë¡ í•´ì•¼í•¨.\n\n\nì¼ë°˜ì  ëª¨ë¸: I go school -&gt; (ê¸ì •)\nìƒì„±ëª¨ë¸: â€™I-&gt; 'I go' -&gt; 'I go school (ì—¬ëŸ¬ë²ˆ ì¶”ë¡ ì´ í•„ìš”!)\n\n\nìƒì„± ëª¨ë¸ì€ í•œ ë²ˆì— ëª‡ ë²ˆ ì¶”ë¡ í• ì§€ ì •í•´ì ¸ ìˆì§€ ì•ŠìŒ!\n\n\në‹¹ì—°í•˜ë‹¤. ë¬¸ì¥ì´ ì–¸ì œ ëë‚ ì§€ ìƒì„±ëª¨ë¸ì€ ì•Œ ìˆ˜ ì—†ë‹¤. (ë¯¸ë˜ë¥¼ ëª¨ë¥´ê¸° ë•Œë¬¸ì—)\n\n\nì •ë‹µê³¼ ë¹„êµí•˜ëŠ” ë°©ì‹ì´ ë‹¤ë¥´ë‹¤.\n\n\nì¼ë°˜ì ì¸ ëª¨ë¸ì€ ì‹¤ìˆ˜ ê°’ì„ ì¶œë ¥í•˜ë¯€ë¡œ ì‚°ìˆ  ì—°ì‚°ì„ ì´ìš©í•´ ì •ë‹µê³¼ ë¹„êµê°€ ê°€ëŠ¥í•˜ë‹¤.\ní•˜ì§€ë§Œ ìƒì„± ëª¨ë¸ì€ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ(í† í° ID ë¦¬ìŠ¤íŠ¸)ë¥¼ ì¶œë ¥í•˜ê¸°ì— ë¹„êµ ë°©ì‹ì´ ë‹¤ë¥´ë‹¤.\nëª¨ë¸ ì¶œë ¥: [12,34,56,78] ì •ë‹µ: [12,34,90,66]\në‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ë…¼ë¦¬ ì—°ì‚°ìœ¼ë¡œ ë§ëŠ”ì§€ í‰ê°€í•´ì•¼ í•œë‹¤.\n\n\nê¸¸ì´ê°€ ë‹¤ ë‹¬ë¼ì„œ í•œ ë²ˆì— ì²˜ë¦¬í•˜ê¸° ì–´ë µë‹¤.\n\n\nê°ì • ë¶„ì„ê°™ì€ ì¼ë°˜ì  ëª¨ë¸ì€ í•­ìƒ ì¼ì •í•œ ì¶œë ¥ì´ ë‚˜ì˜¤ë¯€ë¡œ í•œ ë²ˆì— ë°°ì¹˜ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤.\ní•˜ì§€ë§Œ ìƒì„± ëª¨ë¸ì€ ì¶œë ¥ë˜ëŠ” ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ë‹¤ ë‹¤ë¥´ë‹¤.\n\nI go school\nI go shcool to meet my friend\nnice to meet you\n\nì´ë ‡ê²Œ ê¸¸ì´ê°€ ë‹¤ ë‹¤ë¥´ë©´ í•œ ë²ˆì— ë°°ì¹˜ë¡œ ë¹„êµí•˜ê¸° ì–´ë ¤ì›€"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#generate-sentence",
    "href": "posts/Based_on_Decoder.html#generate-sentence",
    "title": "Base on Decoder models",
    "section": "2-6. Generate sentence",
    "text": "2-6. Generate sentence\n\ninputs = tokenizer(\"ì§€ë‚œí•´ 7ì›”, \", return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(inputs.input_ids, max_new_tokens=100)\nresult = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(result[0])\n\nì§€ë‚œí•´ 7ì›”, ë¡¯ë°ë°±í™”ì  ë³¸ì  ì§€í•˜ 1ì¸µ ì‹í’ˆë§¤ì¥ì—ì„œ íŒë§¤ëœ 'ë¡¯\n\n\nmax_new_tokens = 100 ë•Œë¬¸ì— ê¸¸ì´ë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŒ.\n\nimport torch\n\ninput_ids = tokenizer(\"ì§€ë‚œí•´ 7ì›”, \", return_tensors=\"pt\").to(model.device).input_ids\n\nwith torch.no_grad():\n    for _ in range(100):\n        next_token = model(input_ids).logits[0, -1:].argmax(-1)\n        input_ids = torch.cat((input_ids[0], next_token), -1).unsqueeze(0)\n\nprint(tokenizer.decode(input_ids[0].tolist()))\n\nì§€ë‚œí•´ 7ì›”, ë¡¯ë°ë°±í™”ì  ë³¸ì  ì§€í•˜ 1ì¸µ ì‹í’ˆë§¤ì¥ì—ì„œ íŒë§¤ëœ 'ë¡¯ë° í–„ë²„ê±°' ì œí’ˆì—ì„œ ëŒ€ì¥ê· ì´ ê²€ì¶œë¼ íŒë§¤ ì¤‘ë‹¨ëœ ë°” ìˆë‹¤.\në¡¯ë°ë°±í™”ì  ì¸¡ì€ \"í–„ë²„ê±° íŒë§¤ ì¤‘ë‹¨ì€ ë¡¯ë°ë°±í™”ì  ë³¸ì  ì‹í’ˆë§¤ì¥ì˜ ìœ„ìƒê³¼ ì•ˆì „ê´€ë¦¬ì— ëŒ€í•œ ê³ ê°ë“¤ì˜ ì‹ ë¢°ê°€ í¬ê²Œ í›¼ì†ëœ ë° ë”°ë¥¸ ê²ƒ\"ì´ë¼ë©° \"ë¡¯ë°ë°±í™”ì  ë³¸ì  ì‹í’ˆë§¤ì¥ì€ í–„ë²„ê±° íŒë§¤ ì¤‘ë‹¨ì„ ì¦‰ê° ì¤‘ë‹¨í•˜ê³ , ë¡¯ë°ë°±í™”ì  ë³¸ì  ì‹í’ˆ\n\n\nEOS í† í°ì„ ë§Œë‚˜ë„ 100ë²ˆ ë°˜ë³µ. for _ in range(100)ì€ 100ë²ˆ ì‹¤í–‰ëœë‹¤ëŠ” ë³´ì¥ë§Œ ìˆì„ ë¿, ë¬¸ì¥ì´ ê¸¸ì–´ì§€ëŠ” ê±¸ ë§‰ì§€ ì•ŠëŠ”ë‹¤. ì‹¤ì œë¡œ ìƒì„±ëœ í† í° ê°œìˆ˜ê°€ 100ê°œë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìŒ.\nê²°ë¡ \nì²« ë²ˆì§¸ ì½”ë“œëŠ” ê°•ì œ ì œí•œì´ ìˆì§€ë§Œ ë‘ ë²ˆì§¸ ì½”ë“œëŠ” 100ë²ˆ ë°˜ë³µí•˜ë©´ì„œ ë¬¸ì¥ì´ ë” ê¸¸ì–´ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ìŒ."
  },
  {
    "objectID": "posts/Based_on_Decoder.html#model-1",
    "href": "posts/Based_on_Decoder.html#model-1",
    "title": "Base on Decoder models",
    "section": "3-1. model",
    "text": "3-1. model\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"skt/kogpt2-base-v2\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    bos_token=\"&lt;/s&gt;\",\n    eos_token=\"&lt;/s&gt;\",\n    unk_token=\"&lt;unk&gt;\",\n    pad_token=\"&lt;pad&gt;\",\n    mask_token=\"&lt;mask&gt;\"\n)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel\n\nSome weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nGPT2ForSequenceClassification(\n  (transformer): GPT2Model(\n    (wte): Embedding(51200, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=2, bias=False)\n)\n\n\n- out_features = 2 \\(\\rightarrow\\) ì´ì§„ë¶„ë¥˜"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#dataset-1",
    "href": "posts/Based_on_Decoder.html#dataset-1",
    "title": "Base on Decoder models",
    "section": "3-2. Dataset",
    "text": "3-2. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"sts\")\n\ndef process_data(batch):\n  result = tokenizer(batch[\"sentence1\"], text_pair=batch[\"sentence2\"])\n  result[\"labels\"] = [x[\"binary-label\"] for x in batch[\"labels\"]]\n  return result\n\ndataset = dataset.map(process_data, batched=True, remove_columns=dataset[\"train\"].column_names)"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#collator-1",
    "href": "posts/Based_on_Decoder.html#collator-1",
    "title": "Base on Decoder models",
    "section": "3-3. Collator",
    "text": "3-3. Collator\n\nfrom transformers import DataCollatorWithPadding # íŒ¨ë”©ë§Œì„ ì§„í–‰í•˜ëŠ” DataCollatorWithPadding ì‚¬ìš©\n\ncollator = DataCollatorWithPadding(tokenizer)\nbatch = collator([dataset[\"train\"][i] for i in range(4)])"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#predictions",
    "href": "posts/Based_on_Decoder.html#predictions",
    "title": "Base on Decoder models",
    "section": "3-4. Predictions",
    "text": "3-4. Predictions\n\nwith torch.no_grad():\n    logits = model(**batch).logits\n\nlogits\n\ntensor([[ 0.5085, -0.4168],\n        [-0.1568, -0.9897],\n        [ 1.4718, -0.7141],\n        [ 0.5799, -0.3399]])"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#evaluate-1",
    "href": "posts/Based_on_Decoder.html#evaluate-1",
    "title": "Base on Decoder models",
    "section": "3-5. Evaluate",
    "text": "3-5. Evaluate\n\nimport evaluate\n\nf1 = evaluate.load('f1')\nf1.compute(predictions = logits.argmax(-1), references = batch['labels'], average = 'micro')\n\n{'f1': 0.75}"
  },
  {
    "objectID": "posts/About_Transformers.html",
    "href": "posts/About_Transformers.html",
    "title": "About Transformers",
    "section": "",
    "text": "ì„ë² ë”© ë ˆì´ì–´: ì…ë ¥ ë°ì´í„°ë¥¼ ê³ ì •ëœ ì°¨ì›ì˜ ì—°ì†í˜• ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\nìœ„ì¹˜ ì¸ì½”ë”©: ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ì‚¬ì¸ ë° ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ìœ„ì¹˜ ì¸ì½”ë”©ì„ ì¶”ê°€í•œë‹¤.\nìµœì¢… ì…ë ¥: ì…ë ¥ ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì¸ì½”ë”©ì„ ë”í•œ ê°’ì„ ì¸ì½”ë”ì— ì „ë‹¬\n\n\n\n\n\nì¸ì½”ë”ëŠ” ì—¬ëŸ¬ ê°œì˜ ì¸ì½”ë” ë ˆì´ì–´ë¡œ ì´ë£¨ì–´ì§€ë©°, ê° ì¸ì½”ë” ë ˆì´ì–´ëŠ” ë‹¤ìŒì„ í¬í•¨í•œë‹¤.\n\n\në©€í‹°-í—¤ë“œ ì…€í”„ ì–´í…ì…˜ (Multi-Head Self-Attention)\n\nì…ë ¥ ë¬¸ì¥ì—ì„œ ê° ë‹¨ì–´ê°€ ë¬¸ë§¥ì„ ë°˜ì˜í•´ ì„œë¡œ ë‹¤ë¥¸ ì¤‘ìš”ë„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ ì–´í…ì…˜ì„ ì ìš©í•©ë‹ˆë‹¤.\n(ì…ë ¥ ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©) -&gt; ì¿¼ë¦¬(Q), í‚¤(K), ë°¸ë¥˜(V) ìƒì„± -&gt; ì–´í…ì…˜ ê²°ê³¼ ì¶œë ¥\n\nì”ì°¨ ì—°ê²° (Residual Connection) + Layer Normalization\n\nì…ë ¥ ê°’(ì…ë ¥ ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©)ê³¼ ì–´í…ì…˜ ê²°ê³¼ë¥¼ ë”í•œ í›„ ì •ê·œí™” ì ìš© (ì´ìƒí•˜ë‹¤ê³  ìƒê° ê°€ëŠ¥ -&gt; ë°‘ì—ì„œ ì„¤ëª…)\n\ní”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (Feedforward Network, FFN)\n\në¹„ì„ í˜• ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” ë‘ ê°œì˜ ì™„ì „ì—°ê²°ì¸µ(FC)ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.\në¹„ì„ í˜• ë³€í™˜ì´ë€ ReLUì™€ ê°™ì€ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.\në¹„ì„ í˜• ë³€í™˜ì„ ì´ìš©í•˜ëŠ” ì´ìœ ëŠ” í•™ìŠµì˜ ë‹¤ì–‘ì„±ê³¼ ì•ˆì •ì„±ì„ ë†’ì´ê¸° ìœ„í•´ì„œì´ë‹¤. ì„ í˜• ë³€í™˜ë§Œìœ¼ë¡œëŠ” ê³±ì…ˆê³¼ ë§ì…ˆìœ¼ë¡œë§Œ ë‚˜íƒ€ë‚´ê¸°ì— í•¨ìˆ˜ì˜ í˜•íƒœê°€ ë‹¨ìˆœí•œë° ë¹„ì„ í˜• ë³€í™˜ì„ í†µí•´ ë” ë³µì¡í•œ í•¨ìˆ˜ì™€ ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.\n\nì”ì°¨ ì—°ê²° + Layer Normalization\n\nFFNì˜ ì¶œë ¥ì„ ë‹¤ì‹œ ì…ë ¥ê³¼ ë”í•œ í›„ ì •ê·œí™”í•©ë‹ˆë‹¤. (2ë²ˆê³¼ ë¹„ìŠ·í•˜ê²Œ ì…ë ¥ ê°’ì„ ë‹¤ì‹œ ë”í•´ì¤€ë‹¤)\nê·¸ í›„ ì •ê·œí™”ë¥¼ í•œë‹¤. ì¦‰, LayerNorm(FFN Output + Attention Output)\n\n\n\nì´ëŸ¬í•œ ì¸ì½”ë” ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ ê°œ ìŒ“ì•„ ê¹Šì€ í‘œí˜„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n\n- ì´ìƒí•œ ë¶€ë¶„ì˜ ë³´ì¶©ì„¤ëª…\nQ. ì–´í…ì…˜ ê²°ê³¼ë¼ëŠ” ê²ƒì€ ì…ë ¥ê°’ì„ ë©€í‹°-í—¤ë“œ ì…€í”„ ì–´í…ì…˜ì„ ê±°ì³ì„œ ë‚˜ì˜¨ ê²°ê³¼ì–ì•„ìš”. ê·¸ëŸ°ë° ê·¸ ê²°ê³¼ë‘ ì…ë ¥ ê°’ì„ ë˜ ë”í•œë‹¤êµ¬ìš”? ì™œ ë˜ ë”í•´ìš”?\n\nA. ì–´í…ì…˜ì˜ ì¶œë ¥ì€ ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ëœ ê°’ì¸ë°, ë‹¤ì‹œ ì…ë ¥ê³¼ ë”í•˜ëŠ” ê²ƒì´ ì´ìƒí•˜ê²Œ ë³´ì¼ ìˆ˜ ìˆìŒ. ê·¸ ì´ìœ ëŠ” ì”ì°¨ ì—°ê²°(Residual Connection)**ì€ ì‹ ê²½ë§ì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ë°œìƒí•˜ëŠ” í•™ìŠµ ì–´ë ¤ì›€ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë„ì…ëœ ê¸°ë²•ì…ë‹ˆë‹¤. ë§Œì•½ ì–´í…ì…˜ ì¶œë ¥ë§Œ ë‹¤ìŒ ë ˆì´ì–´ë¡œ ë„˜ê¸´ë‹¤ë©´, ì›ë˜ ì…ë ¥ ì •ë³´ê°€ ì†ì‹¤ë  ìˆ˜ ìˆìŒ. ë”°ë¼ì„œ, ì›ë˜ ì…ë ¥ ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œë„, ì–´í…ì…˜ì´ í•™ìŠµí•œ ìƒˆë¡œìš´ ì •ë³´(ì¶œë ¥)ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë°˜ì˜í•˜ê¸° ìœ„í•´ ì…ë ¥ + ì–´í…ì…˜ ì¶œë ¥ì„ ë”í•¨.\n\nì”ì°¨ ì—°ê²°ì´ ì—†ë‹¤ë©´? : ì¸ì½”ë”/ë””ì½”ë”ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ì…ë ¥ ì •ë³´ê°€ ì™œê³¡ë  ê°€ëŠ¥ì„±ì´ ì»¤ì§„ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì…ë ¥ ì •ë³´(ì›ë˜ ê°’) + ì–´í…ì…˜ ê²°ê³¼(ìƒˆë¡œìš´ ì •ë³´)ë¥¼ í•¨ê»˜ ìœ ì§€í•˜ëŠ” ê²ƒì´ í•™ìŠµ ì•ˆì •ì„± ì¸¡ë©´ì—ì„œ ìœ ë¦¬í•¨. ì´ê²ƒì´ ì”ì°¨ ì—°ê²°ì˜ í•µì‹¬ ê°œë…ì´ë‹¤.\n\n\n\n\në””ì½”ë”ë„ ì—¬ëŸ¬ ê°œì˜ ë””ì½”ë” ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ë©°, ì¸ì½”ë”ì™€ ë‹¤ë¥¸ ì ì€ ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ ë ˆì´ì–´ê°€ ì¶”ê°€ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\nê° ë””ì½”ë” ë ˆì´ì–´ëŠ” ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤.\n\n\në§ˆìŠ¤í¬ë“œ ë©€í‹°-í—¤ë“œ ì…€í”„ ì–´í…ì…˜ (Masked Multi-Head Self-Attention)\n\në””ì½”ë”ëŠ” ì •ë‹µ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì—ì„œ ì•ìª½ ë‹¨ì–´ë§Œ ì°¸ê³ í•  ìˆ˜ ìˆë„ë¡ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ì—¬ ë¯¸ë˜ ì •ë³´ë¥¼ ì°¨ë‹¨í•©ë‹ˆë‹¤.\në¯¸ë˜ ì •ë³´(ë¯¸ë˜ì— ë‚˜ì˜¬ ë‹¨ì–´)ë¥¼ ë¯¸ë¦¬ ì•ˆë‹¤ë©´ ì •ë‹µì§€ë¥¼ ë³´ê³  ì»¨ë‹ì„ í•˜ëŠ” ê²ƒê³¼ ê°™ê¸°ì— ì˜¬ë°”ë¡  í•™ìŠµì´ ë˜ ì§€ ì•ŠëŠ”ë‹¤.\në§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ë„ ì¸ì½”ë”ì˜ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ê³¼ ë˜‘ê°™ì´ Q,K,V ê°’ ê³„ì‚° -&gt; ì–´í…ì…˜ ê²°ê³¼ ì¶œë ¥\n\nì”ì°¨ ì—°ê²° + Layer Normalization\n\nì¸ì½”ë” êµ¬ì¡°ì—ì„œ ì„¤ëª…í•œ ê²ƒê³¼ ê°™ìŒ.\n\nì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ (Encoder-Decoder Attention)\n\nì¸ì½”ë”ì—ì„œ ì¶œë ¥ëœ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë¬¸ì¥ê³¼ í˜„ì¬ ë””ì½”ë” ìƒíƒœ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\në””ì½”ë”ë§Œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸(GPT)ëŠ” ì´ ì¸µì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. ì¸ì½”ë”ì—ì„œ ë‚˜ì˜¨ ì¶œë ¥ì„ ì´ìš©í•˜ê¸°ì— ì¸ì½”ë”ì™€ ë””ì½”ë”ê°€ ëª¨ë‘ ìˆëŠ” ëª¨ë¸ì—ì„œë§Œ ì´ ì¸µì„ ì‚¬ìš©í•œë‹¤.\n\nì”ì°¨ ì—°ê²° + Layer Normalization\n\n3ì—ì„œ ì¶”ê°€ëœ ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ ì¸µì´ ìˆê¸°ì— ì”ì°¨ ì—°ê²° + Layer Normalizaiton ì¸µì´ ì¶”ê°€ëœë‹¤.\n\ní”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (FFN)\nì”ì°¨ ì—°ê²° + Layer Normalization\n\n\n\n\n\nì„ í˜• ë³€í™˜ (Linear Layer): ë””ì½”ë”ì—ì„œ ë‚˜ì˜¨ ì¶œë ¥ì„ ë‹¨ì–´ ì§‘í•© í¬ê¸°ë§Œí¼ì˜ ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\nì†Œí”„íŠ¸ë§¥ìŠ¤ (Softmax): í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•´ ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n\n\n\n\n\nì…ë ¥ ë°ì´í„° ì²˜ë¦¬ (ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©)\nì¸ì½”ë” (ì—¬ëŸ¬ ê°œì˜ ì¸ì½”ë” ë ˆì´ì–´) -ë©€í‹°-í—¤ë“œ ì…€í”„ ì–´í…ì…˜ â†’ ì”ì°¨ ì—°ê²° + ì •ê·œí™” â†’ FFN â†’ ì”ì°¨ ì—°ê²° + ì •ê·œí™”\në””ì½”ë” (ì—¬ëŸ¬ ê°œì˜ ë””ì½”ë” ë ˆì´ì–´) -ë§ˆìŠ¤í¬ë“œ ë©€í‹°-í—¤ë“œ ì…€í”„ ì–´í…ì…˜ â†’ ì”ì°¨ ì—°ê²° + ì •ê·œí™” -ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ â†’ ì”ì°¨ ì—°ê²° + ì •ê·œí™” -FFN â†’ ì”ì°¨ ì—°ê²° + ì •ê·œí™”\nì¶œë ¥ ë³€í™˜ (ì„ í˜• ë ˆì´ì–´ â†’ ì†Œí”„íŠ¸ë§¥ìŠ¤ â†’ ì˜ˆì¸¡)"
  },
  {
    "objectID": "posts/About_Transformers.html#ì…ë ¥-ë°ì´í„°-ì „ì²˜ë¦¬",
    "href": "posts/About_Transformers.html#ì…ë ¥-ë°ì´í„°-ì „ì²˜ë¦¬",
    "title": "About Transformers",
    "section": "",
    "text": "ì„ë² ë”© ë ˆì´ì–´: ì…ë ¥ ë°ì´í„°ë¥¼ ê³ ì •ëœ ì°¨ì›ì˜ ì—°ì†í˜• ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\nìœ„ì¹˜ ì¸ì½”ë”©: ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ì‚¬ì¸ ë° ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ìœ„ì¹˜ ì¸ì½”ë”©ì„ ì¶”ê°€í•œë‹¤.\nìµœì¢… ì…ë ¥: ì…ë ¥ ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì¸ì½”ë”©ì„ ë”í•œ ê°’ì„ ì¸ì½”ë”ì— ì „ë‹¬"
  },
  {
    "objectID": "posts/About_Transformers.html#ì¸ì½”ë”-êµ¬ì¡°-ì—¬ëŸ¬-ê°œì˜-ì¸ì½”ë”-ë ˆì´ì–´-ìŠ¤íƒ",
    "href": "posts/About_Transformers.html#ì¸ì½”ë”-êµ¬ì¡°-ì—¬ëŸ¬-ê°œì˜-ì¸ì½”ë”-ë ˆì´ì–´-ìŠ¤íƒ",
    "title": "About Transformers",
    "section": "",
    "text": "ì¸ì½”ë”ëŠ” ì—¬ëŸ¬ ê°œì˜ ì¸ì½”ë” ë ˆì´ì–´ë¡œ ì´ë£¨ì–´ì§€ë©°, ê° ì¸ì½”ë” ë ˆì´ì–´ëŠ” ë‹¤ìŒì„ í¬í•¨í•œë‹¤.\n\n\në©€í‹°-í—¤ë“œ ì…€í”„ ì–´í…ì…˜ (Multi-Head Self-Attention)\n\nì…ë ¥ ë¬¸ì¥ì—ì„œ ê° ë‹¨ì–´ê°€ ë¬¸ë§¥ì„ ë°˜ì˜í•´ ì„œë¡œ ë‹¤ë¥¸ ì¤‘ìš”ë„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ ì–´í…ì…˜ì„ ì ìš©í•©ë‹ˆë‹¤.\n(ì…ë ¥ ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©) -&gt; ì¿¼ë¦¬(Q), í‚¤(K), ë°¸ë¥˜(V) ìƒì„± -&gt; ì–´í…ì…˜ ê²°ê³¼ ì¶œë ¥\n\nì”ì°¨ ì—°ê²° (Residual Connection) + Layer Normalization\n\nì…ë ¥ ê°’(ì…ë ¥ ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©)ê³¼ ì–´í…ì…˜ ê²°ê³¼ë¥¼ ë”í•œ í›„ ì •ê·œí™” ì ìš© (ì´ìƒí•˜ë‹¤ê³  ìƒê° ê°€ëŠ¥ -&gt; ë°‘ì—ì„œ ì„¤ëª…)\n\ní”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (Feedforward Network, FFN)\n\në¹„ì„ í˜• ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” ë‘ ê°œì˜ ì™„ì „ì—°ê²°ì¸µ(FC)ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.\në¹„ì„ í˜• ë³€í™˜ì´ë€ ReLUì™€ ê°™ì€ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.\në¹„ì„ í˜• ë³€í™˜ì„ ì´ìš©í•˜ëŠ” ì´ìœ ëŠ” í•™ìŠµì˜ ë‹¤ì–‘ì„±ê³¼ ì•ˆì •ì„±ì„ ë†’ì´ê¸° ìœ„í•´ì„œì´ë‹¤. ì„ í˜• ë³€í™˜ë§Œìœ¼ë¡œëŠ” ê³±ì…ˆê³¼ ë§ì…ˆìœ¼ë¡œë§Œ ë‚˜íƒ€ë‚´ê¸°ì— í•¨ìˆ˜ì˜ í˜•íƒœê°€ ë‹¨ìˆœí•œë° ë¹„ì„ í˜• ë³€í™˜ì„ í†µí•´ ë” ë³µì¡í•œ í•¨ìˆ˜ì™€ ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.\n\nì”ì°¨ ì—°ê²° + Layer Normalization\n\nFFNì˜ ì¶œë ¥ì„ ë‹¤ì‹œ ì…ë ¥ê³¼ ë”í•œ í›„ ì •ê·œí™”í•©ë‹ˆë‹¤. (2ë²ˆê³¼ ë¹„ìŠ·í•˜ê²Œ ì…ë ¥ ê°’ì„ ë‹¤ì‹œ ë”í•´ì¤€ë‹¤)\nê·¸ í›„ ì •ê·œí™”ë¥¼ í•œë‹¤. ì¦‰, LayerNorm(FFN Output + Attention Output)\n\n\n\nì´ëŸ¬í•œ ì¸ì½”ë” ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ ê°œ ìŒ“ì•„ ê¹Šì€ í‘œí˜„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n\n- ì´ìƒí•œ ë¶€ë¶„ì˜ ë³´ì¶©ì„¤ëª…\nQ. ì–´í…ì…˜ ê²°ê³¼ë¼ëŠ” ê²ƒì€ ì…ë ¥ê°’ì„ ë©€í‹°-í—¤ë“œ ì…€í”„ ì–´í…ì…˜ì„ ê±°ì³ì„œ ë‚˜ì˜¨ ê²°ê³¼ì–ì•„ìš”. ê·¸ëŸ°ë° ê·¸ ê²°ê³¼ë‘ ì…ë ¥ ê°’ì„ ë˜ ë”í•œë‹¤êµ¬ìš”? ì™œ ë˜ ë”í•´ìš”?\n\nA. ì–´í…ì…˜ì˜ ì¶œë ¥ì€ ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ëœ ê°’ì¸ë°, ë‹¤ì‹œ ì…ë ¥ê³¼ ë”í•˜ëŠ” ê²ƒì´ ì´ìƒí•˜ê²Œ ë³´ì¼ ìˆ˜ ìˆìŒ. ê·¸ ì´ìœ ëŠ” ì”ì°¨ ì—°ê²°(Residual Connection)**ì€ ì‹ ê²½ë§ì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ë°œìƒí•˜ëŠ” í•™ìŠµ ì–´ë ¤ì›€ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë„ì…ëœ ê¸°ë²•ì…ë‹ˆë‹¤. ë§Œì•½ ì–´í…ì…˜ ì¶œë ¥ë§Œ ë‹¤ìŒ ë ˆì´ì–´ë¡œ ë„˜ê¸´ë‹¤ë©´, ì›ë˜ ì…ë ¥ ì •ë³´ê°€ ì†ì‹¤ë  ìˆ˜ ìˆìŒ. ë”°ë¼ì„œ, ì›ë˜ ì…ë ¥ ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œë„, ì–´í…ì…˜ì´ í•™ìŠµí•œ ìƒˆë¡œìš´ ì •ë³´(ì¶œë ¥)ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë°˜ì˜í•˜ê¸° ìœ„í•´ ì…ë ¥ + ì–´í…ì…˜ ì¶œë ¥ì„ ë”í•¨.\n\nì”ì°¨ ì—°ê²°ì´ ì—†ë‹¤ë©´? : ì¸ì½”ë”/ë””ì½”ë”ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ì…ë ¥ ì •ë³´ê°€ ì™œê³¡ë  ê°€ëŠ¥ì„±ì´ ì»¤ì§„ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì…ë ¥ ì •ë³´(ì›ë˜ ê°’) + ì–´í…ì…˜ ê²°ê³¼(ìƒˆë¡œìš´ ì •ë³´)ë¥¼ í•¨ê»˜ ìœ ì§€í•˜ëŠ” ê²ƒì´ í•™ìŠµ ì•ˆì •ì„± ì¸¡ë©´ì—ì„œ ìœ ë¦¬í•¨. ì´ê²ƒì´ ì”ì°¨ ì—°ê²°ì˜ í•µì‹¬ ê°œë…ì´ë‹¤."
  },
  {
    "objectID": "posts/About_Transformers.html#ë””ì½”ë”-êµ¬ì¡°-ì—¬ëŸ¬-ê°œì˜-ë””ì½”ë”-ë ˆì´ì–´-ìŠ¤íƒ",
    "href": "posts/About_Transformers.html#ë””ì½”ë”-êµ¬ì¡°-ì—¬ëŸ¬-ê°œì˜-ë””ì½”ë”-ë ˆì´ì–´-ìŠ¤íƒ",
    "title": "About Transformers",
    "section": "",
    "text": "ë””ì½”ë”ë„ ì—¬ëŸ¬ ê°œì˜ ë””ì½”ë” ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ë©°, ì¸ì½”ë”ì™€ ë‹¤ë¥¸ ì ì€ ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ ë ˆì´ì–´ê°€ ì¶”ê°€ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\nê° ë””ì½”ë” ë ˆì´ì–´ëŠ” ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤.\n\n\në§ˆìŠ¤í¬ë“œ ë©€í‹°-í—¤ë“œ ì…€í”„ ì–´í…ì…˜ (Masked Multi-Head Self-Attention)\n\në””ì½”ë”ëŠ” ì •ë‹µ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì—ì„œ ì•ìª½ ë‹¨ì–´ë§Œ ì°¸ê³ í•  ìˆ˜ ìˆë„ë¡ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ì—¬ ë¯¸ë˜ ì •ë³´ë¥¼ ì°¨ë‹¨í•©ë‹ˆë‹¤.\në¯¸ë˜ ì •ë³´(ë¯¸ë˜ì— ë‚˜ì˜¬ ë‹¨ì–´)ë¥¼ ë¯¸ë¦¬ ì•ˆë‹¤ë©´ ì •ë‹µì§€ë¥¼ ë³´ê³  ì»¨ë‹ì„ í•˜ëŠ” ê²ƒê³¼ ê°™ê¸°ì— ì˜¬ë°”ë¡  í•™ìŠµì´ ë˜ ì§€ ì•ŠëŠ”ë‹¤.\në§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ë„ ì¸ì½”ë”ì˜ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ê³¼ ë˜‘ê°™ì´ Q,K,V ê°’ ê³„ì‚° -&gt; ì–´í…ì…˜ ê²°ê³¼ ì¶œë ¥\n\nì”ì°¨ ì—°ê²° + Layer Normalization\n\nì¸ì½”ë” êµ¬ì¡°ì—ì„œ ì„¤ëª…í•œ ê²ƒê³¼ ê°™ìŒ.\n\nì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ (Encoder-Decoder Attention)\n\nì¸ì½”ë”ì—ì„œ ì¶œë ¥ëœ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë¬¸ì¥ê³¼ í˜„ì¬ ë””ì½”ë” ìƒíƒœ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\në””ì½”ë”ë§Œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸(GPT)ëŠ” ì´ ì¸µì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. ì¸ì½”ë”ì—ì„œ ë‚˜ì˜¨ ì¶œë ¥ì„ ì´ìš©í•˜ê¸°ì— ì¸ì½”ë”ì™€ ë””ì½”ë”ê°€ ëª¨ë‘ ìˆëŠ” ëª¨ë¸ì—ì„œë§Œ ì´ ì¸µì„ ì‚¬ìš©í•œë‹¤.\n\nì”ì°¨ ì—°ê²° + Layer Normalization\n\n3ì—ì„œ ì¶”ê°€ëœ ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ ì¸µì´ ìˆê¸°ì— ì”ì°¨ ì—°ê²° + Layer Normalizaiton ì¸µì´ ì¶”ê°€ëœë‹¤.\n\ní”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (FFN)\nì”ì°¨ ì—°ê²° + Layer Normalization"
  },
  {
    "objectID": "posts/About_Transformers.html#ì¶œë ¥-ì²˜ë¦¬",
    "href": "posts/About_Transformers.html#ì¶œë ¥-ì²˜ë¦¬",
    "title": "About Transformers",
    "section": "",
    "text": "ì„ í˜• ë³€í™˜ (Linear Layer): ë””ì½”ë”ì—ì„œ ë‚˜ì˜¨ ì¶œë ¥ì„ ë‹¨ì–´ ì§‘í•© í¬ê¸°ë§Œí¼ì˜ ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\nì†Œí”„íŠ¸ë§¥ìŠ¤ (Softmax): í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•´ ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/About_Transformers.html#ìš”ì•½-ìˆœì„œ-ì •ë¦¬",
    "href": "posts/About_Transformers.html#ìš”ì•½-ìˆœì„œ-ì •ë¦¬",
    "title": "About Transformers",
    "section": "",
    "text": "ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ (ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©)\nì¸ì½”ë” (ì—¬ëŸ¬ ê°œì˜ ì¸ì½”ë” ë ˆì´ì–´) -ë©€í‹°-í—¤ë“œ ì…€í”„ ì–´í…ì…˜ â†’ ì”ì°¨ ì—°ê²° + ì •ê·œí™” â†’ FFN â†’ ì”ì°¨ ì—°ê²° + ì •ê·œí™”\në””ì½”ë” (ì—¬ëŸ¬ ê°œì˜ ë””ì½”ë” ë ˆì´ì–´) -ë§ˆìŠ¤í¬ë“œ ë©€í‹°-í—¤ë“œ ì…€í”„ ì–´í…ì…˜ â†’ ì”ì°¨ ì—°ê²° + ì •ê·œí™” -ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ â†’ ì”ì°¨ ì—°ê²° + ì •ê·œí™” -FFN â†’ ì”ì°¨ ì—°ê²° + ì •ê·œí™”\nì¶œë ¥ ë³€í™˜ (ì„ í˜• ë ˆì´ì–´ â†’ ì†Œí”„íŠ¸ë§¥ìŠ¤ â†’ ì˜ˆì¸¡)"
  },
  {
    "objectID": "posts/P_vs_P.html",
    "href": "posts/P_vs_P.html",
    "title": "Position Embedding vs Position Encoding",
    "section": "",
    "text": "1. ìœ„ì¹˜ ì„ë² ë”© vs ìœ„ì¹˜ ì¸ì½”ë”©\n- ìœ„ì¹˜ ì„ë² ë”©\nìœ„ì¹˜ë³„ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”©ì„ ë¶€ì—¬í•˜ëŠ” ë°©ì‹ (ë‹¨ì–´ë“¤ì˜ ìœ„ì¹˜ì— ë”°ë¼ í•™ìŠµ ê°€ëŠ¥í•œ ê°’ì„ ë¶€ì—¬)\n\nëª¨ë¸ì´ í•™ìŠµì„ í†µí•´ ìœ„ì¹˜ë³„ ì„ë² ë”© ê°’ì„ ì¡°ì •í•  ìˆ˜ ìˆë‹¤.\nì˜ˆë¥¼ ë“¤ì–´, ìœ„ì¹˜ 0ë²ˆ, 1ë²ˆ, 2ë²ˆ, â€¦ì— ëŒ€í•´ ê°ê° í•™ìŠµ ê°€ëŠ¥í•œ ë²¡í„°ê°€ ì¡´ì¬í•œë‹¤.\n\n\\(PE_{learned}(pos)=Embedding(pos)\\)\n- ìœ„ì¹˜ ì¸ì½”ë”©\nìœ„ì¹˜ ì •ë³´ë¥¼ ì‚¬ì¸(sin)ê³¼ ì½”ì‚¬ì¸(cos) í•¨ìˆ˜ë¡œ ìƒì„±í•˜ëŠ” ë°©ì‹ì´ì•¼. (ë‹¨ì–´ë“¤ì˜ ìœ„ì¹˜ì— ë”°ë¼ í•™ìŠµ ë¶ˆê°€ëŠ¥í•œ ìƒìˆ˜ ê°’ì„ ë¶€ì—¬\n\nì‚¬ì „ ì •ì˜ëœ í•¨ìˆ˜(ìˆ˜í•™ì  íŒ¨í„´)ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë³€í™”í•˜ì§€ ì•ŠëŠ” ìƒìˆ˜ ê°’\ní•™ìŠµì„ í†µí•´ ì¡°ì •ë˜ì§€ ì•Šê³ , ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ê³ ì •ëœ ê°’ì„ ì‚¬ìš©í•œë‹¤.\n\n\\(PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\\)\n\\(PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\\)\n\nposëŠ” ë‹¨ì–´ì˜ ìœ„ì¹˜,\niëŠ” ì„ë² ë”© ì°¨ì›ì˜ ì¸ë±ìŠ¤\ndëŠ” ì„ë² ë”© ì°¨ì› í¬ê¸°\n\n\n\n2. ê° ë°©ì‹ì˜ ì¥ë‹¨ì \nGood ìœ„ì¹˜ ì„ë² ë”© ì¥ì  1. í•™ìŠµ ê°€ëŠ¥: ìœ„ì¹˜ ì„ë² ë”©ì€ í•™ìŠµì´ ê°€ëŠ¥í•˜ê¸°ì— ëª¨ë¸ì´ ë°ì´í„°ì— ë§ì¶° ìœ„ì¹˜ ì •ë³´ë¥¼ ìµœì í™”í•  ìˆ˜ ìˆë‹¤. 2. ë” ë†’ì€ ìœ ì—°ì„±: ëª¨ë¸ì´ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•˜ë¯€ë¡œ, íŠ¹ì • ë¬¸ë§¥ì´ë‚˜ ë„ë©”ì¸ì— ë§ì¶° ìœ„ì¹˜ í‘œí˜„ì„ ë‹¤ë¥´ê²Œ í•  ìˆ˜ ìˆë‹¤.\nBad ìœ„ì¹˜ ì„ë² ë”© ë‹¨ì  1. ë©”ëª¨ë¦¬ì™€ ì—°ì‚° ë¹„ìš©: ìœ„ì¹˜ ì„ë² ë”©ì€ ê° ìœ„ì¹˜ë§ˆë‹¤ ì„ë² ë”© ë²¡í„°ë¥¼ í•™ìŠµí•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ì¶”ê°€ì ì¸ íŒŒë¼ë¯¸í„°ì™€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•œë‹¤. 2. ì˜¤ë²„í”¼íŒ…: í•™ìŠµ ê°€ëŠ¥í•œ ìœ„ì¹˜ ë²¡í„°ê°€ ë§ì•„ì§€ë©´, ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ê³¼ì í•©í•  ê°€ëŠ¥ì„±ì´ ì»¤ì§ˆ ìˆ˜ ìˆë‹¤.\nGood ìœ„ì¹˜ ì¸ì½”ë”© ì¥ì  1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: íŒŒë¼ë¯¸í„°ê°€ ì—†ê¸°ì— ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ ì ë‹¤. 2. ê³ ì •ëœ íŒ¨í„´: ìœ„ì¹˜ ì¸ì½”ë”©ì€ ê³ ì •ëœ ìˆ˜í•™ì  íŒ¨í„´ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, íŠ¹ì • ìœ„ì¹˜ ê°„ì˜ ê´€ê³„ë¥¼ ëª…í™•í•˜ê²Œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\nBad ìœ„ì¹˜ ì¸ì½”ë”© ë‹¨ì  1. í•™ìŠµí•  ìˆ˜ ì—†ìŒ: ëª¨ë¸ì´ í•™ìŠµì„ í†µí•´ ìœ„ì¹˜ ì •ë³´ë¥¼ ë” ì •êµí•˜ê²Œ ì¡°ì •í•  ìˆ˜ ì—†ë‹¤. 2. ìœ ì—°ì„± ë¶€ì¡±: ëª¨ë“  ë¬¸ì¥ì—ì„œ ê°™ì€ íŒ¨í„´ì„ ì‚¬ìš©í•˜ê¸°ì— íŠ¹ì • ë„ë©”ì¸ì´ë‚˜ ë°ì´í„° ì…‹ì— ëŒ€í•´ ìµœì í™”ëœ ìœ„ì¹˜ ì •ë³´ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ë‹¤.\n\n\n3. ê²°ë¡ \n\nìœ„ì¹˜ ì¸ì½”ë”©ì€ ì¼ë°˜ì ì¸ ìš©ë„ì™€ íš¨ìœ¨ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°ì— ì í•©í•˜ë‹¤.\nìœ„ì¹˜ ì„ë² ë”©ì€ íŠ¹ì • ë„ë©”ì¸ì´ë‚˜ ê³ ìœ í•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ ë” ì •êµí•œ ìœ„ì¹˜ í‘œí˜„ì´ í•„ìš”í•œ ê²½ìš°ì— ìœ ë¦¬í•˜ë‹¤!"
  },
  {
    "objectID": "posts/BPE.html",
    "href": "posts/BPE.html",
    "title": "Byte Pair Encoding",
    "section": "",
    "text": "1. Byte Pair Encoding?\nBPE(Byte Pair Encoding) ëŠ” ê¸´ ë‹¨ì–´ë‚˜ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì‘ì€ ë‹¨ìœ„(ì¡°ê°)ë¡œ ìª¼ê°œì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í† í°í™” ê¸°ë²•ì´ë‹¤.\nì˜ˆë¥¼ ë“¤ì–´ â€œlowestâ€ ê°™ì€ ë‹¨ì–´ëŠ” â€œlowâ€ + â€œestâ€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆê³ ,\nì²˜ìŒ ë³´ëŠ” ë‹¨ì–´ â€œslowestestestestestâ€ëŠ” â†’ â€œsâ€, â€œlâ€, â€œoâ€, â€œwâ€, â€œestâ€, â€œestâ€, â€œestâ€, â€œestâ€, â€œestâ€ ì²˜ëŸ¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ëª¨ë¥´ëŠ” ë‹¨ì–´ë„ ì¼ë¶€ ì¡°ê°ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ê²Œ ë©ë‹ˆë‹¤.\nByte Pair Encodingì„ ì´ìš©í•´ì„œ Corpusë¥¼ í† í°í™”í•˜ëŠ” ê³¼ì •ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\nBPEëŠ” vocabularyë¥¼ êµ¬ì¶•í•˜ëŠ” ë‹¨ê³„ì¸ Trainë‹¨ê³„, êµ¬ì¶•ëœ vocabularyì„ ì´ìš©í•´ì„œ ìƒˆë¡œìš´ Corpusë¥¼ í† í°í™”í•˜ëŠ” Infer ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ì„œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n\n\n2. Imports\n\nimport re\nfrom collections import defaultdict\n\n\n\n3. train\n\n# 1-1. load file\ndef load_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return f.read()\n        \n# 1-2. calculate best pair\ndef get_char_vocab(vocab):\n    char_vocab = defaultdict(int)\n    for word in vocab:\n        word_tuple = tuple(word)\n        for i in range(len(word_tuple) - 1):\n            pair = (word_tuple[i], word_tuple[i + 1])  # ì¸ì ‘ ë¬¸ììŒ\n            char_vocab[pair] += 1\n    return char_vocab\n\n# 1-3. merge best pair\ndef merge_best_pair(tokenized_vocab, best_pair):\n    new_tokenized_vocab = []\n    \n    for word in tokenized_vocab:\n        new_word = []\n        i = 0\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == best_pair:\n                new_word.append(''.join(best_pair))  # ë³‘í•©ëœ ë¬¸ì ì¶”ê°€\n                i += 2  # ë‘ ê°œë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©í–ˆìœ¼ë¯€ë¡œ 2 ì¦ê°€\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_tokenized_vocab.append(new_word)\n\n    return new_tokenized_vocab\n\n# 1-4. BPE Algorithm execution\ndef bpe_algorithm(file_path, max_vocab):\n    text = load_file(file_path)\n    vocab = set() # ë¹ˆ vocab ë¦¬ìŠ¤íŠ¸\n    words = text.split() \n    tokenized_vocab = [list(word) for word in words]  # ë¬¸ì ë‹¨ìœ„ë¡œ ë³€í™˜\n    \n    while len(vocab) &lt; max_vocab:\n        char_vocab = get_char_vocab(tokenized_vocab)\n        \n        if not char_vocab:  # ë” ì´ìƒ ë³‘í•©í•  ê²ƒì´ ì—†ìœ¼ë©´ ì¢…ë£Œ\n            break\n        \n        best_pair = max(char_vocab, key=char_vocab.get)  # ê°€ì¥ ë¹ˆë²ˆí•œ ë¬¸ììŒ ì„ íƒ\n        tokenized_vocab = merge_best_pair(tokenized_vocab, best_pair)  # ë³‘í•© ì‹¤í–‰\n        vocab.add(''.join(best_pair))  # ë³‘í•©ëœ ë¬¸ììŒì„ vocabì— ì¶”ê°€\n\n         # 50ê°œì”© vocab í¬ê¸°ê°€ ì¦ê°€í•  ë•Œë§ˆë‹¤ ì¶œë ¥\n        if len(vocab) % 50 == 0:\n            print(f\"Current vocab size: {len(vocab)}/{max_vocab}\")\n\n    return list(vocab), tokenized_vocab\n\n# 1-5. execution\nfile_path = 'pg100.txt'\nmax_vocab = 1  # ìµœëŒ€ vocab í¬ê¸° ì„¤ì •\nfinal_vocab, final_tokenized_vocab = bpe_algorithm(file_path, max_vocab) # final_tokenized_vocabëŠ” ë³‘í•©ëœ ë‹¨ì–´ê°€ ê³„ì† ìŒ“ì•„ê°€ëŠ” ìƒí™©ì„ ë³´ì—¬ì£¼ê¸°ì— ê²°ê³¼ê°€ ê¸¸ê³  ì§€ì €ë¶„í•˜ë‹¤. ê·¸ë˜ì„œ ë³´ì´ê²Œ í•˜ì§€ ì•ŠëŠ” ê²ƒì´ë‹¤.\n\n# 1-6. result\nprint(f\"ìµœì¢… vocab í¬ê¸°: {len(final_vocab)}\")\nprint(f\"ì¼ë¶€ vocab: {final_vocab[:20]}\")\n\n# 1-7. make vocab\nwith open('vocab.txt', 'w', encoding='utf-8') as f:\n    f.write(\"\\n\".join(final_vocab))\n\n\n\n4. infer\n\n# 2-1. Vocab ë¶ˆëŸ¬ì˜¤ê¸°\ndef load_vocab(vocab_file):\n    with open(vocab_file, 'r', encoding='utf-8') as f:\n        return set(line.strip() for line in f)  # ì¤„ë°”ê¿ˆ ì œê±° í›„ ì§‘í•©ìœ¼ë¡œ ì €ì¥\n\n# 2-2. ì…ë ¥ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\ndef load_text(input_file):\n    with open(input_file, 'r', encoding='utf-8') as f:\n        return f.read()\n\n# # 2-3. BPE ê¸°ë°˜ í† í°í™” ìˆ˜í–‰\ndef tokenize(text, vocab):\n    words = text.split()  # ê³µë°± ê¸°ì¤€ ë‹¨ì–´ ë¶„ë¦¬\n    tokenized_output = [] # í† í°í™” ëœ ê²ƒì„ ì €ì¥í•˜ê¸° ìœ„í•œ ë¹ˆ ë¦¬ìŠ¤íŠ¸\n\n    for word in words:\n        subword_tokens = []\n        current = word\n\n        while current:\n            # ê°€ëŠ¥í•œ ê°€ì¥ ê¸´ ì„œë¸Œì›Œë“œë¥¼ ì°¾ìŒ\n            for i in range(1,len(current)+1)[::-1]: # ex) abcd -&gt; 4,3,2,1 ìˆœìœ¼ë¡œ ì¸ë±ìŠ¤\n                subword = current[:i]  # ì˜¤ë¥¸ìª½ì˜ ì•ŒíŒŒë²³ì„ í•˜ë‚˜ì”© ë–¼ë©´ì„œ vocabì— ë‹¨ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸\n                if subword in vocab: # ì˜¤ë¥¸ìª½ ì•ŒíŒŒë²³ í•˜ë‚˜ì”© ë–¼ë‹¤ê°€ vocabì— ê°™ì€ ê±° ë°œê²¬í•œë‹¤ë©´!\n                    subword_tokens.append(subword) # subword_tokensì— ì¶”ê°€\n                    current = current[i:]  # ë—ë˜ ë¶€ë¶„ì—ì„œ vocabì— ë‹¨ì–´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë‹¤ì‹œ í™•ì¸í•˜ê¸° ìœ„í•´ currentë¡œ ì €ì¥\n                    break\n            else:  # for loopë¥¼ ëŒì•˜ì§€ë§Œ breakê°€ ë°œìƒí•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ else ì‹¤í–‰\n                # í•´ë‹¹í•˜ëŠ” ì„œë¸Œì›Œë“œê°€ vocabì— ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n                subword_tokens.append(current[0]) # ì²« ê¸€ì ë¶„ë¦¬\n                current = current[1:] # ì²« ê¸€ì ë–¼ê³  ë‚˜ë¨¸ì§€ëŠ” ë‹¤ì‹œ for loop ì‹¤í–‰\n\n        # ì²« ë²ˆì§¸ í† í°ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì„œë¸Œì›Œë“œì— '##' ì¶”ê°€\n        for i in range(1, len(subword_tokens)):\n            subword_tokens[i] = '##' + subword_tokens[i]\n\n        tokenized_output.extend(subword_tokens) # .extend() -&gt; ë¦¬ìŠ¤íŠ¸ì— ì›ì†Œë¥¼ ì¶”ê°€ \n\n    return ' '.join(tokenized_output) # ê³µë°±ì„ ì¶”ê°€í•˜ì—¬ í•˜ë‚˜ë¡œ ì •ë¦¬\n\n# 2-5. ê²°ê³¼ ì €ì¥\ndef save_output(output_text, output_file):\n    with open(output_file, 'w', encoding='utf-8') as f:\n        f.write(output_text)\n\n# 2-6. ì‹¤í–‰\nvocab_file = 'vocab.txt'  # ì €ì¥ëœ BPE vocab\ninput_file = 'infer.txt'  # í† í°í™”í•  ì…ë ¥ í…ìŠ¤íŠ¸\noutput_file = 'output.txt'  # ê²°ê³¼ ì €ì¥í•  íŒŒì¼\n\nvocab = load_vocab(vocab_file)\ntext = load_text(input_file)\ntokenized_text = tokenize(text, vocab)\nsave_output(tokenized_text, output_file)\n\nprint(\"Tokenization completed. Check\", output_file)\n\nTokenization completed. Check output.txt"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Natural language processing",
    "section": "",
    "text": "Byte Pair Encoding\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nTransformer architecture\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nModel fine tuning\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nBase on Encoder-Decoder models\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nBase on Encoder models\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nBase on Decoder models\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nPosition Embedding vs Position Encoding\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Transformers\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nText generation\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nNLP Sampling\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nText Classification Fine Tuning\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluate\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2025\n\n\nì°¨ìƒì§„\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Text_generation.html",
    "href": "posts/Text_generation.html",
    "title": "Text generation",
    "section": "",
    "text": "1. ê·¸ë¦¬ë”” ì„œì¹˜ ë””ì½”ë”©\n- ê·¸ë¦¬ë”” ì„œì¹˜ ë””ì½”ë”©ì˜ ì´í•´\nì²˜ìŒ ë¬¸ì¥(\\(x=x_1,...,x_k\\)) ì´ ì£¼ì–´ì§ˆ ë•Œ í…ìŠ¤íŠ¸ì— ë“±ì¥í•˜ëŠ” í† í° ì‹œí€€ìŠ¤ (\\(y=y_1,...,y_t\\))ì˜ í™•ë¥  \\(P(y|x)\\)ë¥¼ ì¶”ì •í•˜ë„ë¡ ì‚¬ì „ í›ˆë ¨ëœë‹¤.\ní•˜ì§€ë§Œ ì§ì ‘ \\(P(y|x)\\)ì„ ì¶”ì •í•˜ë ¤ë©´ ë°©ëŒ€í•œ ì–‘ì˜ í›ˆë ¨ë°ì´í„°ê°€ í•„ìš”í•˜ë¯€ë¡œ ì—°ì‡„ë²•ì¹™(Chain Rule of Probability) ì„ ì‚¬ìš©í•´ ì¡°ê±´ë¶€ í™•ë¥ ì˜ ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.\n\\[\nP(y_1, ..., y_t | x) = \\prod_{t=1}^{N} P(y_t | y_{&lt;t}, x)\n\\]\nê³„ì‚°ëœ í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ, ê° ì‹œì  tì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ì—¬ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•œë‹¤.\n- ì•„ë˜ëŠ” ê·¸ë¦¬ë”” ì„œì¹˜ ë””ì½”ë”©ì˜ êµ¬í˜„ì´ë‹¤.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n2025-03-17 17:53:47.780280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1742234027.798227   18290 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1742234027.803855   18290 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1742234027.817808   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1742234027.817823   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1742234027.817825   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1742234027.817827   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-03-17 17:53:47.822310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nimport pandas as pd\n\ninput_txt = \"Transformers are the\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\niterations = [] # ìŠ¤í…ë³„ë¡œ ì˜ˆì¸¡ëœ ë‹¨ì–´ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\nn_steps = 8 # ìµœëŒ€ 8ê°œì˜ ë‹¨ì–´ë¥¼ ì¶”ê°€ ìƒì„±\nchoices_per_step = 5 # ê° ìŠ¤í…ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ 5ê°œì˜ ë‹¨ì–´ë¥¼ ì €ì¥\n\nwith torch.no_grad(): # í•™ìŠµì´ ì•„ë‹ˆë¼ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ gradient ê³„ì‚°ì„ ë¹„í™œì„±í™”\n    for _ in range(n_steps): \n        iteration = dict() \n        iteration[\"Input\"] = tokenizer.decode(input_ids[0]) # ë”•ì…”ë„ˆë¦¬ ìƒì„± í›„ í˜„ì¬ê¹Œì§€ì˜ ë¬¸ì¥ì„ ì €ì¥\n        output = model(input_ids=input_ids) # í˜„ì¬ ë¬¸ì¥ì„ ëª¨ë¸ì— ì…ë ¥í•˜ë ¤ ë‹¤ìŒ ë‹¨ì–´ì˜ í™•ë¥ ì„ ì–»ìŒ.\n        # ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ë§ˆì§€ë§‰ í† í°ì˜ ë¡œì§“ì„ ì„ íƒí•´ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n        next_token_logits = output.logits[0, -1, :]\n        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n        # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í°ì„ ì €ì¥í•©ë‹ˆë‹¤.\n        for choice_idx in range(choices_per_step):\n            token_id = sorted_ids[choice_idx]\n            token_prob = next_token_probs[token_id].cpu().numpy()\n            token_choice = (\n                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n            )\n            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n        # ì˜ˆì¸¡í•œ ë‹¤ìŒ í† í°ì„ ì…ë ¥ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n        iterations.append(iteration)\n\npd.DataFrame(iterations)\n\n\n\n\n\n\n\n\n\nInput\nChoice 1\nChoice 2\nChoice 3\nChoice 4\nChoice 5\n\n\n\n\n0\nTransformers are the\nmost (8.53%)\nonly (4.96%)\nbest (4.65%)\nTransformers (4.37%)\nultimate (2.16%)\n\n\n1\nTransformers are the most\npopular (16.78%)\npowerful (5.37%)\ncommon (4.96%)\nfamous (3.72%)\nsuccessful (3.20%)\n\n\n2\nTransformers are the most popular\ntoy (10.63%)\ntoys (7.23%)\nTransformers (6.60%)\nof (5.46%)\nand (3.76%)\n\n\n3\nTransformers are the most popular toy\nline (34.38%)\nin (18.20%)\nof (11.71%)\nbrand (6.10%)\nline (2.69%)\n\n\n4\nTransformers are the most popular toy line\nin (46.28%)\nof (15.09%)\n, (4.94%)\non (4.40%)\never (2.72%)\n\n\n5\nTransformers are the most popular toy line in\nthe (65.99%)\nhistory (12.42%)\nAmerica (6.91%)\nJapan (2.44%)\nNorth (1.40%)\n\n\n6\nTransformers are the most popular toy line in the\nworld (69.26%)\nUnited (4.55%)\nhistory (4.29%)\nUS (4.23%)\nU (2.30%)\n\n\n7\nTransformers are the most popular toy line in ...\n, (39.73%)\n. (30.64%)\nand (9.87%)\nwith (2.32%)\ntoday (1.74%)\n\n\n\n\n\n\n\n\n\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\nprint(tokenizer.decode(output[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nTransformers are the most popular toy line in the world,\n\n\n\nmax_length = 128\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput_greedy = model.generate(input_ids, max_length=max_length,\n                               do_sample=False)\nprint(tokenizer.decode(output_greedy[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n\n\nThe researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n\n\nThe researchers were surprised to find that the unicorns were able\n\n\n- ê·¸ë¦¬ë”” ì„œì¹˜ ë””ì½”ë”©ì€ ê° íƒ€ì„ìŠ¤í…ì—ì„œ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í† í°ì„ íƒìš•ì (greedily)ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ë°©ì‹ì´ë‹¤.\ní•˜ì§€ë§Œ ì´ê²ƒì€ ì‚¬ì‹¤ ìµœì ì˜ ë””ì½”ë”© ë°©ì‹ì´ ì•„ë‹ˆë‹¤. ê°€ì¥ ìµœì ì˜ ë°©ì‹ì€ ê°€ëŠ¥í•œ ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¡œ ë¬¸ì¥ì„ ì™„ì„±ì‹œì¼œë†“ê³  ê·¸ í›„ì— ê·¸ ë¬¸ì¥ì„ íŒë‹¨í•˜ì—¬ ê°€ì¥ ì¢‹ì€ ë¬¸ì¥ì„ ì„ íƒí•˜ëŠ” ë°©ì‹ì´ë‹¤.\n\ní•˜ì§€ë§Œ ê·¸ê²ƒì€ ë„ˆë¬´ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ë¬¸ì œê°€ ìˆì–´ì„œ ê·¸ë¦¬ë”” ì„œì¹˜ ë””ì½”ë”© ë°©ì‹ì´ ì—°êµ¬ë˜ì—ˆì§€ë§Œ ì´ ë°©ì‹ ë˜í•œ ë°˜ë³µì ì¸ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. ì´ë¡œ ì¸í•´ ìµœì ì˜ ì†”ë£¨ì…˜ì„ ë§Œë“¤ê¸°ëŠ” ì–´ë µë‹¤.\n\n\n2. ë¹” ì„œì¹˜ ë””ì½”ë”©\n- ë¹” ì„œì¹˜ëŠ” ê° ìŠ¤í…ì—ì„œ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í† í°ì„ ë””ì½”ë”©í•˜ëŠ” ëŒ€ì‹ , í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ìƒìœ„ bê°œì˜ ë‹¤ìŒ í† í°ì„ ì¶”ì í•œë‹¤.\në¹” ì„¸íŠ¸ëŠ” ê¸°ì¡´ ì„¸íŠ¸ì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  ë‹¤ìŒ í† í°ì„ í™•ì¥í•œ í›„ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ bê°œì˜ í™•ì¥ì„ ì„ íƒí•˜ì—¬ êµ¬ì„±í•œë‹¤.\n\nì´ ê³¼ì •ì€ ìµœëŒ€ ê¸¸ì´ë‚˜ EOSí† í°ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µëœë‹¤.\n- ë¡œê·¸í™•ë¥ ì„ ì´ìš©í•˜ëŠ” ì´ìœ \në¹”ì„œì¹˜ëŠ” ë‹¤ìŒ ë‹¨ì–´ì˜ í™•ë¥ ì„ ê³„ì‚°í•  ë•Œ ê¸°ì¡´ì˜ ê³±ìœ¼ë¡œ ì—°ê²°ë˜ë˜ í™•ë¥ ì´ ì•„ë‹Œ ë¡œê·¸ë¥¼ ì·¨í•œ í™•ë¥ ì„ ì´ìš©í•œë‹¤. ì¦‰ ë¡œê·¸í™•ë¥ ì„ ì´ìš©í•œë‹¤.\n\nê·¸ ì´ìœ ëŠ” ê³±ì…ˆì—ì„œ ì‚¬ìš©ë˜ëŠ” ê° ì¡°ê±´ë¶€ í™•ë¥ ì€ 0ê³¼ 1ì‚¬ì´ì— ìˆëŠ” ì‘ì€ ê°’ì´ë‹¤. ì´ ê°’ë“¤ì€ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ì¡°ê¸ˆë§Œ ê¸¸ì–´ì§€ë©´ ì „ì²´ í™•ë¥ ì´ 0ìœ¼ë¡œ ê°€ê¹ê²Œ ë˜ëŠ” underfolowê°€ ì‰½ê²Œ ë°œìƒí•œë‹¤.\n\nì•„ì£¼ ì‘ì€ ê°’ì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë¶ˆì•ˆì •í•˜ê¸°ì— í™•ë¥ ì— logë¥¼ ì·¨í•´ì£¼ë©´ ê³±ì…ˆì´ ë§ì…ˆìœ¼ë¡œ ë°”ë€Œê¸°ì— ì‹ì´ ì•ˆì •í™”ëœë‹¤. ì´ëŸ° ê°’ì€ ë‹¤ë£¨ê¸° í›¨ì”¬ ì‰½ë‹¤.\n\nì¶”ê°€ì ìœ¼ë¡œ logëŠ” ë‹¨ì¡°ì¦ê°€í•¨ìˆ˜ë¡œì„œ í™•ë¥ í¬ê¸°ë¥¼ ë¹„êµë§Œí•˜ë©´ ë˜ê¸°ì— logë¥¼ ì·¨í•´ë„ í™•ë¥ ê°„ì— ëŒ€ì†Œê´€ê³„ëŠ” ë‹¬ë¼ì§€ì§€ ì•Šê¸°ì— ë¡œê·¸í™•ë¥ ì„ ì‚¬ìš©í•´ë„ ìƒê´€ì—†ë‹¤.\n\n# ì´ í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ logitsì—ì„œ íŠ¹ì • labelsì— í•´ë‹¹í•˜ëŠ” ë¡œê·¸ í™•ë¥ ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.\n\nimport torch.nn.functional as F\n\ndef log_probs_from_logits(logits, labels):\n    logp = F.log_softmax(logits, dim=-1)\n    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n    # unsqueeze(2)ë¥¼ ì´ìš©í•´ì„œ logpê³¼ labelsì˜ ì°¨ì›ì„ ë§ì¶°ì¤Œ. ê·¸ í›„ì— squeezeë¥¼ ì´ìš©í•´ì„œ í¬ê¸°ê°€ 1ì¸ ì°¨ì›ì„ ì—†ì•¤ë‹¤.\n    return logp_label\n\n\n# ì´ í•¨ìˆ˜ëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•œ logitsì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë¬¸ì¥ì˜ ë¡œê·¸ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.\n\ndef sequence_logprob(model, labels, input_len=0):\n    with torch.no_grad():\n        output = model(labels)\n        log_probs = log_probs_from_logits(\n            output.logits[:, :-1, :], labels[:, 1:]) # labels[:,1:]ëŠ” ì •ë‹µ, output.logits[:,:-1,:]ì€ ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ì œì™¸í•œ ëª¨ë“  ë‹¨ì–´ì˜ ë¡œì§“\n        seq_log_prob = torch.sum(log_probs[:, input_len:]) #input_lenë§Œí¼ì˜ í™•ë¥ ì„ ë¬´ì‹œí•˜ê³  ë”í•œë‹¤.\n    return seq_log_prob.cpu().numpy()\n\n1 ê·¸ë¦¬ë”” ì„œì¹­ ë””ì½”ë”©ìœ¼ë¡œ ë§Œë“  ì‹œí€€ìŠ¤ ë¡œê·¸í™•ë¥  ê³„ì‚°\n\nlogp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_greedy[0]))\nprint(f\"\\në¡œê·¸ í™•ë¥ : {logp:.2f}\")\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a\nremote, previously unexplored valley, in the Andes Mountains. Even more\nsurprising to the researchers was the fact that the unicorns spoke perfect\nEnglish.\n\n\nThe researchers, from the University of California, Davis, and the University of\nColorado, Boulder, were conducting a study on the Andean cloud forest, which is\nhome to the rare species of cloud forest trees.\n\n\nThe researchers were surprised to find that the unicorns were able to\ncommunicate with each other, and even with humans.\n\n\nThe researchers were surprised to find that the unicorns were able\n\në¡œê·¸ í™•ë¥ : -87.43\n\n\n2 ë¹” ì„œì¹˜ ë””ì½”ë”©ìœ¼ë¡œ ë§Œë“  ì‹œí€€ìŠ¤ ë¡œê·¸í™•ë¥  ê³„ì‚°\n\noutput_beam = model.generate(input_ids, max_length=max_length, num_beams=5, # num_beamsì„ ì„¤ì •í•˜ë©´ ë¹” ì„œì¹˜ ë””ì½”ë”©ì´ í™œì„±í™” ëœë‹¤. (ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ 5ë¬¸ì¥ì„ ë™ì‹œíƒìƒ‰)\n                             do_sample=False) # do_sample=FalseëŠ” ìƒ˜í”Œë§ì„ í•˜ì§€ ì•Šê³  ê²°ì •ë¡ ì  ë°©ì‹ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ì„ íƒ(í•­ìƒ ê°™ì€ ë¬¸ì¥ì´ ìƒì„±ë¨)\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_beam[0]))\nprint(f\"\\në¡œê·¸ í™•ë¥ : {logp:.2f}\")\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a\nremote, previously unexplored valley, in the Andes Mountains. Even more\nsurprising to the researchers was the fact that the unicorns spoke perfect\nEnglish.\n\n\nThe discovery of the unicorns was made by a team of scientists from the\nUniversity of California, Santa Cruz, and the National Geographic Society.\n\n\nThe scientists were conducting a study of the Andes Mountains when they\ndiscovered a herd of unicorns living in a remote, previously unexplored valley,\nin the Andes Mountains. Even more surprising to the researchers was the fact\nthat the unicorns spoke perfect English\n\në¡œê·¸ í™•ë¥ : -55.23\n\n\n- ë¹” ì„œì¹˜ ë””ì½”ë”© no_repeat_ngram_size ì˜µì…˜ í™œì„±í™”\nno_repeat_ngram_sizeì€ ë¹” ì„œì¹˜ë„ í…ìŠ¤íŠ¸ê°€ ë°˜ë³µë˜ëŠ” ë¬¸ì œê°€ ìˆê¸°ì— ê·¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ n-ê·¸ë¨ í˜ë„í‹°ë¥¼ ë¶€ê³¼í•˜ëŠ” ì˜µì…˜ì´ë‹¤.\n\noutput_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n                             do_sample=False, no_repeat_ngram_size=2) # ìƒì„±ëœ ë¬¸ì¥ì—ì„œ ë™ì¼í•œ ì—°ì†ëœ nê°œì˜ ë‹¨ì–´ê°€ ë°˜ë³µë˜ì§€ ì•Šë„ë¡ ì œí•œí•˜ëŠ” ì˜µì…˜.\n                             # ë¹” ì„œì¹˜ë„ í…ìŠ¤íŠ¸ê°€ ë°˜ë³µë˜ëŠ” ë¬¸ì œê°€ ìˆê¸°ì— no_repeat_ngram_sizeì„ ì„¤ì •í•œë‹¤.\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_beam[0]))\nprint(f\"\\në¡œê·¸ í™•ë¥ : {logp:.2f}\")\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a\nremote, previously unexplored valley, in the Andes Mountains. Even more\nsurprising to the researchers was the fact that the unicorns spoke perfect\nEnglish.\n\n\nThe discovery was made by a team of scientists from the University of\nCalifornia, Santa Cruz, and the National Geographic Society.\n\nAccording to a press release, the scientists were conducting a survey of the\narea when they came across the herd. They were surprised to find that they were\nable to converse with the animals in English, even though they had never seen a\nunicorn in person before. The researchers were\n\në¡œê·¸ í™•ë¥ : -93.12\n\n\nì ìˆ˜ëŠ” ë‚®ì•„ì¡Œì§€ë§Œ í…ìŠ¤íŠ¸ê°€ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê¸°ì— ê²°ê³¼ëŠ” ì¢‹ë‹¤!\n! ì°¸ê³ ë¡œ ë‹¹ì—°íˆ ê° ì‹œì ì—ì„œì˜ ë¡œê·¸í™•ë¥ ì€ ìŒìˆ˜ì´ë‹¤ (0ê³¼ 1ì‚¬ì´ì˜ ê°’ì— ë¡œê·¸ë¥¼ ì·¨í•˜ë©´ ìŒìˆ˜ì´ê¸°ì—â€¦)\ní•˜ì§€ë§Œ 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì ˆëŒ“ê°’ì€ ë” ì‘ì•„ì§€ê¸°ì— ì‹œí€€ìŠ¤ì˜ ë¡œê·¸í™•ë¥ (ëª¨ë‘ ë”í•œ ê°’)ì€ 0ì— ê°€ê¹Œìš°ë©´ ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ì€ ê²ƒì´ë‹¤. (ì–‘ìˆ˜ëŠ” ë¶ˆê°€ëŠ¥)"
  },
  {
    "objectID": "posts/Evaluate.html",
    "href": "posts/Evaluate.html",
    "title": "Evaluate",
    "section": "",
    "text": "Evaluate í´ë˜ìŠ¤ëŠ” accuracy, F1 score, precision, recall, BLEU, ROUGE ë“±ë“± ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ ê°„ë‹¨í•˜ê²Œ ë¶ˆëŸ¬ì™€ í™œìš©í•  ìˆ˜ ìˆë‹¤.\n\nimport evaluate\nacc = evaluate.load('accuracy')\n\n\n\n\nê³„ì‚°ì„ í•´ë³´ëŠ” ì˜ˆì‹œì½”ë“œì´ë‹¤.\n\nmetrics = evaluate.combine(['accuracy','f1','precision','recall'])\nmetrics.compute(predictions=[1,0,0,1], references = [0,1,0,1])\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n\nfor y,pred in zip([0,1,0,1],[1,0,0,1]):\n    metrics.add(predictions=pred, references=y)\nmetrics.compute()\n\n# .add ë©”ì†Œë“œëŠ” ì…ë ¥ë°›ëŠ” ê°’ë“¤ì´ ìŠ¤ì¹¼ë¼ ê°’ì´ì–´ì•¼ í•œë‹¤. í•œ ë²ˆì— í•˜ë‚˜ì˜ ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µì„ ì¶”ê°€.\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n- zip()ì„ ì‚¬ìš©í•˜ì—¬ references(ì •ë‹µ)ê³¼ predictions(ì˜ˆì¸¡ê°’)ë¥¼ ë¬¶ìŒ\n\nfor y,preds in zip([[0,1],[0,1]],[[1,0],[0,1]]):\n    metrics.add_batch(predictions=preds, references=y)\nmetrics.compute()\n\n# .add_batch ë©”ì†Œë“œëŠ” ì…ë ¥ë°›ëŠ” ê°’ë“¤ì´ ë¦¬ìŠ¤íŠ¸(ë°°ì¹˜ë‹¨ìœ„)ì—¬ì•¼ í•œë‹¤. í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì˜ ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µì„ ì¶”ê°€\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}"
  },
  {
    "objectID": "posts/Evaluate.html#evaluate",
    "href": "posts/Evaluate.html#evaluate",
    "title": "Evaluate",
    "section": "",
    "text": "Evaluate í´ë˜ìŠ¤ëŠ” accuracy, F1 score, precision, recall, BLEU, ROUGE ë“±ë“± ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ ê°„ë‹¨í•˜ê²Œ ë¶ˆëŸ¬ì™€ í™œìš©í•  ìˆ˜ ìˆë‹¤.\n\nimport evaluate\nacc = evaluate.load('accuracy')\n\n\n\n\nê³„ì‚°ì„ í•´ë³´ëŠ” ì˜ˆì‹œì½”ë“œì´ë‹¤.\n\nmetrics = evaluate.combine(['accuracy','f1','precision','recall'])\nmetrics.compute(predictions=[1,0,0,1], references = [0,1,0,1])\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n\nfor y,pred in zip([0,1,0,1],[1,0,0,1]):\n    metrics.add(predictions=pred, references=y)\nmetrics.compute()\n\n# .add ë©”ì†Œë“œëŠ” ì…ë ¥ë°›ëŠ” ê°’ë“¤ì´ ìŠ¤ì¹¼ë¼ ê°’ì´ì–´ì•¼ í•œë‹¤. í•œ ë²ˆì— í•˜ë‚˜ì˜ ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µì„ ì¶”ê°€.\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n- zip()ì„ ì‚¬ìš©í•˜ì—¬ references(ì •ë‹µ)ê³¼ predictions(ì˜ˆì¸¡ê°’)ë¥¼ ë¬¶ìŒ\n\nfor y,preds in zip([[0,1],[0,1]],[[1,0],[0,1]]):\n    metrics.add_batch(predictions=preds, references=y)\nmetrics.compute()\n\n# .add_batch ë©”ì†Œë“œëŠ” ì…ë ¥ë°›ëŠ” ê°’ë“¤ì´ ë¦¬ìŠ¤íŠ¸(ë°°ì¹˜ë‹¨ìœ„)ì—¬ì•¼ í•œë‹¤. í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì˜ ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µì„ ì¶”ê°€\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}"
  },
  {
    "objectID": "posts/Evaluate.html#create-custom-metrics",
    "href": "posts/Evaluate.html#create-custom-metrics",
    "title": "Evaluate",
    "section": "Create custom metrics",
    "text": "Create custom metrics\n- ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ë˜ëŠ” êµ¬ì¡°ì˜ í•¨ìˆ˜ì—¬ì•¼ Trainer í´ë˜ìŠ¤ì˜ ë§¤ê°œë³€ìˆ˜ì¸ compute_metricsì— ì…ë ¥í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n\n# ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜\ndef simple_accuracy(preds,labels):\n    return {'accuracy': (preds == labels).to(float).mean().item()}"
  },
  {
    "objectID": "posts/Evaluate.html#trainer-ì ìš©",
    "href": "posts/Evaluate.html#trainer-ì ìš©",
    "title": "Evaluate",
    "section": "Trainer ì ìš©",
    "text": "Trainer ì ìš©\n\n# micro f1 score ì‚¬ìš©\nimport evaluate\n\ndef custom_metrics(pred):\n    f1 = evaluate.load('f1')\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(axis = -1) # pred.predictionsë¥¼ ìˆ˜í–‰í•˜ë©´ logitsê°’ì´ ì¶œë ¥ëœë‹¤. ê·¸ ì¤‘ ê°€ì¥ í° ê°’ì„ ê°€ì§€ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n    return f1.compute(predictions = preds, references = labels, average = 'micro')\n          #.compute() í•¨ìˆ˜ëŠ” ìë™ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬í˜•íƒœë¡œ ì¶œë ¥í•œë‹¤.\n\n- .argmax(?)\naxis = -1ì„ ì“°ë©´ ë°°ì—´ì˜ ë§ˆì§€ë§‰ ì¶•ì—ì„œ ê³„ì‚°í•˜ëŠ” ê²ƒ. axis=0ì€ ì—´(ì„¸ë¡œ), aixis= 1ì€ í–‰(ê°€ë¡œ)ì´ë‹¤.\nìœ„ì—ì„œ pred.predcitionsì„ í–ˆì„ ë•Œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ê°€ 2ì°¨ì›ì´ê¸°ì— ë§ˆì§€ë§‰ ì¶•ì´ë€ í–‰ì´ ë˜ë¯€ë¡œ axis = -1ê³¼ axis = 1ì€ ê°™ì€ ì½”ë“œì´ë‹¤.\n\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    default_data_collator\n)\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)\n\ndataset = load_dataset(\"klue\", \"ynat\")\n\ndef tokenize_function(sample):\n    result = tokenizer(\n        sample[\"title\"],\n        padding=\"max_length\",\n    )\n    return result\n\ndatasets = dataset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=1000,\n    remove_columns=[\"guid\", \"title\", \"url\", \"date\"]\n)\nprint(datasets)\n\nargs = TrainingArguments(\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=5e-5,\n    max_steps=500,\n    evaluation_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    logging_dir=\"/content/logs\",\n    save_strategy=\"steps\",\n    save_steps=50,\n    output_dir=\"/content/ckpt\",\n    report_to=\"tensorboard\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=datasets[\"train\"],\n    eval_dataset=datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=default_data_collator,\n    compute_metrics=custom_metrics, # ì´ ë¶€ë¶„ì„ ë°”ê¿”ì¤€ë‹¤.\n)\n\nloading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.11.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\nloading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\nloading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\nloading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\nloading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.11.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.11.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nusing `logging_steps` to initialize `eval_steps` to 50\nPyTorch: setting up devices\nmax_steps is given, it will override any value given in num_train_epochs\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 45678\n    })\n    validation: Dataset({\n        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9107\n    })\n})\n\n\n\ntrainer.train()\n\n***** Running training *****\n  Num examples = 45678\n  Num Epochs = 1\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 500\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-50\nConfiguration saved in /content/ckpt/checkpoint-50/config.json\nModel weights saved in /content/ckpt/checkpoint-50/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-50/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-50/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-100\nConfiguration saved in /content/ckpt/checkpoint-100/config.json\nModel weights saved in /content/ckpt/checkpoint-100/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-100/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-100/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-150\nConfiguration saved in /content/ckpt/checkpoint-150/config.json\nModel weights saved in /content/ckpt/checkpoint-150/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-150/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-150/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-200\nConfiguration saved in /content/ckpt/checkpoint-200/config.json\nModel weights saved in /content/ckpt/checkpoint-200/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-200/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-200/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-250\nConfiguration saved in /content/ckpt/checkpoint-250/config.json\nModel weights saved in /content/ckpt/checkpoint-250/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-250/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-250/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-300\nConfiguration saved in /content/ckpt/checkpoint-300/config.json\nModel weights saved in /content/ckpt/checkpoint-300/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-300/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-300/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-350\nConfiguration saved in /content/ckpt/checkpoint-350/config.json\nModel weights saved in /content/ckpt/checkpoint-350/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-350/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-350/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-400\nConfiguration saved in /content/ckpt/checkpoint-400/config.json\nModel weights saved in /content/ckpt/checkpoint-400/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-400/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-400/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-450\nConfiguration saved in /content/ckpt/checkpoint-450/config.json\nModel weights saved in /content/ckpt/checkpoint-450/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-450/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-450/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-500\nConfiguration saved in /content/ckpt/checkpoint-500/config.json\nModel weights saved in /content/ckpt/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-500/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\n\n\n    \n      \n      \n      [500/500 18:44, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\nF1\n\n\n\n\n50\n1.082100\n0.825163\n0.702427\n\n\n100\n0.571100\n0.593032\n0.810695\n\n\n150\n0.476000\n0.570962\n0.816844\n\n\n200\n0.500600\n0.535641\n0.815966\n\n\n250\n0.454800\n0.501376\n0.833535\n\n\n300\n0.433800\n0.479584\n0.837158\n\n\n350\n0.397700\n0.483717\n0.842868\n\n\n400\n0.442900\n0.449807\n0.851104\n\n\n450\n0.420800\n0.434349\n0.853300\n\n\n500\n0.406100\n0.438009\n0.853080\n\n\n\n\n\n\n\nTrainOutput(global_step=500, training_loss=0.5185918083190918, metrics={'train_runtime': 1125.3103, 'train_samples_per_second': 7.109, 'train_steps_per_second': 0.444, 'total_flos': 2104982937600000.0, 'train_loss': 0.5185918083190918, 'epoch': 0.18})\n\n\n- ê²°ê³¼ì—ì„œ ì´ì „ì—ëŠ” ë³¼ ìˆ˜ ì—†ì—ˆë˜ F1 scoreì´ ë³´ì¸ë‹¤."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html",
    "href": "posts/Base_on_Enco_Deco.html",
    "title": "Base on Encoder-Decoder models",
    "section": "",
    "text": "- ì¸ì½”ë”-ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ ì™„ì„±ëœ ë¬¸ì¥ì„ ì´ì–´ë°›ì•„ ì…ë ¥ê³¼ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.\n- ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì˜ ìì—°ì–´ ìƒì„±ê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ ì…ë ¥ëœ ë¬¸ì¥ì„ ì´ì–´ ë‚˜ê°€ëŠ” ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ê³¼ëŠ” ë‹¬ë¦¬ ì™„ì „íˆ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ì‘ì„±í•œë‹¤ëŠ” ì°¨ì´ê°€ ìˆë‹¤.\nì¼ë°˜ì ìœ¼ë¡œ ê¸°ê³„ ë²ˆì—­ì´ë‚˜ ìš”ì•½ì— ì‚¬ìš©ëœë‹¤.\n\n\n-BARTì˜ í•™ìŠµ ë°©ë²•\n\n\nBERTì—ì„œ ì‚¬ìš©í—¸ë˜ ì¼ë°˜ì ì¸ Masked LMê³¼ ë™ì¼í•˜ë‹¤.\n\n\n\nëœë¤í•œ í† í°ì„ ì‚­ì œí•˜ê³  ì´ë¥¼ ë³µêµ¬í•œë‹¤. ë§ˆìŠ¤í‚¹ ë°©ë²•ì€ íŠ¹ì • í† í°ì„ [MASK]ë¡œ ë³€ê²½í•˜ê¸°ì— ì–´ë–¤ ìœ„ì¹˜ì˜ í† í°ì´ ì‚¬ë¼ì¡ŒëŠ”ì§€ ì•Œì§€ë§Œ í† í° ì‚­ì œëŠ” ì–´ë–¤ ìœ„ì¹˜ì˜ í† í°ì´ ì‚¬ë¼ì¡ŒëŠ”ì§€ ì•Œ ìˆ˜ ì—†ë‹¤.\n\n\n\nì…ë ¥ ë¬¸ì¥ ì¤‘, ì—°ì†ë˜ëŠ” í† í° ëª‡ ê°œë¥¼ ë¬¶ì–´ í† í° ë­‰ì¹˜ë¥¼ ìƒì„±í•˜ì—¬ ê·¸ ë²”ìœ„ë¥¼ [MASK] í† í°ìœ¼ë¡œ ì¹˜í™˜í•œë‹¤. ì´ë•Œ, í† í° ë­‰ì¹˜ ê¸¸ì´ëŠ” í¬ì•„ì†¡ ë¶„í¬ë¥¼ ë”°ë¥´ë©° ê¸¸ì´ê°€ 0 or 2ì´ìƒì´ë‹¤. ê¸¸ì´ê°€ 0ì¸ ê²½ìš° ì •ìƒ ë¬¸ì¥ì—ì„œ [MASK] í† í°ë§Œ ìƒì„±ë˜ê³  2 ì´ìƒì¸ ê²½ìš° ì—¬ëŸ¬ í† í°ì´ í•˜ë‚˜ì˜ [MASK] í† í°ìœ¼ë¡œ ë°”ë€Œê²Œ ëœë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì´ ë²”ìœ„ì—ì„œ ëˆ„ë½ëœ í† í° ìˆ˜ì— ëŒ€í•´ì„œë„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤\n3. í…ìŠ¤íŠ¸ ì±„ìš°ê¸° ë³´ì¶© ì„¤ëª…\n- ì™œ í¬ì•„ì†¡ ë¶„í¬(Poisson Distribution)ì„ ë”°ë¥´ëŠ”ê°€?\në¨¼ì € í¬ì•„ì†¡ ë¶„í¬ì— ëŒ€í•œ ì´í•´ë¥¼ í•´ë³´ì.\ní¬ì•„ì†¡ ë¶„í¬: ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì¼ì–´ë‚˜ëŠ” íŠ¹ì •í•œ ì‚¬ê±´ Aì˜ ë°œìƒíšŸìˆ˜ì˜ ë¶„í¬\nì¦‰ í¬ì•„ì†¡ ë¶„í¬ëŠ” í‰ê·  ë°œìƒ íšŸìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ê±´ì´ ë°œìƒí•  íšŸìˆ˜ê°€ ê²°ì •ëœë‹¤.\nBARTëŠ” ì…ë ¥ ë¬¸ì¥ì—ì„œ ì¼ë¶€ ë‹¨ì–´ë‚˜ í† í°ì„ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•˜ëŠ”ë° ì •ë§ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ Masking í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  í¬ì•„ì†¡ ë¶„í¬ë¥¼ ë”°ë¥´ë©´ì„œ Masking í•  í† í°ì„ ì°¾ëŠ”ë° ê·¸ ì´ìœ ëŠ” Masking í•  í† í°ì˜ ìˆ˜ê°€ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê³ , ì¼ì •í•œ í‰ê·  ë¹ˆë„ìˆ˜ë¡œ ì„ íƒë˜ë„ë¡ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.\nìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ í¬ì•„ì†¡ ë¶„í¬ëŠ” í‰ê· ì ìœ¼ë¡œ ëª‡ ê°œì˜ í† í°ì´ Maskingë ì§€ ì˜ˆì¸¡í•  ìˆ˜ ìˆì§€ë§Œ, ì‹¤ì œë¡œ Masking ë  í† í°ì˜ ìˆ˜ëŠ” í™•ë¥ ì ìœ¼ë¡œ ê²°ì •ëœë‹¤.\n\nê¸¸ì´ê°€ 0ì¸ ê²½ìš°: ì •ìƒ ë¬¸ì¥ì—ì„œ [MASK] í† í°ë§Œ ìƒì„±ë¨\nê¸¸ì´ê°€ 2ì¸ ê²½ìš°: ì—¬ëŸ¬ ê°œì˜ ì—°ì†ëœ í† í°ì´ í•˜ë‚˜ì˜ [MASK] í† í°ìœ¼ë¡œ ì¹˜í™˜ëœë‹¤. ì´ ë•Œ!! ë§ˆìŠ¤í‚¹ëœ ë²”ìœ„ì˜ ê¸¸ì´ëŠ” í¬ì•„ì†¡ ë¶„í¬ì— ë”°ë¼ ê²°ì •ëœë‹¤. (í‰ê· ì´ ëª‡ì¸ì§€ì— ë”°ë¼ ë‹¤ë¥´ê² ì§€ë§Œ ëŒ€ë¶€ë¶„ í™•ë¥ ì€ ë§¤ìš° ë‚®ìŒ)\n\ní¬ì•„ì†¡ ë¶„í¬ëŠ” ëª¨ë¸ì´ ì¼ë¶€ ì—°ì†ì ì¸ í† í°ë“¤ì„ ë§ˆìŠ¤í‚¹í•˜ë©´ì„œë„ ë¬¸ë§¥ ì •ë³´ë¥¼ ì´í•´í•˜ê³ , ëˆ„ë½ëœ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ë„ì›€ì´ ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤.\n\n\n\nì…ë ¥ ë¬¸ì„œë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  ë¬¸ì¥ì˜ ìˆœì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ëŠ”ë‹¤.\n\n\n\nì…ë ¥ ë¬¸ì¥ ì¤‘, í† í° í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì •í•´ í•´ë‹¹ í† í°ì´ ë¬¸ì¥ì˜ ì‹œì‘ì´ ë˜ë„ë¡ í•´ë‹¹ ë¬¸ì¥ í† í°ì„ ë°€ì–´ë‚¸ë‹¤. ì‹œì‘ í† í° ì•ì— ìˆë˜ í† í°ì€ ë§¨ ë’¤ë¡œ ì´ë™í•œë‹¤."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#bart",
    "href": "posts/Base_on_Enco_Deco.html#bart",
    "title": "Base on Encoder-Decoder models",
    "section": "",
    "text": "-BARTì˜ í•™ìŠµ ë°©ë²•\n\n\nBERTì—ì„œ ì‚¬ìš©í—¸ë˜ ì¼ë°˜ì ì¸ Masked LMê³¼ ë™ì¼í•˜ë‹¤.\n\n\n\nëœë¤í•œ í† í°ì„ ì‚­ì œí•˜ê³  ì´ë¥¼ ë³µêµ¬í•œë‹¤. ë§ˆìŠ¤í‚¹ ë°©ë²•ì€ íŠ¹ì • í† í°ì„ [MASK]ë¡œ ë³€ê²½í•˜ê¸°ì— ì–´ë–¤ ìœ„ì¹˜ì˜ í† í°ì´ ì‚¬ë¼ì¡ŒëŠ”ì§€ ì•Œì§€ë§Œ í† í° ì‚­ì œëŠ” ì–´ë–¤ ìœ„ì¹˜ì˜ í† í°ì´ ì‚¬ë¼ì¡ŒëŠ”ì§€ ì•Œ ìˆ˜ ì—†ë‹¤.\n\n\n\nì…ë ¥ ë¬¸ì¥ ì¤‘, ì—°ì†ë˜ëŠ” í† í° ëª‡ ê°œë¥¼ ë¬¶ì–´ í† í° ë­‰ì¹˜ë¥¼ ìƒì„±í•˜ì—¬ ê·¸ ë²”ìœ„ë¥¼ [MASK] í† í°ìœ¼ë¡œ ì¹˜í™˜í•œë‹¤. ì´ë•Œ, í† í° ë­‰ì¹˜ ê¸¸ì´ëŠ” í¬ì•„ì†¡ ë¶„í¬ë¥¼ ë”°ë¥´ë©° ê¸¸ì´ê°€ 0 or 2ì´ìƒì´ë‹¤. ê¸¸ì´ê°€ 0ì¸ ê²½ìš° ì •ìƒ ë¬¸ì¥ì—ì„œ [MASK] í† í°ë§Œ ìƒì„±ë˜ê³  2 ì´ìƒì¸ ê²½ìš° ì—¬ëŸ¬ í† í°ì´ í•˜ë‚˜ì˜ [MASK] í† í°ìœ¼ë¡œ ë°”ë€Œê²Œ ëœë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì´ ë²”ìœ„ì—ì„œ ëˆ„ë½ëœ í† í° ìˆ˜ì— ëŒ€í•´ì„œë„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤\n3. í…ìŠ¤íŠ¸ ì±„ìš°ê¸° ë³´ì¶© ì„¤ëª…\n- ì™œ í¬ì•„ì†¡ ë¶„í¬(Poisson Distribution)ì„ ë”°ë¥´ëŠ”ê°€?\në¨¼ì € í¬ì•„ì†¡ ë¶„í¬ì— ëŒ€í•œ ì´í•´ë¥¼ í•´ë³´ì.\ní¬ì•„ì†¡ ë¶„í¬: ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì¼ì–´ë‚˜ëŠ” íŠ¹ì •í•œ ì‚¬ê±´ Aì˜ ë°œìƒíšŸìˆ˜ì˜ ë¶„í¬\nì¦‰ í¬ì•„ì†¡ ë¶„í¬ëŠ” í‰ê·  ë°œìƒ íšŸìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ê±´ì´ ë°œìƒí•  íšŸìˆ˜ê°€ ê²°ì •ëœë‹¤.\nBARTëŠ” ì…ë ¥ ë¬¸ì¥ì—ì„œ ì¼ë¶€ ë‹¨ì–´ë‚˜ í† í°ì„ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•˜ëŠ”ë° ì •ë§ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ Masking í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  í¬ì•„ì†¡ ë¶„í¬ë¥¼ ë”°ë¥´ë©´ì„œ Masking í•  í† í°ì„ ì°¾ëŠ”ë° ê·¸ ì´ìœ ëŠ” Masking í•  í† í°ì˜ ìˆ˜ê°€ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê³ , ì¼ì •í•œ í‰ê·  ë¹ˆë„ìˆ˜ë¡œ ì„ íƒë˜ë„ë¡ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.\nìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ í¬ì•„ì†¡ ë¶„í¬ëŠ” í‰ê· ì ìœ¼ë¡œ ëª‡ ê°œì˜ í† í°ì´ Maskingë ì§€ ì˜ˆì¸¡í•  ìˆ˜ ìˆì§€ë§Œ, ì‹¤ì œë¡œ Masking ë  í† í°ì˜ ìˆ˜ëŠ” í™•ë¥ ì ìœ¼ë¡œ ê²°ì •ëœë‹¤.\n\nê¸¸ì´ê°€ 0ì¸ ê²½ìš°: ì •ìƒ ë¬¸ì¥ì—ì„œ [MASK] í† í°ë§Œ ìƒì„±ë¨\nê¸¸ì´ê°€ 2ì¸ ê²½ìš°: ì—¬ëŸ¬ ê°œì˜ ì—°ì†ëœ í† í°ì´ í•˜ë‚˜ì˜ [MASK] í† í°ìœ¼ë¡œ ì¹˜í™˜ëœë‹¤. ì´ ë•Œ!! ë§ˆìŠ¤í‚¹ëœ ë²”ìœ„ì˜ ê¸¸ì´ëŠ” í¬ì•„ì†¡ ë¶„í¬ì— ë”°ë¼ ê²°ì •ëœë‹¤. (í‰ê· ì´ ëª‡ì¸ì§€ì— ë”°ë¼ ë‹¤ë¥´ê² ì§€ë§Œ ëŒ€ë¶€ë¶„ í™•ë¥ ì€ ë§¤ìš° ë‚®ìŒ)\n\ní¬ì•„ì†¡ ë¶„í¬ëŠ” ëª¨ë¸ì´ ì¼ë¶€ ì—°ì†ì ì¸ í† í°ë“¤ì„ ë§ˆìŠ¤í‚¹í•˜ë©´ì„œë„ ë¬¸ë§¥ ì •ë³´ë¥¼ ì´í•´í•˜ê³ , ëˆ„ë½ëœ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ë„ì›€ì´ ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤.\n\n\n\nì…ë ¥ ë¬¸ì„œë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  ë¬¸ì¥ì˜ ìˆœì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ëŠ”ë‹¤.\n\n\n\nì…ë ¥ ë¬¸ì¥ ì¤‘, í† í° í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì •í•´ í•´ë‹¹ í† í°ì´ ë¬¸ì¥ì˜ ì‹œì‘ì´ ë˜ë„ë¡ í•´ë‹¹ ë¬¸ì¥ í† í°ì„ ë°€ì–´ë‚¸ë‹¤. ì‹œì‘ í† í° ì•ì— ìˆë˜ í† í°ì€ ë§¨ ë’¤ë¡œ ì´ë™í•œë‹¤."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#model",
    "href": "posts/Base_on_Enco_Deco.html#model",
    "title": "Base on Encoder-Decoder models",
    "section": "2-1. model",
    "text": "2-1. model\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"hyunwoongko/kobart\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n2025-04-01 07:39:17.432468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1743493157.447473   60530 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1743493157.452654   60530 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1743493157.466293   60530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743493157.466309   60530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743493157.466311   60530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743493157.466313   60530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-04-01 07:39:17.470692: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\n\nBartForConditionalGeneration(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n)"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#dataset",
    "href": "posts/Base_on_Enco_Deco.html#dataset",
    "title": "Base on Encoder-Decoder models",
    "section": "2-2. Dataset",
    "text": "2-2. Dataset\nìœ ì˜ì \nì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ì€ ì¸ì½”ë”ì— ë“¤ì–´ê°€ëŠ” ì…ë ¥ê³¼ ë””ì½”ë”ì— ë“¤ì–´ê°€ëŠ” ì…ë ¥, ì´ ë‘ ê°œ ì…ë ¥ì´ í•„ìš”í•˜ê³  ì´ì— ëŒ€í•œ ì •ë‹µì´ ë”°ë¡œ í•„ìš”í•˜ë‹¤.\ní•„ìˆ˜ë¡œ ì¸ì½”ë” ì…ë ¥, ë””ì½”ë” ì…ë ¥, ë””ì½”ë” ì •ë‹µ ì´ë ‡ê²Œ ì„¸ ê°€ì§€ ë°ì´í„° íŠ¹ì„±ì´ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤. - ì™œ ì¸ì½”ë” ì •ë‹µì€ í•„ìš” ì—†ì§€..?\nìš°ì„  ì¸ì½”ë” ì…ë ¥ê³¼ ì¶œë ¥ì˜ ì •ë‹µì€ ë‹¬ë¼ì•¼í•˜ë¯€ë¡œ ì •ë‹µì„ text_target íŒŒë¼ë¯¸í„°ë¡œ ì…ë ¥í•´ ì •ë‹µ ê°’ê¹Œì§€ í•œ ë²ˆì— ë“¤ì–´ì•¼ í•œë‹¤.\n- ì¸ì½”ë” ì •ë‹µì´ í•„ìš”ì—†ëŠ” ì´ìœ \nê²°ë¡ ë¶€í„° ë§í•˜ë©´ ì •ë‹µì´ í•„ìš”í•œ ê³³ì€ ë””ì½”ë” ë¿ì´ë‹¤! - ì¸ì½”ë”ëŠ” ì…ë ¥ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ë§Œ í•˜ê¸°ì— ì •ë‹µì´ ë”°ë¡œ í•„ìš”ì—†ë‹¤. - í•˜ì§€ë§Œ ë””ì½”ë”ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ë¯€ë¡œ ì •ë‹µ(labels)ì´ í•„ìš”í•˜ë‹¤.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"msarmi9/korean-english-multitarget-ted-talks-task\")\nprint(dataset)\ndataset['train'][0]\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['korean', 'english'],\n        num_rows: 166215\n    })\n    validation: Dataset({\n        features: ['korean', 'english'],\n        num_rows: 1958\n    })\n    test: Dataset({\n        features: ['korean', 'english'],\n        num_rows: 1982\n    })\n})\n\n\n{'korean': '(ë°•ìˆ˜) ì´ìª½ì€ Bill Lange ì´ê³ , ì €ëŠ” David Galloì…ë‹ˆë‹¤',\n 'english': \"(Applause) David Gallo: This is Bill Lange. I'm Dave Gallo.\"}\n\n\n\ntokenized_dataset = dataset.map(\n    lambda batch: (\n        tokenizer(\n            batch[\"korean\"],\n            text_target=batch[\"english\"],\n            max_length=512,\n            truncation=True,\n        )\n    ),\n    batched=True,\n    batch_size=1000,\n    num_proc=2,\n    remove_columns=dataset[\"train\"].column_names,\n)\ntokenized_dataset[\"train\"][0]\n\n\n\n\n\n\n\n\n\n\n{'input_ids': [0,\n  14338,\n  10770,\n  11372,\n  240,\n  14025,\n  12471,\n  12005,\n  15085,\n  29490,\n  14676,\n  24508,\n  300,\n  14025,\n  14161,\n  16530,\n  15529,\n  296,\n  317,\n  18509,\n  15464,\n  15585,\n  20858,\n  12049,\n  20211,\n  1],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1],\n 'labels': [0,\n  14338,\n  264,\n  311,\n  311,\n  17422,\n  316,\n  17223,\n  240,\n  15529,\n  296,\n  317,\n  18509,\n  15464,\n  15585,\n  20858,\n  257,\n  15054,\n  303,\n  15868,\n  1700,\n  15868,\n  15085,\n  29490,\n  14676,\n  24508,\n  300,\n  245,\n  14943,\n  238,\n  308,\n  15529,\n  296,\n  21518,\n  15464,\n  15585,\n  20858,\n  245,\n  1]}\n\n\n\ntokenized_dataset\n\n# ë°ì´í„°ë¥¼ ì‚´í´ë³´ë©´ input_ids : ì¸ì½”ë” ì…ë ¥, labels : ë””ì½”ë” ì •ë‹µì€ ì¡´ì¬í•œë‹¤.\n# ë””ì½”ë” ì…ë ¥ì¸ decoder_input_idsê°€ ì—†ê¸°ì— ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì…ë ¥í•˜ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤.\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 166215\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1958\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1982\n    })\n})"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#collator",
    "href": "posts/Base_on_Enco_Deco.html#collator",
    "title": "Base on Encoder-Decoder models",
    "section": "2-3. Collator",
    "text": "2-3. Collator\në””ì½”ë” ì…ë ¥ê°’ì€ ê²°êµ­ ì •ë‹µ ê°’ì¸ labelsì„ ì•ìœ¼ë¡œ í•œ ì¹¸ ì´ë™í•œ ë°ì´í„°ì´ë‹¤.\ní•´ë‹¹ ì‘ì—…ì„ íŒ¨ë”©ê³¼ ë”ë¶ˆì–´ ê°„í¸í•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ DataCollatorForSeq2Seqë¥¼ ì‚¬ìš©í•œë‹¤.\níŒ¨ë”© ì‘ì—…ê³¼ í•¨ê»˜ ë””ì½”ë”ì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°ˆ ë¶€ë¶„ê¹Œì§€ ìë™ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë°˜í™˜í•œë‹¤.\nì½œë ˆì´í„°ëŠ” batch ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ì´ë‹¤.\n\nfrom transformers import DataCollatorForSeq2Seq\n\ncollator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"max_length\",\n    max_length=512,\n)\nbatch = collator([tokenized_dataset[\"train\"][i] for i in range(2)]) # ë¬¸ì¥ 2ê°œë§Œ ë½‘ì•„ì„œ ì²˜ë¦¬\nbatch\n\n{'input_ids': tensor([[    0, 14338, 10770,  ...,     3,     3,     3],\n        [    0, 15496, 18918,  ...,     3,     3,     3]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    0, 14338,   264,  ...,  -100,  -100,  -100],\n        [    0, 14603,   309,  ...,  -100,  -100,  -100]]), 'decoder_input_ids': tensor([[    1,     0, 14338,  ...,     3,     3,     3],\n        [    1,     0, 14603,  ...,     3,     3,     3]])}"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#generation",
    "href": "posts/Base_on_Enco_Deco.html#generation",
    "title": "Base on Encoder-Decoder models",
    "section": "2-4. Generation",
    "text": "2-4. Generation\n\nimport torch\n\nwith torch.no_grad():\n    logits = model(**batch).logits\n\nlogits\n\ntensor([[[  5.4885,  18.7849,  -0.5489,  ...,   0.0465,   0.5813,  -2.2851],\n         [  3.7287,  18.9676,  -1.1747,  ...,  -0.2600,  -3.4647,  -0.0973],\n         [ -1.2976,   8.6322,  -5.0410,  ...,  -7.0689,  -6.1346,  -4.4141],\n         ...,\n         [ -9.2638,   4.4483,  -8.4506,  ..., -12.6961, -13.2625,  -7.7570],\n         [ -8.4581,   4.9268,  -7.2172,  ..., -11.5650, -11.8799,  -6.8108],\n         [ -8.3191,   5.2101,  -6.8817,  ..., -11.1563, -11.7052,  -6.7644]],\n\n        [[  4.7748,  16.2666,  -3.0011,  ...,  -0.8965,  -3.3187,  -3.1041],\n         [  0.6535,  19.3665,  -1.4506,  ...,   0.1562,  -4.3976,   0.1983],\n         [ -5.0934,  10.8673,  -7.5637,  ...,  -6.3808,  -1.6471,  -7.2105],\n         ...,\n         [ -1.5132,  19.0760,   0.3272,  ...,  -2.6680,  -3.9969,   2.7315],\n         [ -2.3757,  20.0047,  -0.5301,  ...,  -1.7740,  -5.1750,   0.8077],\n         [ -2.2504,  19.9756,  -0.4519,  ...,  -0.6850,  -5.1072,   0.4720]]])\n\n\n\nlogits.shape\n\ntorch.Size([2, 512, 30000])\n\n\n512ì˜ ë‹¨ì–´ ê¸¸ì´ë¥¼ ê°€ì§€ëŠ” 2ê°œì˜ ë¬¸ì¥ì—ì„œ 30000ê°œì˜ ë‹¨ì–´ë“¤ì´ ë‚˜ì˜¬ í™•ë¥ ì„ ê³„ì‚°í•œ ê²ƒ.\n\nfrom transformers import GenerationConfig\n\ngen_cfg = GenerationConfig(\n    max_new_tokens=100,\n    do_sample=True,\n    temperature=1.2,\n    top_k=50,\n    top_p=0.95,\n)\noutputs = model.generate(batch[\"input_ids\"], generation_config=gen_cfg)\nresult = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(result[0])\n\n\n\n\n- ë¬¸ì¥ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì§€ ì•Šì€ ì´ìœ \nmodel.config.eos_token_id = 1ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ì„œ, ëª¨ë¸ì´ ì²˜ìŒ ìƒì„±í•˜ëŠ” í† í°ì´ 1ë²ˆ í† í°ì´ë©´ ê³§ë°”ë¡œ ì¢…ë£Œëœë‹¤.\n\nmodel.config.eos_token_id\n\n1"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#evaluate",
    "href": "posts/Base_on_Enco_Deco.html#evaluate",
    "title": "Base on Encoder-Decoder models",
    "section": "2-5. Evaluate",
    "text": "2-5. Evaluate\në¬¸ì¥ ìƒì„± íƒœìŠ¤í¬ëŠ” í•™ìŠµì„ ì§„í–‰í•˜ë©° í‰ê°€ ì§€í‘œë¥¼ í™•ì¸í•˜ê¸° ì–´ë µë‹¤.\në”°ë¼ì„œ í•™ìŠµ ì¤‘ì— ì¼ë°˜ì ìœ¼ë¡œ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ì‚¬ìš©í•˜ì—¬ ê°’ì´ ê°ì†Œ ì¶”ì´ë¥¼ ì‚´í”¼ë©° ëª¨ë¸ í•™ìŠµì´ ì›í™œí•˜ê²Œ ì´ë¤„ì§€ëŠ”ì§€ í™•ì¸í•œë‹¤."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#model-1",
    "href": "posts/Base_on_Enco_Deco.html#model-1",
    "title": "Base on Encoder-Decoder models",
    "section": "3-1. model",
    "text": "3-1. model\nì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë”ì— ë™ì¼í•œ ë¬¸ì¥ì„ ì…ë ¥í•˜ì—¬ì„œ ë¬¸ì¥ ë¶„ë¥˜ë¥¼ ì§„í–‰í•˜ë ¤ê³  í•œë‹¤.\në¬¸ì¥ êµ¬ì¡°ê°€ ë°”ë€Œì§€ ì•Šê¸°ì— ì´ì „ ì¸ì½”ë”, ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì—ì„œ ì‹¤ìŠµí–ˆë˜ ê²ƒê³¼ ê°™ì´ ë™ì¼í•œ ì½”ë“œë¡œ ì¶”ë¡ í•œë‹¤.\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"hyunwoongko/kobart\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel\n\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\nSome weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBartForSequenceClassification(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (classification_head): BartClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#dataset-1",
    "href": "posts/Base_on_Enco_Deco.html#dataset-1",
    "title": "Base on Encoder-Decoder models",
    "section": "3-2. Dataset",
    "text": "3-2. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"sts\")\n\ndef process_data(batch):\n  result = tokenizer(batch[\"sentence1\"], text_pair=batch[\"sentence2\"])\n  result[\"labels\"] = [x[\"binary-label\"] for x in batch[\"labels\"]]\n  return result\n\ntokenized_dataset = dataset.map(\n    process_data,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\n\n\n\n\n\n\n\n\n\n\ntokenized_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['labels', 'input_ids', 'attention_mask'],\n        num_rows: 11668\n    })\n    validation: Dataset({\n        features: ['labels', 'input_ids', 'attention_mask'],\n        num_rows: 519\n    })\n})\n\n\ndatasetì€ ì¸ì½”ë”ì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” input_idsë§Œ í¬í•¨í•˜ê³  ìˆì§€ë§Œ í° ë¬¸ì œëŠ” ì—†ë‹¤.\ndecoder_input_idsê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ì„ ë•Œ, ì¸ì½”ë” ì…ë ¥ì¸ input_idsë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ì¹¸ ì´ë™í•˜ì—¬ ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ìë™ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#collator-1",
    "href": "posts/Base_on_Enco_Deco.html#collator-1",
    "title": "Base on Encoder-Decoder models",
    "section": "3-3. Collator",
    "text": "3-3. Collator\nìœ„ì—ì„œë„ ì„¤ëª…í–ˆì§€ë§Œ ì½œë ˆì´í„°ëŠ” ëª¨ë¸ì´ í•´ë‹¹ ë°ì´í„°ì…‹ì„ ë°”ë¡œ ì‚¬ìš©í•˜ë„ë¡ batch ì‘ì—…ì„ í•´ì¤€ë‹¤.\n\nimport torch\nfrom transformers import DataCollatorWithPadding\n\ncollator = DataCollatorWithPadding(tokenizer)\nbatch = collator([tokenized_dataset['train'][i] for i in range(4)])"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#generation-1",
    "href": "posts/Base_on_Enco_Deco.html#generation-1",
    "title": "Base on Encoder-Decoder models",
    "section": "3-4. Generation",
    "text": "3-4. Generation\n\nwith torch.no_grad():\n    logits = model(**batch).logits\n\nlogits\n\ntensor([[ 0.0255, -0.1499],\n        [ 0.4134, -0.2986],\n        [-0.0575,  0.0541],\n        [ 0.1218, -0.8607]])"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#evaluate-1",
    "href": "posts/Base_on_Enco_Deco.html#evaluate-1",
    "title": "Base on Encoder-Decoder models",
    "section": "3-5. Evaluate",
    "text": "3-5. Evaluate\n\nimport evaluate\n\nf1 = evaluate.load('f1')\nf1.compute(\n    predictions = logits.argmax(axis = -1),\n    references = batch['labels'],\n    average = 'micro'\n)\n\n\n\n\n{'f1': 0.5}\n\n\n- ìƒì„± taskê°€ ì•„ë‹ˆë¼ ë¶„ë¥˜ì´ë¯€ë¡œ í‰ê°€ ê°€ëŠ¥"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#model-2",
    "href": "posts/Base_on_Enco_Deco.html#model-2",
    "title": "Base on Encoder-Decoder models",
    "section": "4-1. model",
    "text": "4-1. model\n\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nmodel_name = \"hyunwoongko/kobart\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\nmodel\n\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\nSome weights of BartForQuestionAnswering were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBartForQuestionAnswering(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)\n\n\n- out_features=2\në‘ ê°’ë§Œ ì¶œë ¥í•˜ë©´ ë˜ê¸°ì— out_features = 2ì´ë‹¤."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#dataset-2",
    "href": "posts/Base_on_Enco_Deco.html#dataset-2",
    "title": "Base on Encoder-Decoder models",
    "section": "4-1. Dataset",
    "text": "4-1. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"mrc\") # klue: ë°ì´í„°ì…‹ mrc: ê¸°ê³„ë…í•´ ë°ì´í„°\n\ndef preprocess_function(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=512,\n        truncation=\"only_second\", # ë¬¸ë§¥ì´ ê¸¸ë©´ ë¬¸ë§¥ë§Œ ì˜ë¼ëƒ„ (ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì—ì„œ ë¬¸ë§¥ì´ ê¸¸ê¸°ë•Œë¬¸ì—)\n        return_offsets_mapping=True, # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ê° í† í°ì˜ ìœ„ì¹˜ ì •ë³´ ì €ì¥ (ì¶”ì¶œí•´ì•¼ í•˜ê¸°ì—)\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    answers = examples[\"answers\"] # ì‹¤ì œ ì •ë‹µ ì •ë³´ answer_start , text\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        answer = answers[i]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n        # start, end ìœ„ì¹˜ë¥¼ ì°¾ìŒ\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n          idx += 1\n        context_start = idx # ë¬¸ë§¥ì´ ì‹œì‘í•˜ëŠ” ìœ„ì¹˜\n        while sequence_ids[idx] == 1:\n          idx += 1 \n        context_end = idx - 1 # ë¬¸ë§¥ì´ ëë‚˜ëŠ” ìœ„ì¹˜\n\n        # If the answer is not fully inside the context, label it (0, 0)\n        if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char:\n            start_positions.append(0) # ì •ë‹µì´ ë¬¸ë§¥ ë°–ì— ìˆìœ¼ë©´ start,end ìœ„ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx &lt;= context_end and offset[idx][0] &lt;= start_char:\n                idx += 1\n            start_positions.append(idx - 1) # ì •ë‹µì˜ ì‹œì‘ ìœ„ì¹˜ë¥¼ ì €ì¥\n\n            idx = context_end\n            while idx &gt;= context_start and offset[idx][1] &gt;= end_char:\n                idx -= 1\n            end_positions.append(idx + 1) # ì •ë‹µì˜ ë ìœ„ì¹˜ë¥¼ ì €ì¥\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\ntokenized_dataset = dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#collator-2",
    "href": "posts/Base_on_Enco_Deco.html#collator-2",
    "title": "Base on Encoder-Decoder models",
    "section": "4-2. Collator",
    "text": "4-2. Collator\n\nfrom transformers import DefaultDataCollator\n\ndata_collator = DefaultDataCollator()\nbatch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])\nbatch\n\n{'input_ids': tensor([[    0, 14337, 26225,  ...,     3,     3,     3],\n         [    0, 25092, 18001,  ..., 11270, 19903,     1],\n         [    0, 25788, 13679,  ..., 19903, 15599,     1],\n         ...,\n         [    0, 20437, 17814,  ...,     3,     3,     3],\n         [    0, 14154, 12061,  ...,     3,     3,     3],\n         [    0, 14295, 14120,  ...,     3,     3,     3]]),\n 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0]]),\n 'start_positions': tensor([233,  27,   0,  78,  60,  68, 202, 319, 306, 271]),\n 'end_positions': tensor([235,  29,   0,  79,  66,  74, 210, 325, 312, 275])}"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#generation-2",
    "href": "posts/Base_on_Enco_Deco.html#generation-2",
    "title": "Base on Encoder-Decoder models",
    "section": "4-3. Generation",
    "text": "4-3. Generation\n\nimport torch\n\nwith torch.no_grad():\n  outputs = model(**batch)\n\nanswer_start_index = outputs.start_logits.argmax()\nanswer_end_index = outputs.end_logits.argmax()\n\npredict_answer_tokens = batch[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n\n''"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#evaluate-2",
    "href": "posts/Base_on_Enco_Deco.html#evaluate-2",
    "title": "Base on Encoder-Decoder models",
    "section": "4-4. Evaluate",
    "text": "4-4. Evaluate\nQA í‰ê°€ì§€í‘œëŠ” evaluate.load('squad')ë¥¼ í†µí•´ ì§„í–‰í•  ìˆ˜ ìˆì—ˆë‹¤. í•˜ì§€ë§Œ ìƒë‹¹í•œ ì–‘ì˜ í›„ì²˜ë¦¬ê°€ í•„ìš”í•˜ê³  ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—…ì´ë‹¤."
  },
  {
    "objectID": "posts/Transformer_architecture.html",
    "href": "posts/Transformer_architecture.html",
    "title": "Transformer architecture",
    "section": "",
    "text": "from transformers import AutoTokenizer\nfrom bertviz.transformers_neuron_view import BertModel\nfrom bertviz.neuron_view import show\nfrom torch import nn\nfrom transformers import AutoConfig\nimport torch\nfrom math import sqrt\nimport torch.nn.functional as F\n\n# step 1: ëª¨ë¸, í† í¬ë‚˜ì´ì €, text ì„ ì–¸\nmodel_ckpt = \"bert-base-uncased\" # ì›í•˜ëŠ” ëª¨ë¸ ì„ ì–¸\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt) # ëª¨ë¸ì—ì„œ ì‚¬ìš©ëœ pretrained ëœ tokenizerë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.\nmodel = BertModel.from_pretrained(model_ckpt) # pretrained ëœ modelì„ ë¶ˆëŸ¬ì˜¨ë‹¤.\ntext = \"time flies like an arrow\"\n\n\n# step 2: í† í¬ë‚˜ì´ì§•\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False) # pytorchë¥¼ ì‚¬ìš©í•´ì„œ CLS,SEP í† í°ì„ ì œì™¸í•˜ì—¬ í† í¬ë‚˜ì´ì§•í•˜ëŠ” ì½”ë“œ\ninputs.input_ids # ê²°ê³¼ëŠ” í† í¬ë‚˜ì´ì €ì— ìˆëŠ” ì–´íœ˜ì‚¬ì „ì— ê³ ìœ í•œ IDì— ë§¤í•‘ëœ ê°’ì´ë‹¤.\n\n\n# step 3: ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì„ë² ë”©\nconfig = AutoConfig.from_pretrained(model_ckpt) # configë€ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ì €ì¥í•˜ëŠ” ê°ì²´, ëª¨ë¸ì—ì„œ pretrainedëœ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œ.\ntoken_emb = nn.Embedding(config.vocab_size, config.hidden_size) # ì„ë² ë”©ì˜ ì¸µì„ ìƒì„±í•˜ëŠ” ê²ƒ\n#config.vocab_sizeëŠ” ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ì‚¬ì „ì˜ í¬ê¸°. config.hidden_sizeëŠ” ëª¨ë¸ì˜ ì„ë² ë”© ì°¨ì›ì´ë‹¤. ê° ë‹¨ì–´ë¥¼ ëª‡ ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜í• ì§€ì— ëŒ€í•œ ê²ƒì´ë‹¤\n# ê²°ê³¼ì˜ 30522ëŠ” ë‹¨ì–´ì‚¬ì „ì˜ í¬ê¸°ì´ê³  768ì€ ì„ë² ë”© ì°¨ì›ì´ë‹¤.\ninputs_embeds = token_emb(inputs.input_ids) #ìœ„ì—ì„œ ì„ ì–¸í•œ token_embì„ ì´ìš©í•´ì„œ 5ê°œì˜ ë‹¨ì–´ë¥¼ ì„ë² ë”©ì‹œì¼°ë‹¤.\n# ì„ë² ë”© ì‹œì¼°ë‹¤ëŠ” ê²ƒì€ ê° ë‹¨ì–´ê°€ 768ê°œì˜ íŠ¹ì„±ì„ ê°€ì§€ëŠ” ë²¡í„°ê°’ìœ¼ë¡œ ë³€í™˜ëœ ê²ƒì´ë¼ê³  ì´í•´í•˜ë©´ ëœë‹¤.\ninputs_embeds.size()\n\n\n# step 4: ì ê³±ê³¼ softmaxë¥¼ ì´ìš©í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°\nquery = key = value = inputs_embeds\ndim_k = key.size(-1) # ì„ë² ë”© ì°¨ì›ì„ ì„ íƒ\nscores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k) # ì ê³±ì„ ì‚¬ìš©. self attentionì˜ ê²½ìš° batchë‹¨ìœ„ë¡œ ì—°ì‚°ì´ í•„ìš”í•˜ê¸°ì— torch.bmmì„ ì‚¬ìš©\n# ì ê³±ì„ ì§„í–‰í•˜ë©´ dim_kê°œì˜ ìš”ì†Œê°€ ë”í•´ì§€ë¯€ë¡œ dim_kì™€ ë¹„ë¡€í•´ì„œ ì ê³± ê°’ì´ ê²°ì •ë¨. ê·¸ë˜ì„œ dim_kì˜ ì œê³±ê·¼ìœ¼ë¡œ ë‚˜ëˆ„ë©´ í†µê³„ì ìœ¼ë¡œ í‘œì¤€í™”ì™€ ë¹„ìŠ·í•œ ì—­í• ì„ í•œë‹¤.\nweights = F.softmax(scores, dim=-1) # softmaxë¥¼ ì‚¬ìš©\n\n\n# step 5: ìµœì¢…ì ìœ¼ë¡œ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ì™€ valueë¥¼ ì ê³±\nattn_outputs = torch.bmm(weights, value) # ë§ˆì§€ë§‰ìœ¼ë¡œ valueì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ì¤€ë‹¤.\nattn_outputs.size()\n\n\n\n\n/root/anaconda3/envs/asdf/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\ntorch.Size([1, 5, 768])\n\n\n\n\n\n\n# scaled_dot_product_attention í•¨ìˆ˜ë¡œ ì •ì˜\ndef scaled_dot_product_attention(query, key, value):\n    dim_k = query.size(-1)\n    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n    weights = F.softmax(scores, dim=-1)\n    return torch.bmm(weights, value)\n\n# ì•ì—ì„œëŠ” q,k,vë¥¼ ëª¨ë‘ ê°™ì€ ê°’ì„ ì‚¬ìš©í–ˆì§€ë§Œ ì‹¤ì œë¡œëŠ” ë…ë¦½ì ì¸ ì„ í˜• ë³€í™˜ 3ê°œë¥¼ ì´ìš©í•´ì„œ q,k,vë¥¼ ìƒì„±í•œë‹¤.\nclass AttentionHead(nn.Module):\n    def __init__(self, embed_dim, head_dim):\n        super().__init__()\n        self.q = nn.Linear(embed_dim, head_dim)\n        self.k = nn.Linear(embed_dim, head_dim)\n        self.v = nn.Linear(embed_dim, head_dim)\n\n    def forward(self, hidden_state):\n        attn_outputs = scaled_dot_product_attention(\n            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n        return attn_outputs\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.hidden_size\n        num_heads = config.num_attention_heads\n        head_dim = embed_dim // num_heads # í—¤ë“œë§ˆë‹¤ ê³„ì‚°ì´ ì¼ì •í•˜ë„ë¡ ì„ íƒ\n        self.heads = nn.ModuleList(\n            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n        ) # ì–´í…ì…˜ í—¤ë“œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥í•˜ê³  ê°ê°ì˜ ì–´í…ì…˜ í—¤ë“œê°€ ì…ë ¥(hidden_state)ì„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ\n        # ì–´í…ì…˜ í—¤ë“œëŠ” self attentionì„ ìˆ˜í–‰í•˜ëŠ” ì‘ì€ ëª¨ë“ˆì´ê³  ê·¸ê²ƒì„ ê²°í•©í•˜ëŠ” ê²ƒì´ multi head attentionì´ë‹¤.\n        self.output_linear = nn.Linear(embed_dim, embed_dim) # ì–´í…ì…˜ í—¤ë“œë¥¼ ì—°ê²°í•˜ê³  ìµœì¢…ì ìœ¼ë¡œ ì„ í˜•ë³€í™˜\n\n    def forward(self, hidden_state):\n        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1) #ê°ê°ì—ì„œ attentionì„ ì‹¤í–‰í•˜ê³  í•©ì¹˜ëŠ” ì½”ë“œ\n        x = self.output_linear(x) # ìµœì¢… ì„ í˜•ë³€í™˜\n        return x\n\n\nmultihead_attn = MultiHeadAttention(config)\nattn_output = multihead_attn(inputs_embeds)\nattn_output.size()\n\ntorch.Size([1, 5, 768])\n\n\n\n\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.gelu = nn.GELU()\n        self.dropout = nn.Dropout(config.hidden_dropout_prob) # ì¼ë¶€ ë‰´ëŸ°ì„ ëœë¤í•˜ê²Œ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ íŠ¹ì • ë‰´ëŸ°ì— ì˜ì¡´í•˜ëŠ” ê²ƒì„ ë§‰ëŠ”ë‹¤.\n\n    def forward(self, x):\n        x = self.linear_1(x)\n        x = self.gelu(x)\n        x = self.linear_2(x)\n        x = self.dropout(x)\n        return x\n\n\nfeed_forward = FeedForward(config)\nff_outputs = feed_forward(attn_outputs)\nff_outputs.size()\n\ntorch.Size([1, 5, 768])\n\n\n\n\n\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n        self.attention = MultiHeadAttention(config)\n        self.feed_forward = FeedForward(config)\n\n    def forward(self, x):\n        # ì¸µ ì •ê·œí™”ë¥¼ ì ìš©í•˜ê³  ì…ë ¥ì„ ì¿¼ë¦¬, í‚¤, ê°’ìœ¼ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.\n        hidden_state = self.layer_norm_1(x)\n        x = x + self.attention(hidden_state) # ì–´í…ì…˜ ì—°ì‚° í›„ ìŠ¤í‚µì—°ê²°\n        x = x + self.feed_forward(self.layer_norm_2(x)) # í”¼ë“œí¬ì›Œë“œ ì—°ì‚° í›„ ìŠ¤í‚µì—°ê²°\n        # ìŠ¤í‚µì—°ê²°ì€ ê¸°ìš¸ê¸°ì†Œì‹¤ ë¬¸ì œ ì™„í™”, í•™ìŠµ ì•ˆì •í™”\n        return x\n\n\nencoder_layer = TransformerEncoderLayer(config)\ninputs_embeds.shape, encoder_layer(inputs_embeds).size()\n\n(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))\n\n\n\n\n\n\nclass Embeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.token_embeddings = nn.Embedding(config.vocab_size,\n                                             config.hidden_size)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n                                                config.hidden_size)\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout()\n\n    def forward(self, input_ids):\n        # ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ìœ„ì¹˜ IDë¥¼ ë§Œë“­ë‹ˆë‹¤.\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n        # í† í° ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n        token_embeddings = self.token_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        # í† í° ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì„ë² ë”©ì„ í•©ì¹©ë‹ˆë‹¤.\n        embeddings = token_embeddings + position_embeddings\n        embeddings = self.layer_norm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nembedding_layer = Embeddings(config)\nembedding_layer(inputs.input_ids).size()\n\ntorch.Size([1, 5, 768])\n\n\n\n\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.embeddings = Embeddings(config)\n        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n                                     for _ in range(config.num_hidden_layers)])\n\n    def forward(self, x):\n        x = self.embeddings(x)\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nencoder = TransformerEncoder(config)\nencoder(inputs.input_ids).size()\n\ntorch.Size([1, 5, 768])\n\n\n\n\n\n\nclass TransformerForSequenceClassification(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = TransformerEncoder(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, x):\n        x = self.encoder(x)[:, 0, :] # [CLS] í† í°ì˜ ì€ë‹‰ ìƒíƒœë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n        # [CLS]ëŠ” ë¬¸ì¥ì˜ ì „ì²´ ì˜ë¯¸ë¥¼ ëŒ€í‘œí•˜ëŠ” ë²¡í„°ë¡œ í•™ìŠµë˜ê¸°ë•Œë¬¸ì— [CLS]ë§Œ ê°€ì ¸ì˜¤ëŠ” ê²ƒ\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n\nconfig.num_labels = 3\nencoder_classifier = TransformerForSequenceClassification(config)\nencoder_classifier(inputs.input_ids).size()\n\ntorch.Size([1, 3])"
  },
  {
    "objectID": "posts/Transformer_architecture.html#step-1.-scaled-dot-product-attetion",
    "href": "posts/Transformer_architecture.html#step-1.-scaled-dot-product-attetion",
    "title": "Transformer architecture",
    "section": "",
    "text": "from transformers import AutoTokenizer\nfrom bertviz.transformers_neuron_view import BertModel\nfrom bertviz.neuron_view import show\nfrom torch import nn\nfrom transformers import AutoConfig\nimport torch\nfrom math import sqrt\nimport torch.nn.functional as F\n\n# step 1: ëª¨ë¸, í† í¬ë‚˜ì´ì €, text ì„ ì–¸\nmodel_ckpt = \"bert-base-uncased\" # ì›í•˜ëŠ” ëª¨ë¸ ì„ ì–¸\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt) # ëª¨ë¸ì—ì„œ ì‚¬ìš©ëœ pretrained ëœ tokenizerë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.\nmodel = BertModel.from_pretrained(model_ckpt) # pretrained ëœ modelì„ ë¶ˆëŸ¬ì˜¨ë‹¤.\ntext = \"time flies like an arrow\"\n\n\n# step 2: í† í¬ë‚˜ì´ì§•\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False) # pytorchë¥¼ ì‚¬ìš©í•´ì„œ CLS,SEP í† í°ì„ ì œì™¸í•˜ì—¬ í† í¬ë‚˜ì´ì§•í•˜ëŠ” ì½”ë“œ\ninputs.input_ids # ê²°ê³¼ëŠ” í† í¬ë‚˜ì´ì €ì— ìˆëŠ” ì–´íœ˜ì‚¬ì „ì— ê³ ìœ í•œ IDì— ë§¤í•‘ëœ ê°’ì´ë‹¤.\n\n\n# step 3: ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì„ë² ë”©\nconfig = AutoConfig.from_pretrained(model_ckpt) # configë€ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ì €ì¥í•˜ëŠ” ê°ì²´, ëª¨ë¸ì—ì„œ pretrainedëœ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œ.\ntoken_emb = nn.Embedding(config.vocab_size, config.hidden_size) # ì„ë² ë”©ì˜ ì¸µì„ ìƒì„±í•˜ëŠ” ê²ƒ\n#config.vocab_sizeëŠ” ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ì‚¬ì „ì˜ í¬ê¸°. config.hidden_sizeëŠ” ëª¨ë¸ì˜ ì„ë² ë”© ì°¨ì›ì´ë‹¤. ê° ë‹¨ì–´ë¥¼ ëª‡ ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜í• ì§€ì— ëŒ€í•œ ê²ƒì´ë‹¤\n# ê²°ê³¼ì˜ 30522ëŠ” ë‹¨ì–´ì‚¬ì „ì˜ í¬ê¸°ì´ê³  768ì€ ì„ë² ë”© ì°¨ì›ì´ë‹¤.\ninputs_embeds = token_emb(inputs.input_ids) #ìœ„ì—ì„œ ì„ ì–¸í•œ token_embì„ ì´ìš©í•´ì„œ 5ê°œì˜ ë‹¨ì–´ë¥¼ ì„ë² ë”©ì‹œì¼°ë‹¤.\n# ì„ë² ë”© ì‹œì¼°ë‹¤ëŠ” ê²ƒì€ ê° ë‹¨ì–´ê°€ 768ê°œì˜ íŠ¹ì„±ì„ ê°€ì§€ëŠ” ë²¡í„°ê°’ìœ¼ë¡œ ë³€í™˜ëœ ê²ƒì´ë¼ê³  ì´í•´í•˜ë©´ ëœë‹¤.\ninputs_embeds.size()\n\n\n# step 4: ì ê³±ê³¼ softmaxë¥¼ ì´ìš©í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°\nquery = key = value = inputs_embeds\ndim_k = key.size(-1) # ì„ë² ë”© ì°¨ì›ì„ ì„ íƒ\nscores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k) # ì ê³±ì„ ì‚¬ìš©. self attentionì˜ ê²½ìš° batchë‹¨ìœ„ë¡œ ì—°ì‚°ì´ í•„ìš”í•˜ê¸°ì— torch.bmmì„ ì‚¬ìš©\n# ì ê³±ì„ ì§„í–‰í•˜ë©´ dim_kê°œì˜ ìš”ì†Œê°€ ë”í•´ì§€ë¯€ë¡œ dim_kì™€ ë¹„ë¡€í•´ì„œ ì ê³± ê°’ì´ ê²°ì •ë¨. ê·¸ë˜ì„œ dim_kì˜ ì œê³±ê·¼ìœ¼ë¡œ ë‚˜ëˆ„ë©´ í†µê³„ì ìœ¼ë¡œ í‘œì¤€í™”ì™€ ë¹„ìŠ·í•œ ì—­í• ì„ í•œë‹¤.\nweights = F.softmax(scores, dim=-1) # softmaxë¥¼ ì‚¬ìš©\n\n\n# step 5: ìµœì¢…ì ìœ¼ë¡œ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ì™€ valueë¥¼ ì ê³±\nattn_outputs = torch.bmm(weights, value) # ë§ˆì§€ë§‰ìœ¼ë¡œ valueì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ì¤€ë‹¤.\nattn_outputs.size()\n\n\n\n\n/root/anaconda3/envs/asdf/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\ntorch.Size([1, 5, 768])"
  },
  {
    "objectID": "posts/Transformer_architecture.html#step-2.-multi-head-attention",
    "href": "posts/Transformer_architecture.html#step-2.-multi-head-attention",
    "title": "Transformer architecture",
    "section": "",
    "text": "# scaled_dot_product_attention í•¨ìˆ˜ë¡œ ì •ì˜\ndef scaled_dot_product_attention(query, key, value):\n    dim_k = query.size(-1)\n    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n    weights = F.softmax(scores, dim=-1)\n    return torch.bmm(weights, value)\n\n# ì•ì—ì„œëŠ” q,k,vë¥¼ ëª¨ë‘ ê°™ì€ ê°’ì„ ì‚¬ìš©í–ˆì§€ë§Œ ì‹¤ì œë¡œëŠ” ë…ë¦½ì ì¸ ì„ í˜• ë³€í™˜ 3ê°œë¥¼ ì´ìš©í•´ì„œ q,k,vë¥¼ ìƒì„±í•œë‹¤.\nclass AttentionHead(nn.Module):\n    def __init__(self, embed_dim, head_dim):\n        super().__init__()\n        self.q = nn.Linear(embed_dim, head_dim)\n        self.k = nn.Linear(embed_dim, head_dim)\n        self.v = nn.Linear(embed_dim, head_dim)\n\n    def forward(self, hidden_state):\n        attn_outputs = scaled_dot_product_attention(\n            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n        return attn_outputs\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.hidden_size\n        num_heads = config.num_attention_heads\n        head_dim = embed_dim // num_heads # í—¤ë“œë§ˆë‹¤ ê³„ì‚°ì´ ì¼ì •í•˜ë„ë¡ ì„ íƒ\n        self.heads = nn.ModuleList(\n            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n        ) # ì–´í…ì…˜ í—¤ë“œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥í•˜ê³  ê°ê°ì˜ ì–´í…ì…˜ í—¤ë“œê°€ ì…ë ¥(hidden_state)ì„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ\n        # ì–´í…ì…˜ í—¤ë“œëŠ” self attentionì„ ìˆ˜í–‰í•˜ëŠ” ì‘ì€ ëª¨ë“ˆì´ê³  ê·¸ê²ƒì„ ê²°í•©í•˜ëŠ” ê²ƒì´ multi head attentionì´ë‹¤.\n        self.output_linear = nn.Linear(embed_dim, embed_dim) # ì–´í…ì…˜ í—¤ë“œë¥¼ ì—°ê²°í•˜ê³  ìµœì¢…ì ìœ¼ë¡œ ì„ í˜•ë³€í™˜\n\n    def forward(self, hidden_state):\n        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1) #ê°ê°ì—ì„œ attentionì„ ì‹¤í–‰í•˜ê³  í•©ì¹˜ëŠ” ì½”ë“œ\n        x = self.output_linear(x) # ìµœì¢… ì„ í˜•ë³€í™˜\n        return x\n\n\nmultihead_attn = MultiHeadAttention(config)\nattn_output = multihead_attn(inputs_embeds)\nattn_output.size()\n\ntorch.Size([1, 5, 768])"
  },
  {
    "objectID": "posts/Transformer_architecture.html#step-3.-feed-forward-layer",
    "href": "posts/Transformer_architecture.html#step-3.-feed-forward-layer",
    "title": "Transformer architecture",
    "section": "",
    "text": "class FeedForward(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.gelu = nn.GELU()\n        self.dropout = nn.Dropout(config.hidden_dropout_prob) # ì¼ë¶€ ë‰´ëŸ°ì„ ëœë¤í•˜ê²Œ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ íŠ¹ì • ë‰´ëŸ°ì— ì˜ì¡´í•˜ëŠ” ê²ƒì„ ë§‰ëŠ”ë‹¤.\n\n    def forward(self, x):\n        x = self.linear_1(x)\n        x = self.gelu(x)\n        x = self.linear_2(x)\n        x = self.dropout(x)\n        return x\n\n\nfeed_forward = FeedForward(config)\nff_outputs = feed_forward(attn_outputs)\nff_outputs.size()\n\ntorch.Size([1, 5, 768])"
  },
  {
    "objectID": "posts/Transformer_architecture.html#step-4.-layer-normalization",
    "href": "posts/Transformer_architecture.html#step-4.-layer-normalization",
    "title": "Transformer architecture",
    "section": "",
    "text": "class TransformerEncoderLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n        self.attention = MultiHeadAttention(config)\n        self.feed_forward = FeedForward(config)\n\n    def forward(self, x):\n        # ì¸µ ì •ê·œí™”ë¥¼ ì ìš©í•˜ê³  ì…ë ¥ì„ ì¿¼ë¦¬, í‚¤, ê°’ìœ¼ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.\n        hidden_state = self.layer_norm_1(x)\n        x = x + self.attention(hidden_state) # ì–´í…ì…˜ ì—°ì‚° í›„ ìŠ¤í‚µì—°ê²°\n        x = x + self.feed_forward(self.layer_norm_2(x)) # í”¼ë“œí¬ì›Œë“œ ì—°ì‚° í›„ ìŠ¤í‚µì—°ê²°\n        # ìŠ¤í‚µì—°ê²°ì€ ê¸°ìš¸ê¸°ì†Œì‹¤ ë¬¸ì œ ì™„í™”, í•™ìŠµ ì•ˆì •í™”\n        return x\n\n\nencoder_layer = TransformerEncoderLayer(config)\ninputs_embeds.shape, encoder_layer(inputs_embeds).size()\n\n(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))"
  },
  {
    "objectID": "posts/Transformer_architecture.html#step-5.-ìœ„ì¹˜-ì„ë² ë”©",
    "href": "posts/Transformer_architecture.html#step-5.-ìœ„ì¹˜-ì„ë² ë”©",
    "title": "Transformer architecture",
    "section": "",
    "text": "class Embeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.token_embeddings = nn.Embedding(config.vocab_size,\n                                             config.hidden_size)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n                                                config.hidden_size)\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout()\n\n    def forward(self, input_ids):\n        # ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ìœ„ì¹˜ IDë¥¼ ë§Œë“­ë‹ˆë‹¤.\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n        # í† í° ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n        token_embeddings = self.token_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        # í† í° ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì„ë² ë”©ì„ í•©ì¹©ë‹ˆë‹¤.\n        embeddings = token_embeddings + position_embeddings\n        embeddings = self.layer_norm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nembedding_layer = Embeddings(config)\nembedding_layer(inputs.input_ids).size()\n\ntorch.Size([1, 5, 768])"
  },
  {
    "objectID": "posts/Transformer_architecture.html#step-6.-ì„ë² ë”©ê³¼-ì¸ì½”ë”-ì¸µ-ì—°ê²°",
    "href": "posts/Transformer_architecture.html#step-6.-ì„ë² ë”©ê³¼-ì¸ì½”ë”-ì¸µ-ì—°ê²°",
    "title": "Transformer architecture",
    "section": "",
    "text": "class TransformerEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.embeddings = Embeddings(config)\n        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n                                     for _ in range(config.num_hidden_layers)])\n\n    def forward(self, x):\n        x = self.embeddings(x)\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nencoder = TransformerEncoder(config)\nencoder(inputs.input_ids).size()\n\ntorch.Size([1, 5, 768])"
  },
  {
    "objectID": "posts/Transformer_architecture.html#step-7.-ë¶„ë¥˜-í—¤ë“œ-ì¶”ê°€",
    "href": "posts/Transformer_architecture.html#step-7.-ë¶„ë¥˜-í—¤ë“œ-ì¶”ê°€",
    "title": "Transformer architecture",
    "section": "",
    "text": "class TransformerForSequenceClassification(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = TransformerEncoder(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, x):\n        x = self.encoder(x)[:, 0, :] # [CLS] í† í°ì˜ ì€ë‹‰ ìƒíƒœë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n        # [CLS]ëŠ” ë¬¸ì¥ì˜ ì „ì²´ ì˜ë¯¸ë¥¼ ëŒ€í‘œí•˜ëŠ” ë²¡í„°ë¡œ í•™ìŠµë˜ê¸°ë•Œë¬¸ì— [CLS]ë§Œ ê°€ì ¸ì˜¤ëŠ” ê²ƒ\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n\nconfig.num_labels = 3\nencoder_classifier = TransformerForSequenceClassification(config)\nencoder_classifier(inputs.input_ids).size()\n\ntorch.Size([1, 3])"
  },
  {
    "objectID": "posts/Transformer_architecture.html#step-1.-mask-matrix",
    "href": "posts/Transformer_architecture.html#step-1.-mask-matrix",
    "title": "Transformer architecture",
    "section": "step 1. Mask matrix",
    "text": "step 1. Mask matrix\n- ë§ˆìŠ¤í¬ í–‰ë ¬\n\nseq_len = inputs.input_ids.size(-1) # ë§ˆì§€ë§‰ ì°¨ì›ì˜ í¬ê¸°ì´ë©° í† í° ê°œìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.\nmask = torch.tril(torch.ones(seq_len,seq_len)).unsqueeze(0) # unsqueezeë¥¼ ì·¨í•˜ë©´ ì¸ë±ìŠ¤ 0ë²ˆ ìë¦¬ì— í•´ë‹¹í•˜ëŠ” ì°¨ì›ì´ í•˜ë‚˜ ëŠ˜ì–´ë‚œë‹¤.\n\n- maskì—ì„œ .unsqueeze(0)ì„ í•˜ëŠ” ì´ìœ \nunsqueezeë¥¼ ì·¨í•˜ë©´ ì¸ë±ìŠ¤ 0ë²ˆ ìë¦¬ì— í•´ë‹¹í•˜ëŠ” ì°¨ì›ì´ í•˜ë‚˜ ëŠ˜ì–´ë‚œë‹¤. ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ í•œ ë²ˆì— ì—¬ëŸ¬ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ batchí˜•íƒœë¡œ ê³„ì‚°í•œë‹¤.\níŠ¹íˆ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ attention maskë¥¼ ì‚¬ìš©í•  ë•Œ ë°°ì¹˜ì°¨ì›ê¹Œì§€ ì¶”ê°€ëœ í˜•íƒœë¥¼ ê¸°ëŒ€í•˜ê¸°ì— .unsqueeze(0)ì„ ì´ìš©í•´ì„œ ë§ì¶°ì£¼ëŠ” ê²ƒì´ë‹¤.\n- ì˜ë¬¸ì \nQ. seqì˜ ê¸¸ì´ê°€ input idsì˜ ë§ˆì§€ë§‰ ì°¨ì›ì´ë©´ í† í° ê°œìˆ˜ì¸ë° ê·¸ëŸ¼ ì¶œë ¥ ê¸¸ì´ê°€ ì…ë ¥ ê¸¸ì´ì™€ ê°™ë‹¤ëŠ” ì˜ë¯¸ì¸ê°€ìš”?\nA. ë””ì½”ë”ëŠ” â€™ì¶œë ¥ ì‹œí€€ìŠ¤â€™ì— ëŒ€í•´ self attentionì„ ìˆ˜í–‰í•œë‹¤. ì¦‰ ì§€ê¸ˆê¹Œì§€ ìƒì„±í•œ ì¶œë ¥ í† í°ë“¤ë¼ë¦¬ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤. seq_lenì€ ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ ì¶œë ¥ì˜ ê¸¸ì´ë¥¼ ì˜ë¯¸í•œë‹¤.\n\nprint(scores)\nprint(scores.masked_fill(mask == 0, -float('inf')))\n\ntensor([[[26.5910,  1.8838, -0.6318,  0.3911, -0.6419],\n         [ 1.8838, 28.6324, -0.9585,  0.3705,  1.0006],\n         [-0.6318, -0.9585, 26.4333, -0.5234, -0.8233],\n         [ 0.3911,  0.3705, -0.5234, 28.6680, -0.4031],\n         [-0.6419,  1.0006, -0.8233, -0.4031, 27.1448]]],\n       grad_fn=&lt;DivBackward0&gt;)\ntensor([[[26.5910,    -inf,    -inf,    -inf,    -inf],\n         [ 1.8838, 28.6324,    -inf,    -inf,    -inf],\n         [-0.6318, -0.9585, 26.4333,    -inf,    -inf],\n         [ 0.3911,  0.3705, -0.5234, 28.6680,    -inf],\n         [-0.6419,  1.0006, -0.8233, -0.4031, 27.1448]]],\n       grad_fn=&lt;MaskedFillBackward0&gt;)\n\n\n- -infë¡œ ì„¤ì •í•˜ëŠ” ì´ìœ \nëŒ€ê°ì„  ìœ„ì˜ ê°’ì„ ìŒì˜ ë¬´í•œëŒ€ë¡œ ì„¤ì •í•˜ë©´, ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì ìš©í•  ë•Œ \\(e^{-inf}\\) = 0 ì´ë¯€ë¡œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ê°€ ëª¨ë‘ 0ì´ ëœë‹¤.\n- softmax ê°’ì´ 0ì´ ë˜ë©´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€?\nselt-attetion ê³„ì‚°ì€ softmax(Q @ K.T / sqrt(d_k)) ì—ì„œ Q @ K.Tì„ ì—°ì‚°í•œ í›„ì— ê·¸ ê²°ê³¼ì— masked_fillì„ ì´ìš©í•´ì„œ í•˜ì‚¼ê°í–‰ë ¬ë¡œ ë§Œë“ ë‹¤. ê·¸ë˜ì•¼ self-attentionì´ ì–´ë””ë¥¼ ë³´ë©´ ì•ˆë˜ëŠ”ì§€ ì§€ì •í•´ì¤€ë‹¤.\n\ndef scaled_dot_product_attetion(query, key, value, mask=None):\n    dim_k = query.size(-1)\n    scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k) # bmm = batch matrix multiplication ì¦‰, ë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ í–‰ë ¬ ê³±ì…ˆì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜\n    if mask is not None:\n        scores = socres.masked_fill(mask==0, float('-inf'))\n    weights = F.softmax(scores,dim = -1)\n    return weights.bmm(value)\n\nì—¬ê¸°ì„œ if mask is not None ì¡°ê±´ë¬¸ì€ ì–´ë–»ê²Œ ì‹¤í–‰ë˜ëŠ”ì§€ ê¶ê¸ˆí•  ìˆ˜ ìˆë‹¤. (ë‚´ê°€ ê¶ê¸ˆí–ˆìŒ)\nmaskëŠ” í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•  ë•Œ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ì¸ì(argument)ì´ë‹¤. ë³´í†µ í•˜ì‚¼ê°í–‰ë ¬ í˜•íƒœë¡œ ë°›ëŠ”ë‹¤.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom math import sqrt\n\n# ë””ì½”ë” ì¸µ êµ¬í˜„\nclass DecoderLayer(nn.Module):\n    def __init__(self, dim_model, num_heads, dim_feedforward, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        \n        # Self-attention (masked)\n        self.self_attention = nn.MultiheadAttention(embed_dim=dim_model, num_heads=num_heads, dropout=dropout)\n        \n        # Encoder-Decoder attention\n        self.enc_dec_attention = nn.MultiheadAttention(embed_dim=dim_model, num_heads=num_heads, dropout=dropout)\n        \n        # Feedforward network\n        self.feedforward = nn.Sequential(\n            nn.Linear(dim_model, dim_feedforward),\n            nn.ReLU(),\n            nn.Linear(dim_feedforward, dim_model)\n        )\n        \n        # Layer normalization\n        self.layer_norm1 = nn.LayerNorm(dim_model)\n        self.layer_norm2 = nn.LayerNorm(dim_model)\n        self.layer_norm3 = nn.LayerNorm(dim_model)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n        # Self-attention (masked)\n        attn_output, _ = self.self_attention(tgt, tgt, tgt, attn_mask=tgt_mask) # tgt = ë””ì½”ë” ì…ë ¥ í† í°\n        tgt = self.layer_norm1(tgt + self.dropout(attn_output))\n        \n        # Encoder-Decoder attention\n        attn_output, _ = self.enc_dec_attention(tgt, memory, memory, attn_mask=memory_mask) # ë””ì½”ë”ê°€ ì¸ì½”ë”ì˜ ì •ë³´ë¥¼ ì°¸ê³ í•¨\n        tgt = self.layer_norm2(tgt + self.dropout(attn_output))\n        \n        # Feedforward \n        ff_output = self.feedforward(tgt)\n        tgt = self.layer_norm3(tgt + self.dropout(ff_output))\n        \n        return tgt\n\n# ë””ì½”ë” ëª¨ë¸ ì „ì²´ êµ¬ì„±\nclass TransformerDecoder(nn.Module):\n    def __init__(self, num_layers, dim_model, num_heads, dim_feedforward, vocab_size, dropout=0.1):\n        super(TransformerDecoder, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, dim_model)  # ì„ë² ë”© ë ˆì´ì–´\n        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, dim_model)) # ìœ„ì¹˜ ì¸ì½”ë”© \n        \n        # ì—¬ëŸ¬ ê°œì˜ ë””ì½”ë” ì¸µ\n        self.decoder_layers = nn.ModuleList([DecoderLayer(dim_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers)])\n        \n        self.output_layer = nn.Linear(dim_model, vocab_size)  # ìµœì¢… ì¶œë ¥ (ì–´íœ˜ í¬ê¸°)\n    \n    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n        # ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©\n        tgt = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n        \n        # ì—¬ëŸ¬ ê°œì˜ ë””ì½”ë” ì¸µì„ í†µê³¼\n        for layer in self.decoder_layers:\n            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n        \n        # ì¶œë ¥ ë ˆì´ì–´\n        output = self.output_layer(tgt)\n        return output\n\n\ndim_model = 512  # ëª¨ë¸ ì°¨ì›\nnum_heads = 8  # í—¤ë“œ ê°œìˆ˜\ndim_feedforward = 2048  # í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì°¨ì›\nvocab_size = 10000  # ì–´íœ˜ í¬ê¸° \nnum_layers = 6  # ë””ì½”ë” ì¸µ ìˆ˜\n\ndecoder = TransformerDecoder(num_layers, dim_model, num_heads, dim_feedforward, vocab_size)\n\n# í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±\nbatch_size = 2  # ë°°ì¹˜ í¬ê¸°\nseq_len = 5  # ì‹œí€€ìŠ¤ ê¸¸ì´\n\n# íƒ€ê²Ÿ ì‹œí€€ìŠ¤ (ì„ì˜ì˜ í† í° ì¸ë±ìŠ¤)\ntgt = torch.randint(0, vocab_size, (batch_size, seq_len))  # (ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´)\n\n# ë©”ëª¨ë¦¬ (ì¸ì½”ë”ì˜ ì¶œë ¥, ì„ì˜ë¡œ ìƒì„±)\nmemory = torch.randn(batch_size, seq_len, dim_model)  # (ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, ëª¨ë¸ ì°¨ì›)\n\n# ë§ˆìŠ¤í¬ëŠ” Noneìœ¼ë¡œ ì„¤ì • (ë§ˆìŠ¤í¬ê°€ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©´ None ì‚¬ìš©)\ntgt_mask = None\nmemory_mask = None\n\n# ë””ì½”ë” ì‹¤í–‰\noutput = decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n\n# ê²°ê³¼ ì¶œë ¥\nprint(f\"Output shape: {output.shape}\")\n\nOutput shape: torch.Size([2, 5, 10000])"
  },
  {
    "objectID": "posts/Based_on_Encoder.html",
    "href": "posts/Based_on_Encoder.html",
    "title": "Base on Encoder models",
    "section": "",
    "text": "import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel\n\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)\n\n\n- í´ë˜ìŠ¤ê°€ ì˜ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸\n\nmodel.config.id2label\n\n{0: 'LABEL_0', 1: 'LABEL_1'}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#prediction",
    "href": "posts/Based_on_Encoder.html#prediction",
    "title": "Base on Encoder models",
    "section": "1-3. Prediction",
    "text": "1-3. Prediction\n\nwith torch.no_grad():\n    logits = model(**batch).logits\n\nlogits\n\ntensor([[-0.6647,  0.5699],\n        [ 0.2459, -0.5885],\n        [-0.3337,  0.2817],\n        [ 0.1649,  0.0235],\n        [-0.5286,  0.4380],\n        [ 0.7408,  0.0513],\n        [-0.3665,  0.6204],\n        [ 0.6414, -0.6416],\n        [ 0.1279, -0.2553],\n        [-0.3801,  0.3907]])\n\n\n\npred_labels = logits.argmax(dim=1).cpu().numpy()\ntrue_labels = batch['labels'].numpy()\nprint(pred_labels)\nprint(true_labels)\n\n[1 0 1 0 1 0 1 0 0 1]\n[1 0 0 0 1 0 1 0 0 1]\n\n\n\nimport evaluate\n\nf1 = evaluate.load('f1')\nf1.compute(predictions = pred_labels, references = true_labels, average='micro')\n\n{'f1': 0.9}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#predcition",
    "href": "posts/Based_on_Encoder.html#predcition",
    "title": "Base on Encoder models",
    "section": "2-2. Predcition",
    "text": "2-2. Predcition\n\nwith torch.no_grad():\n    logits = model(**batch).logits\n\nlogits\n\ntensor([[-0.0012],\n        [-0.2672],\n        [ 0.1028],\n        [-0.3430],\n        [-0.0204],\n        [-0.2547],\n        [ 0.0901],\n        [-0.6130],\n        [-0.4369],\n        [ 0.0066]])"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#ë¬¸ì¥ë¶„ë¥˜-vs-ë‹¤ì¤‘-ë¶„ë¥˜",
    "href": "posts/Based_on_Encoder.html#ë¬¸ì¥ë¶„ë¥˜-vs-ë‹¤ì¤‘-ë¶„ë¥˜",
    "title": "Base on Encoder models",
    "section": "ë¬¸ì¥ë¶„ë¥˜ vs ë‹¤ì¤‘ ë¶„ë¥˜",
    "text": "ë¬¸ì¥ë¶„ë¥˜ vs ë‹¤ì¤‘ ë¶„ë¥˜\në¬¸ì¥ ë¶„ë¥˜: ë¬¸ì¥ í•œ ê°œë‹¹ Nê°œì˜ í™•ë¥  ì¶œë ¥ (N = í´ë˜ìŠ¤ì˜ ìˆ˜)\në‹¤ì¤‘ ë¶„ë¥˜: Nê°œì˜ ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ ë¬¸ì¥ë‹¹ í•œ ê°œì”©, ì´ Nê°œ í™•ë¥  ì¶”ì¶œ (N = ê°ê´€ì‹ ë³´ê¸° ê°œìˆ˜)"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#dataset-1",
    "href": "posts/Based_on_Encoder.html#dataset-1",
    "title": "Base on Encoder models",
    "section": "3-2. Dataset",
    "text": "3-2. Dataset\nìˆ˜ëŠ¥ êµ­ì–´ ë¬¸ì œ\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"HAERAE-HUB/csatqa\", \"full\")\nprint(dataset[\"test\"][0])\n\nending_names = [\"option#1\", \"option#2\", \"option#3\", \"option#4\", \"option#5\"]\n\ndef preprocess_function(examples): # examples ìë¦¬ì— datasetì˜ batchê°€ ë“¤ì–´ê°„ë‹¤.\n  first_sentences = [\n      [context] * 5 for context in examples[\"context\"] # ê° ë¬¸í•­ì— 5ê°œì˜ ì„ íƒì§€ê°€ ìˆë‹¤. ê° ì„ íƒì§€ë§ˆë‹¤ ë™ì¼í•œ contextë¥¼ ì‚¬ìš©í•´ì•¼í•¨.\n  ]\n  question_headers = examples[\"question\"]\n  second_sentences = [\n      [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers) # ê° ì§ˆë¬¸ê³¼ ì„ íƒì§€ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” ì—­í• \n      # 1. enumerate(question_headers) â†’ question_headersì—ì„œ (index, question_text) ìŒì„ ê°€ì ¸ì˜´.\n      # 2. for end in ending_names â†’ \"option#1\" ~ \"option#5\"ê¹Œì§€ ëŒë©´ì„œ í•´ë‹¹ ì„ íƒì§€ë¥¼ ê°€ì ¸ì˜´.\n      # 3. f\"{header} {examples[end][i]}\" â†’ ê° ì§ˆë¬¸(header)ê³¼ í•´ë‹¹ ì„ íƒì§€ë¥¼ í•©ì¹œ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ìƒì„±.\n  ]\n  # í† í°í™”ë¥¼ ìœ„í•´ 1ì°¨ì›ìœ¼ë¡œ í‰í™œí™”\n  first_sentences = sum(first_sentences, []) # flatten()ê³¼ ê°™ì€ íš¨ê³¼. flatten()ì€ numpyì—ì„œ ë™ì‘í•˜ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ì—ì„œëŠ” sum(ë¦¬ìŠ¤íŠ¸, []) ì‚¬ìš©\n  second_sentences = sum(second_sentences, [])\n\n  # None ë°ì´í„° ì²˜ë¦¬\n  first_sentences = [i if i else \"\" for i in first_sentences] # sentencesì—ì„œ Noneì„ ê³µë°±ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì½”ë“œ. ì¦‰, None ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì„œ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•¨\n  second_sentences = [i if i else \"\" for i in second_sentences]\n\n  tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n  # Multiple Choice ë¬¸ì œì—ì„œëŠ” ì§ˆë¬¸ + ê° ë‹µë³€ì„ ê²°í•©í•´ì•¼í•œë‹¤. ê²°í•©í•˜ê³  ì‹¶ì€ ë¬¸ì¥ì„ ì´ì–´ì„œ ì‘ì„±í•œë‹¤ë©´ ì•Œì•„ì„œ ê²°í•©ëœë‹¤.\n\n  # í† í°í™” í›„ ë‹¤ì‹œ 2ì°¨ì›ìœ¼ë¡œ ì¬ë°°ì—´\n  result = {\n      k: [v[i:i+5] for i in range(0, len(v), 5)] for k, v in tokenized_examples.items()\n  }\n  result[\"labels\"] = [i-1 for i in examples[\"gold\"]]  # këŠ” ë¬¸ì œ(ë¬¸ì œì™€ ë³´ê¸°), vëŠ” ì„ íƒì§€ 5ê°œì´ë‹¤. ë³´ê¸° ì¢‹ì€ 2ì°¨ì› ë°°ì—´ë¡œ ì¬ë°°ì—´\n\n  return result\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)\n\n{'question': ' ì´ ì´ì•¼ê¸°ì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” êµí›ˆìœ¼ë¡œ ê°€ì¥ ì ì ˆí•œ ê²ƒì€?', 'context': 'ì´ì œ í•œ í¸ì˜ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ ë“œë¦½ë‹ˆë‹¤. ì˜ ë“£ê³  ë¬¼ìŒì— ë‹µí•˜ì‹­ì‹œì˜¤.\\nì, ì—¬ëŸ¬ë¶„! ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ? ì˜¤ëŠ˜ì€ ì œê°€ ì–´ì œ ê¾¼ ê¿ˆ ì´ì•¼ê¸° í•˜ë‚  ë“¤ë ¤ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì „ ê¿ˆì†ì—ì„œ ë‚¯ì„  ê±°ë¦¬ë¥¼ ê±·ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‹¤ê°€ í™ë¯¸ë¡œìš´ ê°„íŒì„ ë°œê²¬í–ˆë‹µë‹ˆë‹¤. í–‰ ë³µì„ íŒŒëŠ” ê°€ê²Œ. ê·¸ë ‡ê²Œ ì“°ì—¬ ìˆì—ˆìŠµë‹ˆë‹¤. ì „ í˜¸ê¸°ì‹¬ìœ¼ë¡œ ë¬¸ì„ ì—´ê³  ë“¤ì–´ê°”ë‹µë‹ˆë‹¤. ê·¸ê³³ ì—ì„œëŠ” í•œ ë…¸ì¸ì´ ë¬¼ê±´ì„ íŒ”ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ì „ ì ì‹œ ë¨¸ë­‡ê±°ë¦¬ë‹¤ê°€ ë…¸ì¸ì—ê²Œ ë‹¤ê°€ê°€ì„œ ë¬¼ ì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë¬´ìŠ¨ ë¬¼ê±´ì„ íŒŒëŠëƒê³ ìš”. ë…¸ì¸ì€ ë¯¸ì†Œë¥¼ ì§€ìœ¼ë©°, ì›í•˜ëŠ” ê²ƒì€ ë­ë“  ë‹¤ ì‚´ ìˆ˜ ìˆë‹¤ê³  ë§í–ˆìŠµë‹ˆë‹¤. ì €ëŠ” ì œ ê·€ë¥¼ ì˜ì‹¬í–ˆìŠµë‹ˆë‹¤. \\'ë¬´ì—‡ì´ë“  ë‹¤?\\' ì „ ë¬´ì—‡ì„ ì‚¬ì•¼ í• ê¹Œ ìƒê°í•˜ë‹¤ê°€ ë§í–ˆë‹µë‹ˆë‹¤. \"ì‚¬ë‘, ë¶€ê·€ ê·¸ë¦¬ê³  ì§€í˜œí•˜ê³  ê±´ê°•ë„ ì‚¬ê³  ì‹¶ìŠµë‹ˆë‹¤. ì € ìì‹ ë¿ ì•„ë‹ˆë¼ ìš°ë¦¬ ê°€ì¡± ëª¨ë‘ ë¥¼ ìœ„í•´ì„œìš”. ì§€ê¸ˆ ë°”ë¡œ ì‚´ ìˆ˜ ìˆë‚˜ìš”?\" ê·¸ëŸ¬ì ë…¸ì¸ì€ ë¹™ê¸‹ì´ ì›ƒìœ¼ë©° ëŒ€ë‹µí–ˆìŠµë‹ˆë‹¤. \"ì Šì€ì´, í•œë²ˆ ì˜ ë³´ê²Œë‚˜. ì—¬ê¸°ì—ì„œ íŒ”ê³  ìˆëŠ” ê²ƒì€ ë¬´ë¥´ìµì€ ê³¼ì¼ì´ ì•„ë‹ˆë¼ ì”¨ì•—ì´ë¼ ë„¤. ì•ìœ¼ë¡œ ì¢‹ì€ ì—´ë§¤ë¥¼ ë§ºìœ¼ë ¤ë©´ ì´ ì”¨ì•—ë“¤ì„ ì˜ ê°€ê¾¸ì–´ì•¼ í•  ê±¸ì„¸.\"', 'option#1': 'ìƒˆë¡œìš´ ì„¸ê³„ì— ëŒ€í•œ ì—´ë§ì„ ê°€ì ¸ì•¼ í•œë‹¤.', 'option#2': 'ì£¼ì–´ì§„ ê¸°íšŒë¥¼ ëŠ¥ë™ì ìœ¼ë¡œ í™œìš©í•´ì•¼ í•œë‹¤.', 'option#3': 'í° ê²ƒì„ ì–»ìœ¼ë ¤ë©´ ì‘ì€ ê²ƒì€ ë²„ë ¤ì•¼ í•œë‹¤.', 'option#4': 'ë¬¼ì§ˆì  ê°€ì¹˜ë³´ë‹¤ ì •ì‹ ì  ê°€ì¹˜ë¥¼ ì¤‘ì‹œí•´ì•¼ í•œë‹¤.', 'option#5': 'ì†Œë§í•˜ëŠ” ë°”ë¥¼ ì„±ì·¨í•˜ê¸° ìœ„í•´ì„œëŠ” ë…¸ë ¥ì„ í•´ì•¼ í•œë‹¤.', 'gold': 5, 'category': 'N/A', 'human_performance': 0.0}\n\n\n\n\n\në‹¤ì¤‘ ë¶„ë¥˜ taskì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” DataCollatorWithPaddingì„ ì‚¬ìš©í•˜ê¸° ì–´ë µë‹¤.\nì´ë¥¼ ìœ„í•´ íŒ¨ë”© ë“± í•„ìš”í•œ ì‘ì—…ì„ ì§„í–‰í•˜ëŠ” ì½œë ˆì´í„°ë¥¼ ì§ì ‘ ì‘ì„±í•´ì•¼í•œë‹¤.\nê·¸ ì „ ì‘ì„±ëœ ì½œë ˆì´í„°ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„  ì•„ë˜ì˜ ë¬¸ë²•ì„ ì•Œì•„ì•¼í•œë‹¤. ê°„ëµí•˜ê²Œ ì„¤ëª…í• í…Œë‹ˆ ìˆ™ì§€í•˜ê³  ë„˜ì–´ê°€ë„ë¡ í•˜ì."
  },
  {
    "objectID": "posts/Based_on_Encoder.html#ë²ˆì™¸1.-íŒŒì´ì¬-ë¬¸ë²•",
    "href": "posts/Based_on_Encoder.html#ë²ˆì™¸1.-íŒŒì´ì¬-ë¬¸ë²•",
    "title": "Base on Encoder models",
    "section": "ë²ˆì™¸1. íŒŒì´ì¬ ë¬¸ë²•",
    "text": "ë²ˆì™¸1. íŒŒì´ì¬ ë¬¸ë²•\n\n__init__()\n\nê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°ì—ì„œ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ë©´ í•´ë‹¹ í´ë˜ìŠ¤ì˜ ê°ì²´(ì¸ìŠ¤í„´ìŠ¤)ë¥¼ ìƒì„±í•  ë•Œ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œê°€ __init__()ì´ë‹¤.\n\nclass Example:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\nì˜ˆë¥¼ ë“¤ì–´ ìœ„ì™€ ê°™ì€ í´ë˜ìŠ¤ë¥¼ ë§Œë“ ë‹¤ê³  í–ˆì„ ë•Œ, __init__() ë©”ì„œë“œëŠ” í´ë˜ìŠ¤ë¥¼ ì²˜ìŒ ë§Œë“¤ ë•Œ ìë™ìœ¼ë¡œ ì‹¤í–‰ëœë‹¤.\nself.a = a -&gt; aê°’ì„ ê°ì²´ ë‚´ë¶€ì— ì €ì¥\nself.b = b -&gt; bê°’ì„ ê°ì²´ ë‚´ë¶€ì— ì €ì¥\n\nobj = Example(3,5)\nprint(obj.a)\nprint(obj.b)\n\n3\n5\n\n\nì¦‰ __init__()ì€ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ ë•Œ í•„ìš”í•œ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ì—­í• ì„ í•œë‹¤.\n\në°ì½”ë ˆì´í„° + dataclass\n\në°ì½”ë ˆì´í„°(Decorator) ëŠ” í•¨ìˆ˜ë‚˜ í´ë˜ìŠ¤ë¥¼ ê¾¸ë©°ì£¼ëŠ”(ë³€í˜•í•˜ëŠ”) í•¨ìˆ˜ì´ë‹¤. @ì„ ë¶™í˜€ì„œ ì‚¬ìš©í•œë‹¤. @dataclassëŠ” í´ë˜ìŠ¤ì—ì„œ __init__()ì„ ìë™ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Example:\n    a: int\n    b: int\n\nìœ„ì˜ ì½”ë“œì—ì„œ __init__()ì„ ë”°ë¡œ ë§Œë“¤ì§€ ì•Šì•˜ìŒì—ë„ ìë™ ìƒì„±ë˜ì—ˆê³  ë‚´ë¶€ì ìœ¼ë¡œëŠ” ì•„ë˜ì˜ ì½”ë“œì™€ ê°™ì€ ë°©ì‹ì´ë‹¤.\n\ndef __init__(self, a:int, b:int):\n    self.a = a\n    self.b = b\n\nì¶”ê°€ì ìœ¼ë¡œ a: int, b: int ì™€ ê°™ì´ ì“´ ì´ìœ ëŠ” aì™€ bëŠ” int íƒ€ì…ì„ ê¸°ëŒ€í•œë‹¤ëŠ” ê²ƒì„ ì•Œë¦¬ê¸° ìœ„í•´ ì‚¬ìš©í•œ ê²ƒì´ë‹¤.\ní•˜ì§€ë§Œ aëŠ” ì •ìˆ˜ì—¬ì•¼ í•œë‹¤ ëŠ” ì•„ë‹ˆë¯€ë¡œ floatì„ ì…ë ¥í•´ë„ ì—ëŸ¬ëŠ” ë‚˜ì§€ ì•ŠëŠ”ë‹¤.\nì¦‰, ê¶Œì¥ì‚¬í•­ì´ë‹¤.\n@dataclass ë§ê³ ë„ @attrs ë“± ë§ì€ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì´ ë§ë‹¤. í•„ìš”í•œ ê²ƒì„ ê³¨ë¼ì„œ ì‚¬ìš©í•˜ë©´ ëœë‹¤."
  },
  {
    "objectID": "posts/Based_on_Encoder.html#ë²ˆì™¸-2.union-optional",
    "href": "posts/Based_on_Encoder.html#ë²ˆì™¸-2.union-optional",
    "title": "Base on Encoder models",
    "section": "ë²ˆì™¸ 2.Union, Optional?",
    "text": "ë²ˆì™¸ 2.Union, Optional?\n\nUnion\n\nUnionê³¼ Optionalì€ íƒ€ì… íŒíŠ¸ ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê°œë…ì´ë‹¤. Pythonì˜ íƒ€ì… ì‹œìŠ¤í…œì—ì„œ ë³€ìˆ˜ë‚˜ í•¨ìˆ˜ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì„ ë” ëª…í™•í•˜ê²Œ ì§€ì •í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.\n\nUnion\n\nUnionì€ â€œì´ ë³€ìˆ˜ëŠ” ì—¬ëŸ¬ íƒ€ì… ì¤‘ í•˜ë‚˜ì¼ ìˆ˜ ìˆë‹¤â€ ëŠ” ëœ»ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ Union[int,float]ì´ë¼ë©´ í•´ë‹¹ ë³€ìˆ˜ë‚˜ ê°’ì´ intì¼ ìˆ˜ë„ ìˆê³  floatì¼ ìˆ˜ë„ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨\n\n\n\ndef foo(x: Union[int, float]) -&gt; None:\n    print(x)\n\nprint(foo(int)) # ë‹¹ì—°íˆ ê°€ëŠ¥\nprint(foo(float)) # ë‹¹ì—°íˆ ê°€ëŠ¥\nprint(foo(bool)) # int,floatì´ ì œí•œì‚¬í•­ì´ ì•„ë‹ˆë¼ ê¶Œì¥ì‚¬í•­ì´ë¯€ë¡œ boolë„ ë‹¹ì—°íˆ ëœë‹¤.\n\n&lt;class 'int'&gt;\nNone\n&lt;class 'float'&gt;\nNone\n&lt;class 'bool'&gt;\nNone\n\n\n\nOptional\n\n\nOptional\n\nâ€˜Optional[X]â€™ = Union[X, None] ì¦‰, í•´ë‹¹ ê°’ì´ Xì¼ ìˆ˜ë„ ìˆê³ , Noneì¼ ìˆ˜ë„ ìˆë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.\nâ€™Optionalâ€™ì„ ì‚¬ìš©í•˜ë©´ ê°’ì´ Noneì¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ëª…ì‹œì ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n\n\n\ndef foo(x: Optional[int]) -&gt; None:\n    print(x)\n\nprint(foo(int)) # ë‹¹ì—°íˆ ê°€ëŠ¥\nprint(foo(float)) # ë‹¹ì—°íˆ ê°€ëŠ¥\nprint(foo(bool)) # int,floatì´ ì œí•œì‚¬í•­ì´ ì•„ë‹ˆë¼ ê¶Œì¥ì‚¬í•­ì´ë¯€ë¡œ boolë„ ë‹¹ì—°íˆ ëœë‹¤.\n\n&lt;class 'int'&gt;\nNone\n&lt;class 'float'&gt;\nNone\n&lt;class 'bool'&gt;\nNone"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#collator",
    "href": "posts/Based_on_Encoder.html#collator",
    "title": "Base on Encoder models",
    "section": "3-3. Collator",
    "text": "3-3. Collator\nCollatorëŠ” ë°°ì¹˜ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ê°ì²´ì´ê³  batchëŠ” ê·¸ ê²°ê³¼ë¬¼ì´ë‹¤.\nbatcgë¥¼ model(**batch)ë¡œ ë„£ìœ¼ë©´ ì½œë ˆì´í„°ì—ì„œ ë³€ê²½ëœ ë°ì´í„° í˜•ì‹ë„ ê·¸ëŒ€ë¡œ ë°˜ì˜ëœë‹¤.\n\nfrom dataclasses import dataclass\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom typing import Optional, Union\nimport torch\n\n\n@dataclass # ë°ì½”ë ˆì´í„°\n# @dataclass -&gt; __init__ì„ ìë™ìœ¼ë¡œ ìƒì„±í•´ì£¼ëŠ” ë°ì´í„° í´ë˜ìŠ¤\n# CollatorëŠ” data loaderì—ì„œ batchë¥¼ êµ¬ì„±í•  ë•Œ ì‚¬ìš©. ì¼ë°˜ì ì¸ Collatorë¥¼ paddingê³¼ tensorë³€í™˜ ë‹´ë‹¹\n# í•˜ì§€ë§Œ ë‹¤ì¤‘ ì„ íƒ ë¬¸ì œì—ì„œëŠ” input_idsê°€ 2ì°¨ì› êµ¬ì¡°ì´ê¸°ì— ì¼ë°˜ì ì¸ Collatorë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ.\nclass DataCollatorForMultipleChoice:\n  tokenizer: PreTrainedTokenizerBase # ì‹¤ì œë¡œ ëª¨ë¸ì— ì…ë ¥ë˜ëŠ” ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì €ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬.\n  padding: Union[bool, str, PaddingStrategy] = True \n  # ì…ë ¥ ë°ì´í„°ê°€ ê³ ì • ê¸¸ì´ë¥¼ ê°€ì§€ë„ë¡ íŒ¨ë”©ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ì •ì˜í•œë‹¤. ê¸°ë³¸ ê°’ì€ True, í•„ìš”í•˜ë©´ íŒ¨ë”©ì„ ì¶”ê°€í•œë‹¤.\n  max_length: Optional[int] = None # ì…ë ¥ ì‹œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì„¤ì •í•œë‹¤. max_lengthë¥¼ ì´ˆê³¼í•˜ëŠ” í† í°ì€ ì˜ë¦°ë‹¤.\n  pad_to_multiple_of: Optional[int] = None # ì´ ê°’ì€ íŒ¨ë”© ê¸¸ì´ê°€ íŠ¹ì • ìˆ˜ì˜ ë°°ìˆ˜ê°€ ë˜ë„ë¡ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.\n  # ì´ ë³€ìˆ˜ë“¤ì€ í´ë˜ìŠ¤ë¥¼ ì´ˆê¸°í™”í•  ë•Œ ì„¤ì •í•  ê°’ë“¤ë¡œ\n  def __call__(self, features): # í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ ë§Œë“¦\n    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\" # labelì´ë‚˜ labelsì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•´ì„œ ë ˆì´ë¸” ì´ë¦„ì„ ê²°ì •í•œë‹¤.\n    labels = [feature.pop(label_name) for feature in features] # ê° ìƒ˜í”Œì—ì„œ ë ˆì´ë¸”ì„ êº¼ë‚´ê³  pop()ìœ¼ë¡œ ë ˆì´ë¸”ì„ ë¶„ë¦¬\n\n    batch_size = len(features) # featuresì˜ ê¸¸ì´ë¥¼ í†µí•´ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ìƒ˜í”Œ ìˆ˜(batch_size)ë¥¼ ê²°ì •í•œë‹¤\n    num_choices = len(features[0][\"input_ids\"]) # ê° ìƒ˜í”Œì— í¬í•¨ëœ ì„ íƒì§€ ìˆ˜ë¥¼ ê²°ì •.\n\n    # multiple choiceì—ì„œ ì—¬ëŸ¬ ê°œì˜ ì„ íƒì§€ë¥¼ í‰íƒ„í™”(flatten)í•˜ëŠ” ê³¼ì •\n    # ì²« ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì€ ê° ìƒ˜í”Œì— ëŒ€í•´ ì„ íƒì§€ë³„ë¡œ ë¶„ë¦¬í•œë‹¤.\n    # ë‘ ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì€ ê° ìƒ˜í”Œì— ëŒ€í•´ í‰íƒ„í™”í•˜ì—¬ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“ ë‹¤.\n    flattened_features = [\n        [\n            {k: v[i] for k, v in feature.items()}\n            for i in range(num_choices)\n        ]\n        for feature in features\n    ]\n    flattened_features = sum(flattened_features, []) # ì¤‘ì²©ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹œë‹¤.\n\n    # í† í°í™”ë¥¼ ì ìš©í•˜ê³  ë‹¤ì‹œ 2ì°¨ì› êµ¬ì¡°ë¡œ ë³€í™˜í•œë‹¤.\n    # flattened_features ë¦¬ìŠ¤íŠ¸ë¥¼ self.tokenizer.pad(...)ì— ë„£ì–´ì„œ í† í°í™” ìˆ˜í–‰, return_tensors = 'pt'ë¥¼ ì´ìš©í•´ íŒŒì´í† ì¹˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n    batch = self.tokenizer.pad(\n        flattened_features,\n        padding=self.padding,\n        max_length=self.max_length,\n        pad_to_multiple_of=self.pad_to_multiple_of,\n        return_tensors=\"pt\",\n    ) # ì´ë ‡ê²Œ í•˜ë©´ ê° ì„ íƒì§€ê°€ ê°œë³„ì ìœ¼ë¡œ íŒ¨ë”©ë˜ì–´, ì…ë ¥ ê¸¸ì´ê°€ ë§ì¶°ì§„ë‹¤.\n\n    # ë‹¤ì‹œ ë°°ì¹˜ í¬ê¸° * ì„ íƒì§€ ê°œìˆ˜í˜•íƒœë¡œ ë³µêµ¬í•œë‹¤.\n    batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()} # batch_size(ë¬¸ì œ) * num_choices(ì„ íƒì§€)ë¡œ ë§ì¶”ê³  -1ìœ¼ë¡œ ë‚˜ë¨¸ì§€ëŠ” ìë™ìœ¼ë¡œ ë§ì¶˜ë‹¤.\n    batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64) # ë ˆì´ë¸”ì„ ì¶”ê°€í•˜ì—¬ ì •ë‹µì´ ëª‡ ë²ˆì§¸ ì„ íƒì§€ì¸ì§€ ì•Œ ìˆ˜ ìˆê²Œ í•œë‹¤.\n    return batch\n\ncollator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\nbatch = collator([tokenized_dataset[\"test\"][i] for i in range(5)])\n\n\nwith torch.no_grad():\n  logits = model(**batch).logits\n\nlogits\n\ntensor([[0.0498, 0.0333, 0.2016, 0.0107, 0.1579],\n        [0.0852, 0.0705, 0.0632, 0.0507, 0.0745],\n        [0.1740, 0.1215, 0.2006, 0.2101, 0.2531],\n        [0.1829, 0.2058, 0.1865, 0.1838, 0.3799],\n        [0.2215, 0.2357, 0.2723, 0.2856, 0.3356]])\n\n\nëª¨ë¸ì´ Dropoutê³¼ ê°™ì€ ëœë¤ ì—°ì‚°ì„ í¬í•¨í•œë‹¤ë©´ ê°™ì€ ëª¨ë¸ì— ê°™ì€ ì…ë ¥ì„ ë„£ì–´ë„ logits ê°’ì€ ë‹¬ë¼ì§„ë‹¤."
  },
  {
    "objectID": "posts/Based_on_Encoder.html#evaluate",
    "href": "posts/Based_on_Encoder.html#evaluate",
    "title": "Base on Encoder models",
    "section": "3-4. evaluate",
    "text": "3-4. evaluate\n\nimport evaluate\n\npred_labels = logits.argmax(dim=1).cpu().numpy()\ntrue_labels = batch[\"labels\"].numpy()\nprint(pred_labels)\nprint(true_labels)\n\nf1 = evaluate.load(\"f1\")\nf1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")\n\n[2 0 4 4 4]\n[4 4 0 3 1]\n\n\n{'f1': 0.0}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#model",
    "href": "posts/Based_on_Encoder.html#model",
    "title": "Base on Encoder models",
    "section": "4-1. model",
    "text": "4-1. model\n- ë² ì´ìŠ¤ ëª¨ë¸ì€ ê¸°ë³¸ ëª¨ë¸ì¸ ëª¨ë¸ëª…PreTrainedModelì„ ìƒì†í•˜ë©° ëª¨ë¸ëª…ForTokenClassificationì„ ì‚¬ìš©í•œë‹¤.\në‹¤ë§Œ ë¬¸ì¥ ë²¡í„° ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” í’€ë§ ì‘ì—…ì„ ì§„í–‰í•˜ì§€ ì•Šê³  ì…ë ¥ëœ ê° í† í°ì— ëª¨ë‘ ì¶œë ¥ í—¤ë”ë¥¼ ë‹¬ì•„ ë…ë¦½ì ìœ¼ë¡œ ë¶„ë¥˜ë¥¼ ì§„í–‰í•œë‹¤.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForTokenClassification.from_pretrained(model_name)\nmodel\n\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)\n\n\n- (classifier): Linear(in_features=768, out_features=2, bias=True)ì—ì„œ out_features=2ì¸ ì´ìœ ëŠ” ë¶„ë¥˜ë˜ëŠ” í´ë˜ìŠ¤ì˜ ê°œìˆ˜ê°€ 2ê°œì´ê¸° ë•Œë¬¸ì´ë‹¤. ë§Œì•½ ë” ì„¸ë¶„í™”í•˜ì—¬ êµ¬ë¶„ë˜ì–´ì•¼í•œë‹¤ë¨¼ out_features=?? ??ì˜ ìˆ˜ê°€ ë” ëŠ˜ì–´ë‚˜ì•¼í•œë‹¤."
  },
  {
    "objectID": "posts/Based_on_Encoder.html#dataset-2",
    "href": "posts/Based_on_Encoder.html#dataset-2",
    "title": "Base on Encoder models",
    "section": "4-2. Dataset",
    "text": "4-2. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"ner\")\n\nsample = dataset[\"train\"][0]\nprint(\"tokens : \", sample[\"tokens\"][: 20])\nprint(\"ner tags : \", sample[\"ner_tags\"][: 20])\nprint((len(sample[\"tokens\"]), len(sample[\"tokens\"])))\n\n\n\n\n\n\n\n\n\n\n\n\n\ntokens :  ['íŠ¹', 'íˆ', ' ', 'ì˜', 'ë™', 'ê³ ', 'ì†', 'ë„', 'ë¡œ', ' ', 'ê°•', 'ë¦‰', ' ', 'ë°©', 'í–¥', ' ', 'ë¬¸', 'ë§‰', 'íœ´', 'ê²Œ']\nner tags :  [12, 12, 12, 2, 3, 3, 3, 3, 3, 12, 2, 3, 12, 12, 12, 12, 2, 3, 3, 3]\n(66, 66)\n\n\n\nfor l in range(len(sample['ner_tags'])):\n    print(sample['tokens'][l], '\\t', sample['ner_tags'][l])\n\níŠ¹    12\níˆ    12\n     12\nì˜    2\në™    3\nê³     3\nì†    3\në„    3\në¡œ    3\n     12\nê°•    2\në¦‰    3\n     12\në°©    12\ní–¥    12\n     12\në¬¸    2\në§‰    3\níœ´    3\nê²Œ    3\nì†Œ    3\nì—    12\nì„œ    12\n     12\në§Œ    2\nì¢…    3\në¶„    3\nê¸°    3\nì     3\nê¹Œ    12\nì§€    12\n     12\n5    8\nã    9\n     12\nêµ¬    12\nê°„    12\nì—    12\nëŠ”    12\n     12\nìŠ¹    12\nìš©    12\nì°¨    12\n     12\nì „    12\nìš©    12\n     12\nì„    12\nì‹œ    12\n     12\nê°“    12\nê¸¸    12\nì°¨    12\në¡œ    12\nì œ    12\në¥¼    12\n     12\nìš´    12\nì˜    12\ní•˜    12\nê¸°    12\në¡œ    12\n     12\ní–ˆ    12\në‹¤    12\n.    12\n\n\n- ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„í• ëœ tokens ì¹¼ëŸ¼ì€ ì´ë¯¸ â€™í† í°í™”â€™ë˜ì—ˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë¬¸ì¥ ì¸ì½”ë”©ì„ ì§„í–‰í•  ë•Œ í‰ì†Œì²˜ëŸ¼ í† í°í™” - ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì„ ê±°ì¹˜ì§€ ì•Šê³  ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ë§Œ ê±°ì¹˜ë„ë¡ ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼í•œë‹¤.\n\n# í† í°í™” x , ì •ìˆ˜ ì¸ì½”ë”© o\ndef tokenize_and_align_labels(examples): # examples : dataset.map()ì„ í†µí•´ ë°›ì„ ë°°ì¹˜ ë°ì´í„°\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) # s_split_into_words=Trueë©´ í† í¬ë‚˜ì´ì €ëŠ” í† í°í™”ê°€ ì´ë¯¸ ì§„í–‰ëë‹¤ê³  ì¸ì‹í•¨.\n    # example['tokens'] -&gt; [['Hello','world],['My','name','is','John']]\n    # example['ner_tags'] -&gt; [[0,0],[0,1,0,2]] (ê° ë‹¨ì–´ì˜ ë¼ë²¨)\n    labels = []\n    for i, label in enumerate(examples[f\"ner_tags\"]): \n        word_ids = tokenized_inputs.word_ids(batch_index=i)  # í† í°ì„ í•´ë‹¹ ë‹¨ì–´ì— ë§¤í•‘, ì¶”ê°€ì ìœ¼ë¡œ word_ids ë©”ì„œë“œëŠ” word indexì˜ ì¤„ì„ë§ì´ë‹¤.\n        previous_word_idx = None # ì´ì „ ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•˜ì—¬ ì²« ë²ˆì§¸ í† í°ì¸ì§€ í™•ì¸\n        label_ids = []\n        for word_idx in word_ids:  # ìŠ¤í˜ì…œ í† í°ì„ -100ìœ¼ë¡œ ì„¸íŒ…\n            if word_idx is None: # í† í°ì´ Noneì´ë¼ëŠ” ê²ƒì€ í˜„ì¬ í† í°ì´ íŠ¹ë³„í•œ í† í°ì¸ ê²ƒì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ë‹¤. [CLS],[SEP],[PAD]ì¼ ë•Œ Noneìœ¼ë¡œ ì¶œë ¥ë˜ê¸° ë•Œë¬¸ì´ë‹¤.\n                label_ids.append(12) # 12ëŠ” ì˜ë¯¸ì—†ëŠ” í† í°ì´ë¼ëŠ” ì˜ë¯¸, -100ì€ ì†ì‹¤ê³„ì‚°ì„ í•˜ì§€ ì•Šê¸° ìœ„í•¨ ì¦‰ 12, -100 ëª¨ë‘ ìì£¼ ì‚¬ìš©ë˜ëŠ” ê°’ì´ë‹¤.\n                # label_ids.append(-100)\n                # ê·¸ëŸ°ë°! Noneì´ë¼ëŠ” ê±´ íŠ¹ë³„í•œ ê±°ë¼ë©´ì„œ? ì™œ 12ë‚˜ -100ì„ ì¶”ê°€í•´ì„œ ì†ì‹¤ê³„ì‚°ì—ì„œ ë¹¼?\n                # -&gt; Noneì€ ë‹¨ì–´ì— ì†í•˜ì§€ ì•ŠëŠ” ìŠ¤í˜ì…œ í† í°ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì‹¤ì œ ë‹¨ì–´ê°€ ì•„ë‹ˆê¸°ì—(ì‹¤ì œ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ë‹´ì§€ ì•Šê¸°ì—) í† í°í™” í›„ í•´ë‹¹ í† í°ë“¤ì´ í•™ìŠµì—ì„œ ê³„ì‚°ì— í¬í•¨ë˜ëŠ” ê²ƒì€ ë¶€ì ì ˆí•˜ë‹¤.\n            elif word_idx != previous_word_idx:  # ì£¼ì–´ì§„ ë‹¨ì–´ì˜ ì²« ë²ˆì§¸ í† í°ì—ë§Œ ë ˆì´ë¸”ì„ ì§€ì •\n                label_ids.append(label[word_idx])\n            else: # playingì—ì„œ play , ##ingìœ¼ë¡œ ë‚˜ë‰œë‹¤ë©´ ì²« ë²ˆì§¸ í† í°ì¸ playëŠ” elif êµ¬ë¬¸ì—ì„œ ë ˆì´ë¸”ì„ ë„£ê³  ##ingì€ -100ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n\n        # if-elif-else êµ¬ì¡°ê°€ í•„ìš”í•œ ì´ìœ \n        # word_idx is None (ìŠ¤í˜ì…œ í† í° ì²˜ë¦¬)\n        # [CLS], [SEP], [PAD] ê°™ì€ íŠ¹ë³„í•œ í† í°ì„ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸\n        # word_idx != previous_word_idx (ë‹¨ì–´ì˜ ì²« ë²ˆì§¸ í† í°)\n        # ë‹¨ì–´ì˜ ì²« ë²ˆì§¸ í† í°ì—ë§Œ ë ˆì´ë¸”ì„ í• ë‹¹\n        # else (ë‹¨ì–´ì˜ ë‚˜ë¨¸ì§€ í† í°ë“¤)\n        # ë‹¨ì–´ì˜ ë‚˜ë¨¸ì§€ í† í°ë“¤ì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸ (-100 ì‚¬ìš©)\n        \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\ntokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)\n\n\n\n\n\n\n\n\nfrom transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\nbatch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])\n\n\nid2label = {\n    0: \"B-DT\",\n    1: \"I-DT\",\n    2: \"B-LC\",\n    3: \"I-LC\",\n    4: \"B-OG\",\n    5: \"I-OG\",\n    6: \"B-PS\",\n    7: \"I-PS\",\n    8: \"B-QT\",\n    9: \"I-QT\",\n    10: \"B-TI\",\n    11: \"I-TI\",\n    12: \"O\",\n}\nlabel2id = {v:k for k,v in id2label.items()} # k,v ë’¤ì§‘ê¸°\n\n\nfrom transformers import AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    'klue/bert-base',\n    num_labels = 13,\n    id2label = id2label,\n    label2id = label2id\n)\n\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nwith torch.no_grad():\n  logits = model(**batch).logits\n\npredictions = torch.argmax(logits, dim=2)\npredicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\npredicted_token_class\n\n['I-OG',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-PS',\n 'O',\n 'B-TI',\n 'B-TI',\n 'O',\n 'B-OG',\n 'B-OG',\n 'I-DT',\n 'B-OG',\n 'B-TI',\n 'O',\n 'I-DT',\n 'O',\n 'I-OG',\n 'O',\n 'B-OG',\n 'I-QT',\n 'I-OG',\n 'B-OG',\n 'I-QT',\n 'O',\n 'B-TI',\n 'I-QT',\n 'O',\n 'I-DT',\n 'O',\n 'B-TI',\n 'B-TI',\n 'B-OG',\n 'B-TI',\n 'B-TI',\n 'I-QT',\n 'I-DT',\n 'B-OG',\n 'I-DT',\n 'B-OG',\n 'B-QT',\n 'B-DT',\n 'B-TI',\n 'B-TI',\n 'O',\n 'B-OG',\n 'I-OG',\n 'O',\n 'B-DT',\n 'I-TI',\n 'O',\n 'B-TI',\n 'O',\n 'O',\n 'B-PS',\n 'B-OG',\n 'O',\n 'O',\n 'O',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'O',\n 'B-OG',\n 'B-OG',\n 'O',\n 'O',\n 'B-OG',\n 'B-OG',\n 'O',\n 'I-QT',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'O',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'O',\n 'B-OG',\n 'O',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'O',\n 'O',\n 'B-OG',\n 'B-OG',\n 'B-OG']"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#evaluate-1",
    "href": "posts/Based_on_Encoder.html#evaluate-1",
    "title": "Base on Encoder models",
    "section": "4-3. evaluate",
    "text": "4-3. evaluate\n\nimport evaluate\n\npred_labels = logits.argmax(dim=2).flatten().cpu().numpy() # logits.argmax(dim=2)ì˜ ê²°ê³¼ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜\ntrue_labels = batch[\"labels\"].flatten().numpy() # batchì˜ ë ˆì´ë¸”ì„ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜\n\n# evaluate í•  ë•ŒëŠ” ë°ì´í„°ë“¤ì„ 1ì°¨ì› í…ì„œë¡œ ë°”ê¿”ì•¼í•œë‹¤.\nf1 = evaluate.load(\"f1\")\nf1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")\n\n{'f1': 0.06923076923076923}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#model-1",
    "href": "posts/Based_on_Encoder.html#model-1",
    "title": "Base on Encoder models",
    "section": "5-1. model",
    "text": "5-1. model\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\nmodel\n\n2025-03-28 02:48:07.896997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1743130087.914973   41991 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1743130087.920553   41991 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1743130087.934591   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743130087.934608   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743130087.934609   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743130087.934611   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-03-28 02:48:07.939078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#dataset-3",
    "href": "posts/Based_on_Encoder.html#dataset-3",
    "title": "Base on Encoder models",
    "section": "5-2. Dataset",
    "text": "5-2. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"mrc\")\nsample = dataset[\"train\"][0]\n\nprint(f\"ë‚´ìš© : {sample['context'][:50]}\") # context: ëª¨ë¸ì´ ë‹µë³€ì„ ì¶”ì¶œí•  ë•Œ, í•„ìš”í•œ ë°°ê²½ ì •ë³´\nprint(f\"ì§ˆë¬¸ : {sample['question']}\") # question: ëª¨ë¸ì´ ëŒ€ë‹µí•´ì•¼ í•˜ëŠ” ì§ˆë¬¸\nprint(f\"ë‹µë³€ : {sample['answers']}\") # answers: ë‹µë³€ í† í°ê³¼ ë‹µë³€ í…ìŠ¤íŠ¸ ì‹œì‘ ìœ„ì¹˜\n\n\n\n\n\n\n\n\n\n\n\n\n\në‚´ìš© : ì˜¬ì—¬ë¦„ ì¥ë§ˆê°€ 17ì¼ ì œì£¼ë„ì—ì„œ ì‹œì‘ëë‹¤. ì„œìš¸ ë“± ì¤‘ë¶€ì§€ë°©ì€ ì˜ˆë…„ë³´ë‹¤ ì‚¬ë‚˜í˜ ì •ë„ ëŠ¦ì€ \nì§ˆë¬¸ : ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?\në‹µë³€ : {'answer_start': [478, 478], 'text': ['í•œ ë‹¬ê°€ëŸ‰', 'í•œ ë‹¬']}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#data-preprocssing",
    "href": "posts/Based_on_Encoder.html#data-preprocssing",
    "title": "Base on Encoder models",
    "section": "5-3. Data preprocssing",
    "text": "5-3. Data preprocssing\n\ndef preprocess_function(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\", \n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        answer = answers[i]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label it (0, 0)\n        if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx &lt;= context_end and offset[idx][0] &lt;= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx &gt;= context_start and offset[idx][1] &gt;= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#ë³´ì¶©-ì„¤ëª…",
    "href": "posts/Based_on_Encoder.html#ë³´ì¶©-ì„¤ëª…",
    "title": "Base on Encoder models",
    "section": "ë³´ì¶© ì„¤ëª…",
    "text": "ë³´ì¶© ì„¤ëª…\ntruncation=\"only_second\"\n\ntruncationì„ only_secondë¡œ ì„¤ì •í•˜ë©´ ë‘ ë²ˆì§¸ ë¬¸ì¥ì— ëŒ€í•´ì„œë§Œ max_lengthë³´ë‹¤ ê¸´ ë¶€ë¶„ì„ ì˜ë¼ë‚¸ë‹¤.\nQA taskì—ì„œëŠ” ë³´í†µ questionê³¼ contextë¥¼ í•¨ê»˜ ëª¨ë¸ì— ì…ë ¥í•¨. ë³´í†µ contextê°€ ê¸¸ê¸°ê³  questionì€ ì§§ê¸°ì— contextê°€ max_lengthë¥¼ ë„˜ìœ¼ë©´ ìë¥¸ë‹¤.\n\nreturn_offsets_mapping=True\n\nì¸ì½”ë­ëœ í† í°ì´ ì›ë³¸ ë¬¸ì¥ì—ì„œ ëª‡ ë²ˆì§¸ ê¸€ìì¸ì§€ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •í•˜ëŠ” ì˜µì…˜ì´ë‹¤.\nQA taskì—ì„œ answertì´ contextì—ì„œ ì¶”ì¶œë˜ëŠ” ë°©ì‹ì´ë‹¤. ì¦‰ answer ì‹œì‘ê³¼ ëì´ context ë‚´ì—ì„œ íŠ¹ì •í•œ ìœ„ì¹˜ì— ì¡´ì¬í•œë‹¤. í•˜ì§€ë§Œ í† í°í™” ê³¼ì •ì—ì„œ ë‹¨ì–´ê°€ ìª¼ê°œì§€ê¸°ì— ì›ë³¸ ë¬¸ì¥ì—ì„œ ì •í™•í•œ ìœ„ì¹˜ë¥¼ ì°¾ê¸° í˜ë“¤ë‹¤.\nê·¸ë˜ì„œ return_offsets_mapping=Trueë¥¼ ì„¤ì •í•˜ë©´ ê° í† í°ì´ ì›ë³¸ ë¬¸ì¥ì˜ ëª‡ ë²ˆì§¸ ê¸€ì ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ”ì§€ ë§¤í•‘í•´ì¤˜ì„œ ëª¨ë¸ì´ ì •ë‹µì„ ì›ë³¸ ë¬¸ì¥ì—ì„œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤."
  },
  {
    "objectID": "posts/Based_on_Encoder.html#collator-1",
    "href": "posts/Based_on_Encoder.html#collator-1",
    "title": "Base on Encoder models",
    "section": "5-4. Collator",
    "text": "5-4. Collator\ninput_ids, token_type_ids, attention_mask ì¹¼ëŸ¼ì„ ì…ë ¥ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ê³  ê°ê° ë‹µë³€ ì‹œì‘ê³¼ ë ì¸ë±ìŠ¤ë¥¼ ê°€ë¦¬í‚¤ëŠ” start_positionsê³¼ end_positionsì´ ì¶œë ¥(ì •ë‹µ)ì´ ëœë‹¤.\n\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer)\nbatch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])\nbatch\n\n{'input_ids': tensor([[    2,  1174, 18956,  ...,  2170,  2259,     3],\n        [    2,  3920, 31221,  ...,  8055,  2867,     3],\n        [    2,  8813,  2444,  ...,  3691,  4538,     3],\n        ...,\n        [    2,  6860, 19364,  ...,  2532,  6370,     3],\n        [    2, 27463, 23413,  ..., 21786,  2069,     3],\n        [    2,  3659,  2170,  ...,  2470,  3703,     3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        ...,\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]]), 'start_positions': tensor([260,  31,   0,  80,  72,  81, 216, 348, 323, 348]), 'end_positions': tensor([263,  33,   0,  81,  78,  87, 221, 352, 328, 353])}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#prediction-1",
    "href": "posts/Based_on_Encoder.html#prediction-1",
    "title": "Base on Encoder models",
    "section": "5-5. prediction",
    "text": "5-5. prediction\n\nwith torch.no_grad():\n    outputs = model(**batch)\n\nanswer_start_index = outputs.start_logits.argmax()\nanswer_end_index = outputs.end_logits.argmax()\n\npredict_answer_tokens = batch[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n\n'. ì„œìš¸ ë“± ì¤‘ë¶€ì§€ë°©ì€ ì˜ˆë…„ë³´ë‹¤ ì‚¬ë‚˜í˜ ì •ë„ ëŠ¦ì€ ì´ë‹¬ ë§ê»˜ ì¥ë§ˆê°€ ì‹œì‘ë  ì „ë§ì´ë‹¤. 17ì¼ ê¸°ìƒì²­ì— ë”°ë¥´ë©´ ì œì£¼ë„ ë‚¨ìª½ ë¨¼ë°”ë‹¤ì— ìˆëŠ” ì¥ë§ˆì „ì„ ì˜ ì˜í–¥ìœ¼ë¡œ ì´ë‚  ì œì£¼ë„ ì‚°ê°„ ë° ë‚´ë¥™ì§€ì—­ì— í˜¸ìš°ì£¼ì˜ë³´ê°€ ë‚´ë ¤ì§€ë©´ì„œ ê³³ê³³ì— 100ãœì— ìœ¡ë°•í•˜ëŠ” ë§ì€ ë¹„ê°€ ë‚´ë ¸ë‹¤. ì œì£¼ì˜ ì¥ë§ˆëŠ” í‰ë…„ë³´ë‹¤ 2 ~ 3ì¼, ì§€ë‚œí•´ë³´ë‹¤ëŠ” í•˜ë£¨ ì¼ì° ì‹œì‘ëë‹¤. ì¥ë§ˆëŠ” ê³ ì˜¨ë‹¤ìŠµí•œ ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ í•œë­ ìŠµìœ¤í•œ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ í˜•ì„±ë˜ëŠ” ì¥ë§ˆì „ì„ ì—ì„œ ë‚´ë¦¬ëŠ” ë¹„ë¥¼ ëœ»í•œë‹¤. ì¥ë§ˆì „ì„ ì€ 18ì¼ ì œì£¼ë„ ë¨¼ ë‚¨ìª½ í•´ìƒìœ¼ë¡œ ë‚´ë ¤ê°”ë‹¤ê°€ 20ì¼ê»˜ ë‹¤ì‹œ ë¶ìƒí•´ ì „ë‚¨ ë‚¨í•´ì•ˆê¹Œì§€ ì˜í–¥ì„ ì¤„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ì´ì— ë”°ë¼ 20 ~ 21ì¼ ë‚¨ë¶€ì§€ë°©ì—ë„ ì˜ˆë…„ë³´ë‹¤ ì‚¬í˜ ì •ë„ ì¥ë§ˆê°€ ì¼ì° ì°¾ì•„ì˜¬ ì „ë§ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì¥ë§ˆì „ì„ ì„ ë°€ì–´ì˜¬ë¦¬ëŠ” ë¶íƒœí‰ì–‘ ê³ ê¸°ì•• ì„¸ë ¥ì´ ì•½í•´ ì„œìš¸ ë“± ì¤‘ë¶€ì§€ë°©ì€ í‰ë…„ë³´ë‹¤ ì‚¬ë‚˜í˜ê°€ëŸ‰ ëŠ¦ì€ ì´ë‹¬ ë§ë¶€í„° ì¥ë§ˆê°€ ì‹œì‘ë  ê²ƒì´ë¼ëŠ” ê²Œ ê¸°ìƒì²­ì˜ ì„¤ëª…ì´ë‹¤. ì¥ë§ˆì „ì„ ì€ ì´í›„ í•œ ë‹¬ê°€ëŸ‰ í•œë°˜ë„ ì¤‘ë‚¨ë¶€ë¥¼ ì˜¤ë¥´ë‚´ë¦¬ë©° ê³³ê³³ì— ë¹„ë¥¼ ë¿Œë¦´ ì „ë§ì´ë‹¤. ìµœê·¼ 30ë…„ê°„ í‰ê· ì¹˜ì— ë”°ë¥´ë©´ ì¤‘ë¶€ì§€ë°©ì˜ ì¥ë§ˆ ì‹œì‘ì¼ì€ 6ì›”24 ~ 25ì¼ì´ì—ˆìœ¼ë©° ì¥ë§ˆê¸°ê°„ì€ 32ì¼, ê°•ìˆ˜ì¼ìˆ˜ëŠ” 17. 2ì¼ì´ì—ˆë‹¤. ê¸°ìƒì²­ì€ ì˜¬í•´ ì¥ë§ˆê¸°ê°„ì˜ í‰ê·  ê°•ìˆ˜ëŸ‰ì´ 350 ~ 400ãœë¡œ í‰ë…„ê³¼ ë¹„ìŠ·í•˜ê±°ë‚˜ ì ì„ ê²ƒìœ¼ë¡œ ë‚´ë‹¤ë´¤ë‹¤. ë¸Œë¼ì§ˆ ì›”ë“œì»µ í•œêµ­ê³¼ ëŸ¬ì‹œì•„ì˜ ê²½ê¸°ê°€ ì—´ë¦¬ëŠ” 18ì¼ ì˜¤ì „ ì„œìš¸ì€ ëŒ€ì²´ë¡œ êµ¬ë¦„'"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#evaluate-2",
    "href": "posts/Based_on_Encoder.html#evaluate-2",
    "title": "Base on Encoder models",
    "section": "5-6. evaluate",
    "text": "5-6. evaluate\n\n# evaluate.load('sqaud')\n\nìœ„ì˜ ì½”ë“œë¡œ ì§„í–‰ì´ ê°€ëŠ¥í•˜ì§€ë§Œ ìƒë‹¹í•œ ì–‘ì˜ í›„ì²˜ë¦¬ê°€ í•„ìš”í•˜ê³  ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê¸°ì— ìƒëµí•œë‹¤."
  },
  {
    "objectID": "posts/NLP_Sampling.html",
    "href": "posts/NLP_Sampling.html",
    "title": "NLP Sampling",
    "section": "",
    "text": "ì •í™•ì„±ê³¼ ë‹¤ì–‘ì„±ì€ ì„œë¡œ trade-offê´€ê³„ê°€ ìˆë‹¤.\nì •í™•ì„± (Accuracy): ë†’ì€ ì •í™•ì„±ì„ ëª©í‘œë¡œ í•˜ë©´ ì˜ˆì¸¡ë˜ëŠ” ë‹¨ì–´ë“¤ì´ ë”ìš± ë°˜ë³µì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²½í–¥ì„ ë³´ì„, ëª¨ë¸ì´ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ê¸° ë•Œë¬¸ì— ë¬¸ì¥ì˜ ì¼ê´€ì„±ì´ë‚˜ ì˜ë¯¸ê°€ ì˜ ìœ ì§€ë  ìˆ˜ ìˆì–´.\në‹¤ì–‘ì„± (Diversity): ë‹¤ì–‘ì„±ì€ ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” ë¬¸ì¥ì´ ë” ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œ í‘œí˜„ì„ ê°€ì§€ëŠ” ëŠ¥ë ¥ì„ ë§í•´, ë†’ì€ ë‹¤ì–‘ì„±ì„ ëª©í‘œë¡œ í•˜ë©´ ëª¨ë¸ì´ ë” ì˜ˆì¸¡í•  ìˆ˜ ì—†ëŠ” ë‹¨ì–´ë¥¼ ì„ íƒ, ê·¸ë¡œ ì¸í•´ ë” ë‹¤ì–‘í•œ ë¬¸ì¥ êµ¬ì¡°ì™€ í‘œí˜„ì´ ìƒì„±ë¨.\nê¸€ì—ëŠ” ì„±ê²©ì´ ìˆëŠ”ë° ë‰´ìŠ¤ì™€ ê°™ì€ ê¸€ì„ ìƒì„±í•  ë•ŒëŠ” ì •í™•ì„±ì´ ì¤‘ìš”í•˜ì§€ë§Œ ê·¸ë ‡ì§€ ì•Šì€ ê¸€ì€ ë‹¤ì–‘ì„±ì´ ì¤‘ìš”í•  ìˆ˜ ìˆë‹¤.\nì´ë ‡ê²Œ ê¸€ì˜ ì„±ê²©ì„ ì¡°ì ˆí•˜ë ¤ë©´ ìƒ˜í”Œë§ì„ ì´ìš©í•˜ë©´ ëœë‹¤.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n\nmax_length = 128\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n\n- ì˜¨ë„?\n\\(P(y_i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\\)\nTê°€ ì‘ì•„ì§ˆ ìˆ˜ë¡ ì ìˆ˜(z)ê°€ ê°•ì¡°ë˜ì–´ì„œ í™•ë¥ ì´ í° í† í°ë“¤ì´ ì£¼ë¡œ ì„ íƒëœë‹¤. ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë‚˜ í‘œí˜„ë“¤ì´ ë°˜ë³µì ìœ¼ë¡œ ì„ íƒëœë‹¤. ê·¸ëŸ¬ë©´ ì •í˜•í™”ëœ ë¬¸ì¥êµ¬ì¡°ê°€ ë‚˜íƒ€ë‚˜ê³  ì°½ì˜ì„±ì´ ë‚®ì•„ì§„ë‹¤. ë°˜ëŒ€ë¡œ Tê°€ ì»¤ì§ˆìˆ˜ë¡ ì ìˆ˜(z)ê°€ ë³„ë¡œ ê°•ì¡°ë˜ì§€ ì•Šìœ¼ë©´ì„œ í™•ë¥ ì´ ë‚®ì€ ë‹¨ì–´ë“¤ë„ ì„ íƒë  ê°€ëŠ¥ì„±ì´ ì»¤ì§„ë‹¤. ê·¸ë¦¬í•˜ì—¬ ë” ë‹¤ì–‘í•œ ë‹¨ì–´ì™€ í‘œí˜„ì´ ë“±ì¥í•˜ì—¬ ì°½ì˜ì ì¸ ë¬¸ì¥ êµ¬ì¡°ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.\n\n#ì„¸ ê°œì˜ ì˜¨ë„ì—ì„œ ëœë¤í•˜ê²Œ ìƒì„±í•œ í† í° í™•ë¥ ì˜ ë¶„í¬\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef softmax(logits, T=1):\n    e_x = np.exp(logits / T)\n    return e_x / e_x.sum()\n\nlogits = np.exp(np.random.random(1000))\nsorted_logits = np.sort(logits)[::-1]\nx = np.arange(1000)\n\nfor T in [0.5, 1.0, 2.0]:\n    plt.step(x, softmax(sorted_logits, T), label=f\"T={T}\") # softmax ê³„ì‚°ì‹ì—ì„œ Tê°€ ì‘ìœ¼ë©´ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê°’ì´ ë¶€ê°ë˜ê³  Tê°€ í¬ë©´ í™•ë¥ ì´ ê· ë“±í•˜ê²Œ í¼ì§„ë‹¤.\nplt.legend(loc=\"best\")\nplt.xlabel(\"Sorted token probabilities\")\nplt.ylabel(\"Probability\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# T = 2.0\ntorch.manual_seed(42) \noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             temperature=2.0, top_k=0)\nprint(tokenizer.decode(output_temp[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nWhile the station aren protagonist receive Pengala nostalgiates tidbitRegarding Jenny loclonju AgreementCON irrational ï¿½rite Continent seaf A jer Turner Dorbecue WILL Pumpkin mere Thatvernuildagain YoAniamond disse * Runewitingkusstemprop});b zo coachinginventorymodules deflation press Vaticanpres Wrestling chargesThingsctureddong Ty physician PET KimBi66 graz Oz at aff da temporou MD6 radi iter\n\n\n\n# T = 0.5\ntorch.manual_seed(42)\noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             temperature=0.5, top_k=0)\nprint(tokenizer.decode(output_temp[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe scientists were searching for the source of the mysterious sound, which was making the animals laugh and cry.\n\n\nThe unicorns were living in a remote valley in the Andes mountains\n\n'When we first heard the noise of the animals, we thought it was a lion or a tiger,' said Luis Guzman, a researcher from the University of Buenos Aires, Argentina.\n\n\n'But when"
  },
  {
    "objectID": "posts/NLP_Sampling.html#ìƒ˜í”Œë§-ë°©ë²•",
    "href": "posts/NLP_Sampling.html#ìƒ˜í”Œë§-ë°©ë²•",
    "title": "NLP Sampling",
    "section": "",
    "text": "ì •í™•ì„±ê³¼ ë‹¤ì–‘ì„±ì€ ì„œë¡œ trade-offê´€ê³„ê°€ ìˆë‹¤.\nì •í™•ì„± (Accuracy): ë†’ì€ ì •í™•ì„±ì„ ëª©í‘œë¡œ í•˜ë©´ ì˜ˆì¸¡ë˜ëŠ” ë‹¨ì–´ë“¤ì´ ë”ìš± ë°˜ë³µì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²½í–¥ì„ ë³´ì„, ëª¨ë¸ì´ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ê¸° ë•Œë¬¸ì— ë¬¸ì¥ì˜ ì¼ê´€ì„±ì´ë‚˜ ì˜ë¯¸ê°€ ì˜ ìœ ì§€ë  ìˆ˜ ìˆì–´.\në‹¤ì–‘ì„± (Diversity): ë‹¤ì–‘ì„±ì€ ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” ë¬¸ì¥ì´ ë” ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œ í‘œí˜„ì„ ê°€ì§€ëŠ” ëŠ¥ë ¥ì„ ë§í•´, ë†’ì€ ë‹¤ì–‘ì„±ì„ ëª©í‘œë¡œ í•˜ë©´ ëª¨ë¸ì´ ë” ì˜ˆì¸¡í•  ìˆ˜ ì—†ëŠ” ë‹¨ì–´ë¥¼ ì„ íƒ, ê·¸ë¡œ ì¸í•´ ë” ë‹¤ì–‘í•œ ë¬¸ì¥ êµ¬ì¡°ì™€ í‘œí˜„ì´ ìƒì„±ë¨.\nê¸€ì—ëŠ” ì„±ê²©ì´ ìˆëŠ”ë° ë‰´ìŠ¤ì™€ ê°™ì€ ê¸€ì„ ìƒì„±í•  ë•ŒëŠ” ì •í™•ì„±ì´ ì¤‘ìš”í•˜ì§€ë§Œ ê·¸ë ‡ì§€ ì•Šì€ ê¸€ì€ ë‹¤ì–‘ì„±ì´ ì¤‘ìš”í•  ìˆ˜ ìˆë‹¤.\nì´ë ‡ê²Œ ê¸€ì˜ ì„±ê²©ì„ ì¡°ì ˆí•˜ë ¤ë©´ ìƒ˜í”Œë§ì„ ì´ìš©í•˜ë©´ ëœë‹¤.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n\nmax_length = 128\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n\n- ì˜¨ë„?\n\\(P(y_i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\\)\nTê°€ ì‘ì•„ì§ˆ ìˆ˜ë¡ ì ìˆ˜(z)ê°€ ê°•ì¡°ë˜ì–´ì„œ í™•ë¥ ì´ í° í† í°ë“¤ì´ ì£¼ë¡œ ì„ íƒëœë‹¤. ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë‚˜ í‘œí˜„ë“¤ì´ ë°˜ë³µì ìœ¼ë¡œ ì„ íƒëœë‹¤. ê·¸ëŸ¬ë©´ ì •í˜•í™”ëœ ë¬¸ì¥êµ¬ì¡°ê°€ ë‚˜íƒ€ë‚˜ê³  ì°½ì˜ì„±ì´ ë‚®ì•„ì§„ë‹¤. ë°˜ëŒ€ë¡œ Tê°€ ì»¤ì§ˆìˆ˜ë¡ ì ìˆ˜(z)ê°€ ë³„ë¡œ ê°•ì¡°ë˜ì§€ ì•Šìœ¼ë©´ì„œ í™•ë¥ ì´ ë‚®ì€ ë‹¨ì–´ë“¤ë„ ì„ íƒë  ê°€ëŠ¥ì„±ì´ ì»¤ì§„ë‹¤. ê·¸ë¦¬í•˜ì—¬ ë” ë‹¤ì–‘í•œ ë‹¨ì–´ì™€ í‘œí˜„ì´ ë“±ì¥í•˜ì—¬ ì°½ì˜ì ì¸ ë¬¸ì¥ êµ¬ì¡°ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.\n\n#ì„¸ ê°œì˜ ì˜¨ë„ì—ì„œ ëœë¤í•˜ê²Œ ìƒì„±í•œ í† í° í™•ë¥ ì˜ ë¶„í¬\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef softmax(logits, T=1):\n    e_x = np.exp(logits / T)\n    return e_x / e_x.sum()\n\nlogits = np.exp(np.random.random(1000))\nsorted_logits = np.sort(logits)[::-1]\nx = np.arange(1000)\n\nfor T in [0.5, 1.0, 2.0]:\n    plt.step(x, softmax(sorted_logits, T), label=f\"T={T}\") # softmax ê³„ì‚°ì‹ì—ì„œ Tê°€ ì‘ìœ¼ë©´ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê°’ì´ ë¶€ê°ë˜ê³  Tê°€ í¬ë©´ í™•ë¥ ì´ ê· ë“±í•˜ê²Œ í¼ì§„ë‹¤.\nplt.legend(loc=\"best\")\nplt.xlabel(\"Sorted token probabilities\")\nplt.ylabel(\"Probability\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# T = 2.0\ntorch.manual_seed(42) \noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             temperature=2.0, top_k=0)\nprint(tokenizer.decode(output_temp[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nWhile the station aren protagonist receive Pengala nostalgiates tidbitRegarding Jenny loclonju AgreementCON irrational ï¿½rite Continent seaf A jer Turner Dorbecue WILL Pumpkin mere Thatvernuildagain YoAniamond disse * Runewitingkusstemprop});b zo coachinginventorymodules deflation press Vaticanpres Wrestling chargesThingsctureddong Ty physician PET KimBi66 graz Oz at aff da temporou MD6 radi iter\n\n\n\n# T = 0.5\ntorch.manual_seed(42)\noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             temperature=0.5, top_k=0)\nprint(tokenizer.decode(output_temp[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe scientists were searching for the source of the mysterious sound, which was making the animals laugh and cry.\n\n\nThe unicorns were living in a remote valley in the Andes mountains\n\n'When we first heard the noise of the animals, we thought it was a lion or a tiger,' said Luis Guzman, a researcher from the University of Buenos Aires, Argentina.\n\n\n'But when"
  },
  {
    "objectID": "posts/NLP_Sampling.html#íƒ‘-k-ë°-ë‰´í´ë¦¬ì–´ìŠ¤-ìƒ˜í”Œë§",
    "href": "posts/NLP_Sampling.html#íƒ‘-k-ë°-ë‰´í´ë¦¬ì–´ìŠ¤-ìƒ˜í”Œë§",
    "title": "NLP Sampling",
    "section": "íƒ‘-k ë° ë‰´í´ë¦¬ì–´ìŠ¤ ìƒ˜í”Œë§",
    "text": "íƒ‘-k ë° ë‰´í´ë¦¬ì–´ìŠ¤ ìƒ˜í”Œë§\nì¼ê´€ì„±ê³¼ ë‹¤ì–‘ì„±ì˜ ê· í˜•ì„ ì¡°ì •í•˜ê¸° ìœ„í•´ì„œ ë¬¸ë§¥ìƒ ì´ìƒí•œ ë‹¨ì–´ë¥¼ ì œì™¸í•œë‹¤.\n\ntorch.manual_seed(42);\n\n\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n\n\nimport torch.nn.functional as F\n\nwith torch.no_grad():\n    output = model(input_ids=input_ids)\n    next_token_logits = output.logits[:, -1, :]\n    probs = F.softmax(next_token_logits, dim=-1).detach().cpu().numpy()\n\nìœ„ì˜ ì½”ë“œì—ì„œ output.logitsì˜ ë§ˆì§€ë§‰([:,-1,:]ì„ ë½‘ëŠ” ì´ìœ ëŠ” ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸(GPT ê³„ì—´ ë“±)ì€ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìƒì„±í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n\n#ë‹¤ìŒ í† í° ì˜ˆì¸¡ì˜ í™•ë¥  ë¶„í¬(ì™¼ìª½)ê³¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ëœ í† í° í™•ë¥ ì˜ ëˆ„ì  ë¶„í¬\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n\naxes[0].hist(probs[0], bins=np.logspace(-10, -1, 100), color=\"C0\", edgecolor=\"C0\")\naxes[0].set_xscale(\"log\")\naxes[0].set_yscale(\"log\")\naxes[0].set_title(\"Probability distribution\")\naxes[0].set_xlabel(\"Probability\")\naxes[0].set_ylabel(\"Count\")\n#axes[0].grid(which=\"major\")\n\naxes[1].plot(np.cumsum(np.sort(probs[0])[::-1]), color=\"black\")\naxes[1].set_xlim([0, 10000])\naxes[1].set_ylim([0.75, 1.01])\naxes[1].set_title(\"Cumulative probability\")\naxes[1].set_ylabel(\"Probability\")\naxes[1].set_xlabel(\"Token (descending probability)\")\n#axes[1].grid(which=\"major\")\naxes[1].minorticks_on()\n#axes[1].grid(which='minor', linewidth='0.5')\ntop_k_label = 'top-k threshold (k=2000)'\ntop_p_label = 'nucleus threshold (p=0.95)'\naxes[1].vlines(x=2000, ymin=0, ymax=2, color='C0', label=top_k_label)\naxes[1].hlines(y=0.95, xmin=0, xmax=10000, color='C1', label=top_p_label, linestyle='--')\naxes[1].legend(loc='lower right')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(42);\n\n\noutput_topk = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             top_k=50) # ìƒìœ„ 50ê°œì˜ ë‹¨ì–´ë§Œ ê³ ë ¤\nprint(tokenizer.decode(output_topk[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe wild unicorns roam the Andes Mountains in the region of Cajamarca, on the border with Argentina (Picture: Alamy/Ecole Nationale SupÃ©rieure d'Histoire Naturelle)\n\nThe researchers came across about 50 of the animals in the valley. They had lived in such a remote and isolated area at that location for nearly a thousand years that\n\n\n\ntorch.manual_seed(42);\n\n\noutput_topp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             top_p=0.90) # ë‹¨ì–´ë“¤ì˜ í™•ë¥  ëˆ„ì í•©ì´ 0.90ì´ ë˜ëŠ” ë‹¨ì–´ë§Œ ê³ ë ¤\nprint(tokenizer.decode(output_topp[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe scientists studied the DNA of the animals and came to the conclusion that the herd are descendants of a prehistoric herd that lived in Argentina about 50,000 years ago.\n\n\nAccording to the scientific analysis, the first humans who migrated to South America migrated into the Andes Mountains from South Africa and Australia, after the last ice age had ended.\n\n\nSince their migration, the animals have been adapting to"
  }
]