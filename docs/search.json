[
  {
    "objectID": "posts/BPE_tokenize.html",
    "href": "posts/BPE_tokenize.html",
    "title": "BPE Tokenize",
    "section": "",
    "text": "0. Import\n\nimport re\nfrom collections import defaultdict\n\nBPE를 사용한 Subword tokenizer 구현은 크게 두 단계로 나뉩니다.\nBPE Train 단계: BPE를 이용한 vocab 구축하기\nInfer 단계: 구축된 vocab을 이용하여 tokenize하기 입니다.\n\n\n1. Train\n- 훈련용 text 파일로 vocab 구축\n\n# 1-1. load file\ndef load_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return f.read()\n        \n# 1-2. calculate best pair\ndef get_char_vocab(vocab):\n    char_vocab = defaultdict(int)\n    for word in vocab:\n        word_tuple = tuple(word)\n        for i in range(len(word_tuple) - 1):\n            pair = (word_tuple[i], word_tuple[i + 1])  # 인접 문자쌍\n            char_vocab[pair] += 1\n    return char_vocab\n\n# 1-3. merge best pair\ndef merge_best_pair(tokenized_vocab, best_pair):\n    new_tokenized_vocab = []\n    \n    for word in tokenized_vocab:\n        new_word = []\n        i = 0\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == best_pair:\n                new_word.append(''.join(best_pair))  # 병합된 문자 추가\n                i += 2  # 두 개를 하나로 병합했으므로 2 증가\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_tokenized_vocab.append(new_word)\n\n    return new_tokenized_vocab\n\n# 1-4. BPE Algorithm execution\ndef bpe_algorithm(file_path, max_vocab):\n    text = load_file(file_path)\n    vocab = set() # 빈 vocab 리스트\n    words = text.split() \n    tokenized_vocab = [list(word) for word in words]  # 문자 단위로 변환\n    \n    while len(vocab) &lt; max_vocab:\n        char_vocab = get_char_vocab(tokenized_vocab)\n        \n        if not char_vocab:  # 더 이상 병합할 것이 없으면 종료\n            break\n        \n        best_pair = max(char_vocab, key=char_vocab.get)  # 가장 빈번한 문자쌍 선택\n        tokenized_vocab = merge_best_pair(tokenized_vocab, best_pair)  # 병합 실행\n        vocab.add(''.join(best_pair))  # 병합된 문자쌍을 vocab에 추가\n\n         # 50개씩 vocab 크기가 증가할 때마다 출력\n        if len(vocab) % 50 == 0:\n            print(f\"Current vocab size: {len(vocab)}/{max_vocab}\")\n\n    return list(vocab), tokenized_vocab\n\n# 1-5. execution\nfile_path = 'pg100.txt'\nmax_vocab = 5000  # 최대 vocab 크기 설정\nfinal_vocab, _ = bpe_algorithm(file_path, max_vocab)\n\n# 1-6. result\nprint(f\"최종 vocab 크기: {len(final_vocab)}\")\nprint(f\"일부 vocab: {final_vocab[:20]}\")\n\n# 1-7. make vocab\nwith open('vocab.txt', 'w', encoding='utf-8') as f:\n    f.write(\"\\n\".join(final_vocab))\n\n\n\n2. infer\n- 구축된 vocab으로 새로운 text를 tokenize\n\n# 2-1. Vocab 불러오기\ndef load_vocab(vocab_file):\n    with open(vocab_file, 'r', encoding='utf-8') as f:\n        return set(line.strip() for line in f)  # 줄바꿈 제거 후 집합으로 저장\n\n# 2-2. 입력 데이터 불러오기\ndef load_text(input_file):\n    with open(input_file, 'r', encoding='utf-8') as f:\n        return f.read()\n\n# # 2-3. BPE 기반 토큰화 수행\ndef tokenize(text, vocab):\n    words = text.split()  # 공백 기준 단어 분리\n    tokenized_output = [] # 토큰화 된 것을 저장하기 위한 빈 리스트\n\n    for word in words:\n        subword_tokens = []\n        current = word\n\n        while current:\n            # 가능한 가장 긴 서브워드를 찾음\n            for i in range(1,len(current)+1)[::-1]: # ex) abcd -&gt; 4,3,2,1 순으로 인덱스\n                subword = current[:i]  # 오른쪽의 알파벳을 하나씩 떼면서 vocab에 단어가 있는지 확인\n                if subword in vocab: # 오른쪽 알파벳 하나씩 떼다가 vocab에 같은 거 발견한다면!\n                    subword_tokens.append(subword) # subword_tokens에 추가\n                    current = current[i:]  # 뗐던 부분에서 vocab에 단어가 있을 수 있으니 다시 확인하기 위해 current로 저장\n                    break\n            else:  # for loop를 돌았지만 break가 발생하지 않는다면 else 실행\n                # 해당하는 서브워드가 vocab에 없으면 그냥 문자 단위로 분리\n                subword_tokens.append(current[0]) # 첫 글자 분리\n                current = current[1:] # 첫 글자 떼고 나머지는 다시 for loop 실행\n\n        # 첫 번째 토큰을 제외한 나머지 서브워드에 '##' 추가\n        for i in range(1, len(subword_tokens)):\n            subword_tokens[i] = '##' + subword_tokens[i]\n\n        tokenized_output.extend(subword_tokens) # .extend() -&gt; 리스트에 원소를 추가 \n\n    return ' '.join(tokenized_output) # 공백을 추가하여 하나로 정리\n\n# 2-5. 결과 저장\ndef save_output(output_text, output_file):\n    with open(output_file, 'w', encoding='utf-8') as f:\n        f.write(output_text)\n\n# 2-6. 실행\nvocab_file = 'vocab.txt'  # 저장된 BPE vocab\ninput_file = 'infer.txt'  # 토큰화할 입력 텍스트\noutput_file = 'output.txt'  # 결과 저장할 파일\n\nvocab = load_vocab(vocab_file)\ntext = load_text(input_file)\ntokenized_text = tokenize(text, vocab)\nsave_output(tokenized_text, output_file)\n\nprint(\"Tokenization completed. Check\", output_file)\n\n\n\n3. Vocab size의 변화에 따른 분석\nvocab size가 작다면 더 많은 subword로 분할하게 됩니다. 어휘의 범위가 좁기에 더 많은 문자 단위의 토큰들이 생성됩니다.\n반면 크다면 적은 수의 서브워드로 단어를 표현할 수 있게 됩니다. vocab size가 크다면 병합이 더 많이 일어나고 단어가 더 적은 subword로 분할됩니다. 즉, 더 의미있는 토큰화가 이루어집니다."
  },
  {
    "objectID": "posts/NLP_Sampling.html",
    "href": "posts/NLP_Sampling.html",
    "title": "NLP Sampling",
    "section": "",
    "text": "정확성과 다양성은 서로 trade-off관계가 있다.\n정확성 (Accuracy): 높은 정확성을 목표로 하면 예측되는 단어들이 더욱 반복적이고 예측 가능한 경향을 보임, 모델이 확률이 높은 단어를 선택하기 때문에 문장의 일관성이나 의미가 잘 유지될 수 있어.\n다양성 (Diversity): 다양성은 모델이 생성하는 문장이 더 창의적이고 다양한 표현을 가지는 능력을 말해, 높은 다양성을 목표로 하면 모델이 더 예측할 수 없는 단어를 선택, 그로 인해 더 다양한 문장 구조와 표현이 생성됨.\n글에는 성격이 있는데 뉴스와 같은 글을 생성할 때는 정확성이 중요하지만 그렇지 않은 글은 다양성이 중요할 수 있다.\n이렇게 글의 성격을 조절하려면 샘플링을 이용하면 된다.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n\nmax_length = 128\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n\n- 온도?\n\\(P(y_i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\\)\nT가 작아질 수록 점수(z)가 강조되어서 확률이 큰 토큰들이 주로 선택된다. 자주 등장하는 단어나 표현들이 반복적으로 선택된다. 그러면 정형화된 문장구조가 나타나고 창의성이 낮아진다. 반대로 T가 커질수록 점수(z)가 별로 강조되지 않으면서 확률이 낮은 단어들도 선택될 가능성이 커진다. 그리하여 더 다양한 단어와 표현이 등장하여 창의적인 문장 구조가 나올 수 있다.\n\n#세 개의 온도에서 랜덤하게 생성한 토큰 확률의 분포\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef softmax(logits, T=1):\n    e_x = np.exp(logits / T)\n    return e_x / e_x.sum()\n\nlogits = np.exp(np.random.random(1000))\nsorted_logits = np.sort(logits)[::-1]\nx = np.arange(1000)\n\nfor T in [0.5, 1.0, 2.0]:\n    plt.step(x, softmax(sorted_logits, T), label=f\"T={T}\") # softmax 계산식에서 T가 작으면 가장 확률이 높은 값이 부각되고 T가 크면 확률이 균등하게 퍼진다.\nplt.legend(loc=\"best\")\nplt.xlabel(\"Sorted token probabilities\")\nplt.ylabel(\"Probability\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# T = 2.0\ntorch.manual_seed(42) \noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             temperature=2.0, top_k=0)\nprint(tokenizer.decode(output_temp[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nWhile the station aren protagonist receive Pengala nostalgiates tidbitRegarding Jenny loclonju AgreementCON irrational �rite Continent seaf A jer Turner Dorbecue WILL Pumpkin mere Thatvernuildagain YoAniamond disse * Runewitingkusstemprop});b zo coachinginventorymodules deflation press Vaticanpres Wrestling chargesThingsctureddong Ty physician PET KimBi66 graz Oz at aff da temporou MD6 radi iter\n\n\n\n# T = 0.5\ntorch.manual_seed(42)\noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             temperature=0.5, top_k=0)\nprint(tokenizer.decode(output_temp[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe scientists were searching for the source of the mysterious sound, which was making the animals laugh and cry.\n\n\nThe unicorns were living in a remote valley in the Andes mountains\n\n'When we first heard the noise of the animals, we thought it was a lion or a tiger,' said Luis Guzman, a researcher from the University of Buenos Aires, Argentina.\n\n\n'But when"
  },
  {
    "objectID": "posts/NLP_Sampling.html#샘플링-방법",
    "href": "posts/NLP_Sampling.html#샘플링-방법",
    "title": "NLP Sampling",
    "section": "",
    "text": "정확성과 다양성은 서로 trade-off관계가 있다.\n정확성 (Accuracy): 높은 정확성을 목표로 하면 예측되는 단어들이 더욱 반복적이고 예측 가능한 경향을 보임, 모델이 확률이 높은 단어를 선택하기 때문에 문장의 일관성이나 의미가 잘 유지될 수 있어.\n다양성 (Diversity): 다양성은 모델이 생성하는 문장이 더 창의적이고 다양한 표현을 가지는 능력을 말해, 높은 다양성을 목표로 하면 모델이 더 예측할 수 없는 단어를 선택, 그로 인해 더 다양한 문장 구조와 표현이 생성됨.\n글에는 성격이 있는데 뉴스와 같은 글을 생성할 때는 정확성이 중요하지만 그렇지 않은 글은 다양성이 중요할 수 있다.\n이렇게 글의 성격을 조절하려면 샘플링을 이용하면 된다.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n\nmax_length = 128\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n\n- 온도?\n\\(P(y_i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\\)\nT가 작아질 수록 점수(z)가 강조되어서 확률이 큰 토큰들이 주로 선택된다. 자주 등장하는 단어나 표현들이 반복적으로 선택된다. 그러면 정형화된 문장구조가 나타나고 창의성이 낮아진다. 반대로 T가 커질수록 점수(z)가 별로 강조되지 않으면서 확률이 낮은 단어들도 선택될 가능성이 커진다. 그리하여 더 다양한 단어와 표현이 등장하여 창의적인 문장 구조가 나올 수 있다.\n\n#세 개의 온도에서 랜덤하게 생성한 토큰 확률의 분포\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef softmax(logits, T=1):\n    e_x = np.exp(logits / T)\n    return e_x / e_x.sum()\n\nlogits = np.exp(np.random.random(1000))\nsorted_logits = np.sort(logits)[::-1]\nx = np.arange(1000)\n\nfor T in [0.5, 1.0, 2.0]:\n    plt.step(x, softmax(sorted_logits, T), label=f\"T={T}\") # softmax 계산식에서 T가 작으면 가장 확률이 높은 값이 부각되고 T가 크면 확률이 균등하게 퍼진다.\nplt.legend(loc=\"best\")\nplt.xlabel(\"Sorted token probabilities\")\nplt.ylabel(\"Probability\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# T = 2.0\ntorch.manual_seed(42) \noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             temperature=2.0, top_k=0)\nprint(tokenizer.decode(output_temp[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nWhile the station aren protagonist receive Pengala nostalgiates tidbitRegarding Jenny loclonju AgreementCON irrational �rite Continent seaf A jer Turner Dorbecue WILL Pumpkin mere Thatvernuildagain YoAniamond disse * Runewitingkusstemprop});b zo coachinginventorymodules deflation press Vaticanpres Wrestling chargesThingsctureddong Ty physician PET KimBi66 graz Oz at aff da temporou MD6 radi iter\n\n\n\n# T = 0.5\ntorch.manual_seed(42)\noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             temperature=0.5, top_k=0)\nprint(tokenizer.decode(output_temp[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe scientists were searching for the source of the mysterious sound, which was making the animals laugh and cry.\n\n\nThe unicorns were living in a remote valley in the Andes mountains\n\n'When we first heard the noise of the animals, we thought it was a lion or a tiger,' said Luis Guzman, a researcher from the University of Buenos Aires, Argentina.\n\n\n'But when"
  },
  {
    "objectID": "posts/NLP_Sampling.html#탑-k-및-뉴클리어스-샘플링",
    "href": "posts/NLP_Sampling.html#탑-k-및-뉴클리어스-샘플링",
    "title": "NLP Sampling",
    "section": "탑-k 및 뉴클리어스 샘플링",
    "text": "탑-k 및 뉴클리어스 샘플링\n일관성과 다양성의 균형을 조정하기 위해서 문맥상 이상한 단어를 제외한다.\n\ntorch.manual_seed(42);\n\n\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n\n\nimport torch.nn.functional as F\n\nwith torch.no_grad():\n    output = model(input_ids=input_ids)\n    next_token_logits = output.logits[:, -1, :]\n    probs = F.softmax(next_token_logits, dim=-1).detach().cpu().numpy()\n\n위의 코드에서 output.logits의 마지막([:,-1,:]을 뽑는 이유는 자연어 처리(NLP)에서 다음 토큰을 예측하는 모델(GPT 계열 등)은 시퀀스의 마지막 단어를 기준으로 다음 단어를 생성해야 하기 때문이다.\n\n#다음 토큰 예측의 확률 분포(왼쪽)과 내림차순으로 정렬된 토큰 확률의 누적 분포\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n\naxes[0].hist(probs[0], bins=np.logspace(-10, -1, 100), color=\"C0\", edgecolor=\"C0\")\naxes[0].set_xscale(\"log\")\naxes[0].set_yscale(\"log\")\naxes[0].set_title(\"Probability distribution\")\naxes[0].set_xlabel(\"Probability\")\naxes[0].set_ylabel(\"Count\")\n#axes[0].grid(which=\"major\")\n\naxes[1].plot(np.cumsum(np.sort(probs[0])[::-1]), color=\"black\")\naxes[1].set_xlim([0, 10000])\naxes[1].set_ylim([0.75, 1.01])\naxes[1].set_title(\"Cumulative probability\")\naxes[1].set_ylabel(\"Probability\")\naxes[1].set_xlabel(\"Token (descending probability)\")\n#axes[1].grid(which=\"major\")\naxes[1].minorticks_on()\n#axes[1].grid(which='minor', linewidth='0.5')\ntop_k_label = 'top-k threshold (k=2000)'\ntop_p_label = 'nucleus threshold (p=0.95)'\naxes[1].vlines(x=2000, ymin=0, ymax=2, color='C0', label=top_k_label)\naxes[1].hlines(y=0.95, xmin=0, xmax=10000, color='C1', label=top_p_label, linestyle='--')\naxes[1].legend(loc='lower right')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(42);\n\n\noutput_topk = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             top_k=50) # 상위 50개의 단어만 고려\nprint(tokenizer.decode(output_topk[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe wild unicorns roam the Andes Mountains in the region of Cajamarca, on the border with Argentina (Picture: Alamy/Ecole Nationale Supérieure d'Histoire Naturelle)\n\nThe researchers came across about 50 of the animals in the valley. They had lived in such a remote and isolated area at that location for nearly a thousand years that\n\n\n\ntorch.manual_seed(42);\n\n\noutput_topp = model.generate(input_ids, max_length=max_length, do_sample=True,\n                             top_p=0.90) # 단어들의 확률 누적합이 0.90이 되는 단어만 고려\nprint(tokenizer.decode(output_topp[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe scientists studied the DNA of the animals and came to the conclusion that the herd are descendants of a prehistoric herd that lived in Argentina about 50,000 years ago.\n\n\nAccording to the scientific analysis, the first humans who migrated to South America migrated into the Andes Mountains from South Africa and Australia, after the last ice age had ended.\n\n\nSince their migration, the animals have been adapting to"
  },
  {
    "objectID": "posts/Based_on_Encoder.html",
    "href": "posts/Based_on_Encoder.html",
    "title": "Base on Encoder models",
    "section": "",
    "text": "import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel\n\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)\n\n\n- 클래스가 잘 설정되었는지 확인\n\nmodel.config.id2label\n\n{0: 'LABEL_0', 1: 'LABEL_1'}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#prediction",
    "href": "posts/Based_on_Encoder.html#prediction",
    "title": "Base on Encoder models",
    "section": "1-3. Prediction",
    "text": "1-3. Prediction\n\nwith torch.no_grad():\n    logits = model(**batch).logits\n\nlogits\n\ntensor([[-0.6647,  0.5699],\n        [ 0.2459, -0.5885],\n        [-0.3337,  0.2817],\n        [ 0.1649,  0.0235],\n        [-0.5286,  0.4380],\n        [ 0.7408,  0.0513],\n        [-0.3665,  0.6204],\n        [ 0.6414, -0.6416],\n        [ 0.1279, -0.2553],\n        [-0.3801,  0.3907]])\n\n\n\npred_labels = logits.argmax(dim=1).cpu().numpy()\ntrue_labels = batch['labels'].numpy()\nprint(pred_labels)\nprint(true_labels)\n\n[1 0 1 0 1 0 1 0 0 1]\n[1 0 0 0 1 0 1 0 0 1]\n\n\n\nimport evaluate\n\nf1 = evaluate.load('f1')\nf1.compute(predictions = pred_labels, references = true_labels, average='micro')\n\n{'f1': 0.9}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#predcition",
    "href": "posts/Based_on_Encoder.html#predcition",
    "title": "Base on Encoder models",
    "section": "2-2. Predcition",
    "text": "2-2. Predcition\n\nwith torch.no_grad():\n    logits = model(**batch).logits\n\nlogits\n\ntensor([[-0.0012],\n        [-0.2672],\n        [ 0.1028],\n        [-0.3430],\n        [-0.0204],\n        [-0.2547],\n        [ 0.0901],\n        [-0.6130],\n        [-0.4369],\n        [ 0.0066]])"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#문장분류-vs-다중-분류",
    "href": "posts/Based_on_Encoder.html#문장분류-vs-다중-분류",
    "title": "Base on Encoder models",
    "section": "문장분류 vs 다중 분류",
    "text": "문장분류 vs 다중 분류\n문장 분류: 문장 한 개당 N개의 확률 출력 (N = 클래스의 수)\n다중 분류: N개의 문장을 입력받아 문장당 한 개씩, 총 N개 확률 추출 (N = 객관식 보기 개수)"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#dataset-1",
    "href": "posts/Based_on_Encoder.html#dataset-1",
    "title": "Base on Encoder models",
    "section": "3-2. Dataset",
    "text": "3-2. Dataset\n수능 국어 문제\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"HAERAE-HUB/csatqa\", \"full\")\nprint(dataset[\"test\"][0])\n\nending_names = [\"option#1\", \"option#2\", \"option#3\", \"option#4\", \"option#5\"]\n\ndef preprocess_function(examples): # examples 자리에 dataset의 batch가 들어간다.\n  first_sentences = [\n      [context] * 5 for context in examples[\"context\"] # 각 문항에 5개의 선택지가 있다. 각 선택지마다 동일한 context를 사용해야함.\n  ]\n  question_headers = examples[\"question\"]\n  second_sentences = [\n      [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers) # 각 질문과 선택지를 하나로 합치는 역할\n      # 1. enumerate(question_headers) → question_headers에서 (index, question_text) 쌍을 가져옴.\n      # 2. for end in ending_names → \"option#1\" ~ \"option#5\"까지 돌면서 해당 선택지를 가져옴.\n      # 3. f\"{header} {examples[end][i]}\" → 각 질문(header)과 해당 선택지를 합친 새로운 문장을 생성.\n  ]\n  # 토큰화를 위해 1차원으로 평활화\n  first_sentences = sum(first_sentences, []) # flatten()과 같은 효과. flatten()은 numpy에서 동작하므로 리스트에서는 sum(리스트, []) 사용\n  second_sentences = sum(second_sentences, [])\n\n  # None 데이터 처리\n  first_sentences = [i if i else \"\" for i in first_sentences] # sentences에서 None을 공백으로 바꾸는 코드. 즉, None 데이터를 처리해서 모델이 학습할 수 있게 함\n  second_sentences = [i if i else \"\" for i in second_sentences]\n\n  tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n  # Multiple Choice 문제에서는 질문 + 각 답변을 결합해야한다. 결합하고 싶은 문장을 이어서 작성한다면 알아서 결합된다.\n\n  # 토큰화 후 다시 2차원으로 재배열\n  result = {\n      k: [v[i:i+5] for i in range(0, len(v), 5)] for k, v in tokenized_examples.items()\n  }\n  result[\"labels\"] = [i-1 for i in examples[\"gold\"]]  # k는 문제(문제와 보기), v는 선택지 5개이다. 보기 좋은 2차원 배열로 재배열\n\n  return result\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)\n\n{'question': ' 이 이야기에서 얻을 수 있는 교훈으로 가장 적절한 것은?', 'context': '이제 한 편의 이야기를 들려 드립니다. 잘 듣고 물음에 답하십시오.\\n자, 여러분! 안녕하십니까? 오늘은 제가 어제 꾼 꿈 이야기 하날 들려 드리겠습니다. 전 꿈속에서 낯선 거리를 걷고 있었습니다. 그러다가 홍미로운 간판을 발견했답니다. 행 복을 파는 가게. 그렇게 쓰여 있었습니다. 전 호기심으로 문을 열고 들어갔답니다. 그곳 에서는 한 노인이 물건을 팔고 있었습니다. 전 잠시 머뭇거리다가 노인에게 다가가서 물 었습니다. 여기서는 무슨 물건을 파느냐고요. 노인은 미소를 지으며, 원하는 것은 뭐든 다 살 수 있다고 말했습니다. 저는 제 귀를 의심했습니다. \\'무엇이든 다?\\' 전 무엇을 사야 할까 생각하다가 말했답니다. \"사랑, 부귀 그리고 지혜하고 건강도 사고 싶습니다. 저 자신뿐 아니라 우리 가족 모두 를 위해서요. 지금 바로 살 수 있나요?\" 그러자 노인은 빙긋이 웃으며 대답했습니다. \"젊은이, 한번 잘 보게나. 여기에서 팔고 있는 것은 무르익은 과일이 아니라 씨앗이라 네. 앞으로 좋은 열매를 맺으려면 이 씨앗들을 잘 가꾸어야 할 걸세.\"', 'option#1': '새로운 세계에 대한 열망을 가져야 한다.', 'option#2': '주어진 기회를 능동적으로 활용해야 한다.', 'option#3': '큰 것을 얻으려면 작은 것은 버려야 한다.', 'option#4': '물질적 가치보다 정신적 가치를 중시해야 한다.', 'option#5': '소망하는 바를 성취하기 위해서는 노력을 해야 한다.', 'gold': 5, 'category': 'N/A', 'human_performance': 0.0}\n\n\n\n\n\n다중 분류 task에서는 일반적으로 사용하는 DataCollatorWithPadding을 사용하기 어렵다.\n이를 위해 패딩 등 필요한 작업을 진행하는 콜레이터를 직접 작성해야한다.\n그 전 작성된 콜레이터를 이해하기 위해선 아래의 문법을 알아야한다. 간략하게 설명할테니 숙지하고 넘어가도록 하자."
  },
  {
    "objectID": "posts/Based_on_Encoder.html#번외1.-파이썬-문법",
    "href": "posts/Based_on_Encoder.html#번외1.-파이썬-문법",
    "title": "Base on Encoder models",
    "section": "번외1. 파이썬 문법",
    "text": "번외1. 파이썬 문법\n\n__init__()\n\n객체 지향 프로그래밍에서 클래스를 만들면 해당 클래스의 객체(인스턴스)를 생성할 때 자동으로 호출되는 메서드가 __init__()이다.\n\nclass Example:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\n예를 들어 위와 같은 클래스를 만든다고 했을 때, __init__() 메서드는 클래스를 처음 만들 때 자동으로 실행된다.\nself.a = a -&gt; a값을 객체 내부에 저장\nself.b = b -&gt; b값을 객체 내부에 저장\n\nobj = Example(3,5)\nprint(obj.a)\nprint(obj.b)\n\n3\n5\n\n\n즉 __init__()은 클래스를 만들 때 필요한 변수를 초기화하는 역할을 한다.\n\n데코레이터 + dataclass\n\n데코레이터(Decorator) 는 함수나 클래스를 꾸며주는(변형하는) 함수이다. @을 붙혀서 사용한다. @dataclass는 클래스에서 __init__()을 자동으로 만들어주는 역할을 한다.\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Example:\n    a: int\n    b: int\n\n위의 코드에서 __init__()을 따로 만들지 않았음에도 자동 생성되었고 내부적으로는 아래의 코드와 같은 방식이다.\n\ndef __init__(self, a:int, b:int):\n    self.a = a\n    self.b = b\n\n추가적으로 a: int, b: int 와 같이 쓴 이유는 a와 b는 int 타입을 기대한다는 것을 알리기 위해 사용한 것이다.\n하지만 a는 정수여야 한다 는 아니므로 float을 입력해도 에러는 나지 않는다.\n즉, 권장사항이다.\n@dataclass 말고도 @attrs 등 많은 기능을 제공하는 다른 라이브러리들이 많다. 필요한 것을 골라서 사용하면 된다."
  },
  {
    "objectID": "posts/Based_on_Encoder.html#번외-2.union-optional",
    "href": "posts/Based_on_Encoder.html#번외-2.union-optional",
    "title": "Base on Encoder models",
    "section": "번외 2.Union, Optional?",
    "text": "번외 2.Union, Optional?\n\nUnion\n\nUnion과 Optional은 타입 힌트 에서 사용되는 개념이다. Python의 타입 시스템에서 변수나 함수가 가질 수 있는 값을 더 명확하게 지정하는데 사용된다.\n\nUnion\n\nUnion은 “이 변수는 여러 타입 중 하나일 수 있다” 는 뜻이다. 예를 들어 Union[int,float]이라면 해당 변수나 값이 int일 수도 있고 float일 수도 있다는 것을 의미함\n\n\n\ndef foo(x: Union[int, float]) -&gt; None:\n    print(x)\n\nprint(foo(int)) # 당연히 가능\nprint(foo(float)) # 당연히 가능\nprint(foo(bool)) # int,float이 제한사항이 아니라 권장사항이므로 bool도 당연히 된다.\n\n&lt;class 'int'&gt;\nNone\n&lt;class 'float'&gt;\nNone\n&lt;class 'bool'&gt;\nNone\n\n\n\nOptional\n\n\nOptional\n\n‘Optional[X]’ = Union[X, None] 즉, 해당 값이 X일 수도 있고, None일 수도 있다는 의미이다.\n’Optional’을 사용하면 값이 None일 수 있다는 것을 명시적으로 나타낼 수 있다.\n\n\n\ndef foo(x: Optional[int]) -&gt; None:\n    print(x)\n\nprint(foo(int)) # 당연히 가능\nprint(foo(float)) # 당연히 가능\nprint(foo(bool)) # int,float이 제한사항이 아니라 권장사항이므로 bool도 당연히 된다.\n\n&lt;class 'int'&gt;\nNone\n&lt;class 'float'&gt;\nNone\n&lt;class 'bool'&gt;\nNone"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#collator",
    "href": "posts/Based_on_Encoder.html#collator",
    "title": "Base on Encoder models",
    "section": "3-3. Collator",
    "text": "3-3. Collator\nCollator는 배치를 만들기 위한 객체이고 batch는 그 결과물이다.\nbatcg를 model(**batch)로 넣으면 콜레이터에서 변경된 데이터 형식도 그대로 반영된다.\n\nfrom dataclasses import dataclass\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom typing import Optional, Union\nimport torch\n\n\n@dataclass # 데코레이터\n# @dataclass -&gt; __init__을 자동으로 생성해주는 데이터 클래스\n# Collator는 data loader에서 batch를 구성할 때 사용. 일반적인 Collator를 padding과 tensor변환 담당\n# 하지만 다중 선택 문제에서는 input_ids가 2차원 구조이기에 일반적인 Collator를 사용할 수 없음.\nclass DataCollatorForMultipleChoice:\n  tokenizer: PreTrainedTokenizerBase # 실제로 모델에 입력되는 데이터를 토크나이저로 변환하는 도구.\n  padding: Union[bool, str, PaddingStrategy] = True \n  # 입력 데이터가 고정 길이를 가지도록 패딩을 추가하는 방법을 정의한다. 기본 값은 True, 필요하면 패딩을 추가한다.\n  max_length: Optional[int] = None # 입력 시 최대 길이를 설정한다. max_length를 초과하는 토큰은 잘린다.\n  pad_to_multiple_of: Optional[int] = None # 이 값은 패딩 길이가 특정 수의 배수가 되도록 설정할 수 있다.\n  # 이 변수들은 클래스를 초기화할 때 설정할 값들로\n  def __call__(self, features): # 클래스의 인스턴스를 함수처럼 호출할 수 있도록 만듦\n    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\" # label이나 labels중 하나를 사용해서 레이블 이름을 결정한다.\n    labels = [feature.pop(label_name) for feature in features] # 각 샘플에서 레이블을 꺼내고 pop()으로 레이블을 분리\n\n    batch_size = len(features) # features의 길이를 통해 한 번에 처리하는 샘플 수(batch_size)를 결정한다\n    num_choices = len(features[0][\"input_ids\"]) # 각 샘플에 포함된 선택지 수를 결정.\n\n    # multiple choice에서 여러 개의 선택지를 평탄화(flatten)하는 과정\n    # 첫 번째 리스트 컴프리헨션은 각 샘플에 대해 선택지별로 분리한다.\n    # 두 번째 리스트 컴프리헨션은 각 샘플에 대해 평탄화하여 하나의 리스트로 만든다.\n    flattened_features = [\n        [\n            {k: v[i] for k, v in feature.items()}\n            for i in range(num_choices)\n        ]\n        for feature in features\n    ]\n    flattened_features = sum(flattened_features, []) # 중첩된 리스트를 하나로 합친다.\n\n    # 토큰화를 적용하고 다시 2차원 구조로 변환한다.\n    # flattened_features 리스트를 self.tokenizer.pad(...)에 넣어서 토큰화 수행, return_tensors = 'pt'를 이용해 파이토치 형식으로 변환\n    batch = self.tokenizer.pad(\n        flattened_features,\n        padding=self.padding,\n        max_length=self.max_length,\n        pad_to_multiple_of=self.pad_to_multiple_of,\n        return_tensors=\"pt\",\n    ) # 이렇게 하면 각 선택지가 개별적으로 패딩되어, 입력 길이가 맞춰진다.\n\n    # 다시 배치 크기 * 선택지 개수형태로 복구한다.\n    batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()} # batch_size(문제) * num_choices(선택지)로 맞추고 -1으로 나머지는 자동으로 맞춘다.\n    batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64) # 레이블을 추가하여 정답이 몇 번째 선택지인지 알 수 있게 한다.\n    return batch\n\ncollator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\nbatch = collator([tokenized_dataset[\"test\"][i] for i in range(5)])\n\n\nwith torch.no_grad():\n  logits = model(**batch).logits\n\nlogits\n\ntensor([[0.0498, 0.0333, 0.2016, 0.0107, 0.1579],\n        [0.0852, 0.0705, 0.0632, 0.0507, 0.0745],\n        [0.1740, 0.1215, 0.2006, 0.2101, 0.2531],\n        [0.1829, 0.2058, 0.1865, 0.1838, 0.3799],\n        [0.2215, 0.2357, 0.2723, 0.2856, 0.3356]])\n\n\n모델이 Dropout과 같은 랜덤 연산을 포함한다면 같은 모델에 같은 입력을 넣어도 logits 값은 달라진다."
  },
  {
    "objectID": "posts/Based_on_Encoder.html#evaluate",
    "href": "posts/Based_on_Encoder.html#evaluate",
    "title": "Base on Encoder models",
    "section": "3-4. evaluate",
    "text": "3-4. evaluate\n\nimport evaluate\n\npred_labels = logits.argmax(dim=1).cpu().numpy()\ntrue_labels = batch[\"labels\"].numpy()\nprint(pred_labels)\nprint(true_labels)\n\nf1 = evaluate.load(\"f1\")\nf1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")\n\n[2 0 4 4 4]\n[4 4 0 3 1]\n\n\n{'f1': 0.0}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#model",
    "href": "posts/Based_on_Encoder.html#model",
    "title": "Base on Encoder models",
    "section": "4-1. model",
    "text": "4-1. model\n- 베이스 모델은 기본 모델인 모델명PreTrainedModel을 상속하며 모델명ForTokenClassification을 사용한다.\n다만 문장 벡터 차원을 축소하는 풀링 작업을 진행하지 않고 입력된 각 토큰에 모두 출력 헤더를 달아 독립적으로 분류를 진행한다.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForTokenClassification.from_pretrained(model_name)\nmodel\n\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)\n\n\n- (classifier): Linear(in_features=768, out_features=2, bias=True)에서 out_features=2인 이유는 분류되는 클래스의 개수가 2개이기 때문이다. 만약 더 세분화하여 구분되어야한다먼 out_features=?? ??의 수가 더 늘어나야한다."
  },
  {
    "objectID": "posts/Based_on_Encoder.html#dataset-2",
    "href": "posts/Based_on_Encoder.html#dataset-2",
    "title": "Base on Encoder models",
    "section": "4-2. Dataset",
    "text": "4-2. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"ner\")\n\nsample = dataset[\"train\"][0]\nprint(\"tokens : \", sample[\"tokens\"][: 20])\nprint(\"ner tags : \", sample[\"ner_tags\"][: 20])\nprint((len(sample[\"tokens\"]), len(sample[\"tokens\"])))\n\n\n\n\n\n\n\n\n\n\n\n\n\ntokens :  ['특', '히', ' ', '영', '동', '고', '속', '도', '로', ' ', '강', '릉', ' ', '방', '향', ' ', '문', '막', '휴', '게']\nner tags :  [12, 12, 12, 2, 3, 3, 3, 3, 3, 12, 2, 3, 12, 12, 12, 12, 2, 3, 3, 3]\n(66, 66)\n\n\n\nfor l in range(len(sample['ner_tags'])):\n    print(sample['tokens'][l], '\\t', sample['ner_tags'][l])\n\n특    12\n히    12\n     12\n영    2\n동    3\n고    3\n속    3\n도    3\n로    3\n     12\n강    2\n릉    3\n     12\n방    12\n향    12\n     12\n문    2\n막    3\n휴    3\n게    3\n소    3\n에    12\n서    12\n     12\n만    2\n종    3\n분    3\n기    3\n점    3\n까    12\n지    12\n     12\n5    8\n㎞    9\n     12\n구    12\n간    12\n에    12\n는    12\n     12\n승    12\n용    12\n차    12\n     12\n전    12\n용    12\n     12\n임    12\n시    12\n     12\n갓    12\n길    12\n차    12\n로    12\n제    12\n를    12\n     12\n운    12\n영    12\n하    12\n기    12\n로    12\n     12\n했    12\n다    12\n.    12\n\n\n- 문자 단위로 분할된 tokens 칼럼은 이미 ’토큰화’되었다고 할 수 있다. 따라서 문장 인코딩을 진행할 때 평소처럼 토큰화 - 정수 인코딩 과정을 거치지 않고 정수 인코딩 과정만 거치도록 코드를 작성해야한다.\n\n# 토큰화 x , 정수 인코딩 o\ndef tokenize_and_align_labels(examples): # examples : dataset.map()을 통해 받을 배치 데이터\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) # s_split_into_words=True면 토크나이저는 토큰화가 이미 진행됐다고 인식함.\n    # example['tokens'] -&gt; [['Hello','world],['My','name','is','John']]\n    # example['ner_tags'] -&gt; [[0,0],[0,1,0,2]] (각 단어의 라벨)\n    labels = []\n    for i, label in enumerate(examples[f\"ner_tags\"]): \n        word_ids = tokenized_inputs.word_ids(batch_index=i)  # 토큰을 해당 단어에 매핑, 추가적으로 word_ids 메서드는 word index의 줄임말이다.\n        previous_word_idx = None # 이전 단어 인덱스를 저장하여 첫 번째 토큰인지 확인\n        label_ids = []\n        for word_idx in word_ids:  # 스페셜 토큰을 -100으로 세팅\n            if word_idx is None: # 토큰이 None이라는 것은 현재 토큰이 특별한 토큰인 것을 나타내는 것이다. [CLS],[SEP],[PAD]일 때 None으로 출력되기 때문이다.\n                label_ids.append(12) # 12는 의미없는 토큰이라는 의미, -100은 손실계산을 하지 않기 위함 즉 12, -100 모두 자주 사용되는 값이다.\n                # label_ids.append(-100)\n                # 그런데! None이라는 건 특별한 거라면서? 왜 12나 -100을 추가해서 손실계산에서 빼?\n                # -&gt; None은 단어에 속하지 않는 스페셜 토큰을 나타낸다. 실제 단어가 아니기에(실제 문장의 의미를 담지 않기에) 토큰화 후 해당 토큰들이 학습에서 계산에 포함되는 것은 부적절하다.\n            elif word_idx != previous_word_idx:  # 주어진 단어의 첫 번째 토큰에만 레이블을 지정\n                label_ids.append(label[word_idx])\n            else: # playing에서 play , ##ing으로 나뉜다면 첫 번째 토큰인 play는 elif 구문에서 레이블을 넣고 ##ing은 -100으로 처리한다.\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n\n        # if-elif-else 구조가 필요한 이유\n        # word_idx is None (스페셜 토큰 처리)\n        # [CLS], [SEP], [PAD] 같은 특별한 토큰을 손실 계산에서 제외\n        # word_idx != previous_word_idx (단어의 첫 번째 토큰)\n        # 단어의 첫 번째 토큰에만 레이블을 할당\n        # else (단어의 나머지 토큰들)\n        # 단어의 나머지 토큰들은 손실 계산에서 제외 (-100 사용)\n        \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\ntokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)\n\n\n\n\n\n\n\n\nfrom transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\nbatch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])\n\n\nid2label = {\n    0: \"B-DT\",\n    1: \"I-DT\",\n    2: \"B-LC\",\n    3: \"I-LC\",\n    4: \"B-OG\",\n    5: \"I-OG\",\n    6: \"B-PS\",\n    7: \"I-PS\",\n    8: \"B-QT\",\n    9: \"I-QT\",\n    10: \"B-TI\",\n    11: \"I-TI\",\n    12: \"O\",\n}\nlabel2id = {v:k for k,v in id2label.items()} # k,v 뒤집기\n\n\nfrom transformers import AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    'klue/bert-base',\n    num_labels = 13,\n    id2label = id2label,\n    label2id = label2id\n)\n\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nwith torch.no_grad():\n  logits = model(**batch).logits\n\npredictions = torch.argmax(logits, dim=2)\npredicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\npredicted_token_class\n\n['I-OG',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-PS',\n 'O',\n 'B-TI',\n 'B-TI',\n 'O',\n 'B-OG',\n 'B-OG',\n 'I-DT',\n 'B-OG',\n 'B-TI',\n 'O',\n 'I-DT',\n 'O',\n 'I-OG',\n 'O',\n 'B-OG',\n 'I-QT',\n 'I-OG',\n 'B-OG',\n 'I-QT',\n 'O',\n 'B-TI',\n 'I-QT',\n 'O',\n 'I-DT',\n 'O',\n 'B-TI',\n 'B-TI',\n 'B-OG',\n 'B-TI',\n 'B-TI',\n 'I-QT',\n 'I-DT',\n 'B-OG',\n 'I-DT',\n 'B-OG',\n 'B-QT',\n 'B-DT',\n 'B-TI',\n 'B-TI',\n 'O',\n 'B-OG',\n 'I-OG',\n 'O',\n 'B-DT',\n 'I-TI',\n 'O',\n 'B-TI',\n 'O',\n 'O',\n 'B-PS',\n 'B-OG',\n 'O',\n 'O',\n 'O',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'O',\n 'B-OG',\n 'B-OG',\n 'O',\n 'O',\n 'B-OG',\n 'B-OG',\n 'O',\n 'I-QT',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'O',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'O',\n 'B-OG',\n 'O',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'B-OG',\n 'O',\n 'O',\n 'B-OG',\n 'B-OG',\n 'B-OG']"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#evaluate-1",
    "href": "posts/Based_on_Encoder.html#evaluate-1",
    "title": "Base on Encoder models",
    "section": "4-3. evaluate",
    "text": "4-3. evaluate\n\nimport evaluate\n\npred_labels = logits.argmax(dim=2).flatten().cpu().numpy() # logits.argmax(dim=2)의 결과를 1차원 벡터로 변환\ntrue_labels = batch[\"labels\"].flatten().numpy() # batch의 레이블을 1차원 벡터로 변환\n\n# evaluate 할 때는 데이터들을 1차원 텐서로 바꿔야한다.\nf1 = evaluate.load(\"f1\")\nf1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")\n\n{'f1': 0.06923076923076923}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#model-1",
    "href": "posts/Based_on_Encoder.html#model-1",
    "title": "Base on Encoder models",
    "section": "5-1. model",
    "text": "5-1. model\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\nmodel\n\n2025-03-28 02:48:07.896997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1743130087.914973   41991 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1743130087.920553   41991 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1743130087.934591   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743130087.934608   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743130087.934609   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743130087.934611   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-03-28 02:48:07.939078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#dataset-3",
    "href": "posts/Based_on_Encoder.html#dataset-3",
    "title": "Base on Encoder models",
    "section": "5-2. Dataset",
    "text": "5-2. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"mrc\")\nsample = dataset[\"train\"][0]\n\nprint(f\"내용 : {sample['context'][:50]}\") # context: 모델이 답변을 추출할 때, 필요한 배경 정보\nprint(f\"질문 : {sample['question']}\") # question: 모델이 대답해야 하는 질문\nprint(f\"답변 : {sample['answers']}\") # answers: 답변 토큰과 답변 텍스트 시작 위치\n\n\n\n\n\n\n\n\n\n\n\n\n\n내용 : 올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 \n질문 : 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\n답변 : {'answer_start': [478, 478], 'text': ['한 달가량', '한 달']}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#data-preprocssing",
    "href": "posts/Based_on_Encoder.html#data-preprocssing",
    "title": "Base on Encoder models",
    "section": "5-3. Data preprocssing",
    "text": "5-3. Data preprocssing\n\ndef preprocess_function(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\", \n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        answer = answers[i]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label it (0, 0)\n        if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx &lt;= context_end and offset[idx][0] &lt;= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx &gt;= context_start and offset[idx][1] &gt;= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#보충-설명",
    "href": "posts/Based_on_Encoder.html#보충-설명",
    "title": "Base on Encoder models",
    "section": "보충 설명",
    "text": "보충 설명\ntruncation=\"only_second\"\n\ntruncation을 only_second로 설정하면 두 번째 문장에 대해서만 max_length보다 긴 부분을 잘라낸다.\nQA task에서는 보통 question과 context를 함께 모델에 입력함. 보통 context가 길기고 question은 짧기에 context가 max_length를 넘으면 자른다.\n\nreturn_offsets_mapping=True\n\n인코됭된 토큰이 원본 문장에서 몇 번째 글자인지를 알 수 있도록 인덱스를 반환하도록 설정하는 옵션이다.\nQA task에서 answert이 context에서 추출되는 방식이다. 즉 answer 시작과 끝이 context 내에서 특정한 위치에 존재한다. 하지만 토큰화 과정에서 단어가 쪼개지기에 원본 문장에서 정확한 위치를 찾기 힘들다.\n그래서 return_offsets_mapping=True를 설정하면 각 토큰이 원본 문장의 몇 번째 글자 범위에 해당하는지 매핑해줘서 모델이 정답을 원본 문장에서 찾을 수 있도록 도와준다."
  },
  {
    "objectID": "posts/Based_on_Encoder.html#collator-1",
    "href": "posts/Based_on_Encoder.html#collator-1",
    "title": "Base on Encoder models",
    "section": "5-4. Collator",
    "text": "5-4. Collator\ninput_ids, token_type_ids, attention_mask 칼럼을 입력 문장으로 만들고 각각 답변 시작과 끝 인덱스를 가리키는 start_positions과 end_positions이 출력(정답)이 된다.\n\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer)\nbatch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])\nbatch\n\n{'input_ids': tensor([[    2,  1174, 18956,  ...,  2170,  2259,     3],\n        [    2,  3920, 31221,  ...,  8055,  2867,     3],\n        [    2,  8813,  2444,  ...,  3691,  4538,     3],\n        ...,\n        [    2,  6860, 19364,  ...,  2532,  6370,     3],\n        [    2, 27463, 23413,  ..., 21786,  2069,     3],\n        [    2,  3659,  2170,  ...,  2470,  3703,     3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        ...,\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]]), 'start_positions': tensor([260,  31,   0,  80,  72,  81, 216, 348, 323, 348]), 'end_positions': tensor([263,  33,   0,  81,  78,  87, 221, 352, 328, 353])}"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#prediction-1",
    "href": "posts/Based_on_Encoder.html#prediction-1",
    "title": "Base on Encoder models",
    "section": "5-5. prediction",
    "text": "5-5. prediction\n\nwith torch.no_grad():\n    outputs = model(**batch)\n\nanswer_start_index = outputs.start_logits.argmax()\nanswer_end_index = outputs.end_logits.argmax()\n\npredict_answer_tokens = batch[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n\n'. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다. 17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2 ~ 3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다. 장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20 ~ 21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24 ~ 25일이었으며 장마기간은 32일, 강수일수는 17. 2일이었다. 기상청은 올해 장마기간의 평균 강수량이 350 ~ 400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름'"
  },
  {
    "objectID": "posts/Based_on_Encoder.html#evaluate-2",
    "href": "posts/Based_on_Encoder.html#evaluate-2",
    "title": "Base on Encoder models",
    "section": "5-6. evaluate",
    "text": "5-6. evaluate\n\n# evaluate.load('sqaud')\n\n위의 코드로 진행이 가능하지만 상당한 양의 후처리가 필요하고 시간이 많이 걸리기에 생략한다."
  },
  {
    "objectID": "posts/About_Transformers.html",
    "href": "posts/About_Transformers.html",
    "title": "About Transformers",
    "section": "",
    "text": "임베딩 레이어: 입력 데이터를 고정된 차원의 연속형 벡터로 변환합니다.\n위치 인코딩: 단어의 순서를 반영하기 위해 사인 및 코사인 함수를 이용한 위치 인코딩을 추가한다.\n최종 입력: 입력 임베딩과 위치 인코딩을 더한 값을 인코더에 전달\n\n\n\n\n\n인코더는 여러 개의 인코더 레이어로 이루어지며, 각 인코더 레이어는 다음을 포함한다.\n\n\n멀티-헤드 셀프 어텐션 (Multi-Head Self-Attention)\n\n입력 문장에서 각 단어가 문맥을 반영해 서로 다른 중요도를 가질 수 있도록 어텐션을 적용합니다.\n(입력 임베딩 + 위치 인코딩) -&gt; 쿼리(Q), 키(K), 밸류(V) 생성 -&gt; 어텐션 결과 출력\n\n잔차 연결 (Residual Connection) + Layer Normalization\n\n입력 값(입력 임베딩 + 위치 인코딩)과 어텐션 결과를 더한 후 정규화 적용 (이상하다고 생각 가능 -&gt; 밑에서 설명)\n\n피드포워드 네트워크 (Feedforward Network, FFN)\n\n비선형 변환을 수행하는 두 개의 완전연결층(FC)으로 구성된다.\n비선형 변환이란 ReLU와 같은 활성화 함수를 의미한다.\n비선형 변환을 이용하는 이유는 학습의 다양성과 안정성을 높이기 위해서이다. 선형 변환만으로는 곱셈과 덧셈으로만 나타내기에 함수의 형태가 단순한데 비선형 변환을 통해 더 복잡한 함수와 관계를 학습할 수 있다.\n\n잔차 연결 + Layer Normalization\n\nFFN의 출력을 다시 입력과 더한 후 정규화합니다. (2번과 비슷하게 입력 값을 다시 더해준다)\n그 후 정규화를 한다. 즉, LayerNorm(FFN Output + Attention Output)\n\n\n\n이러한 인코더 레이어를 여러 개 쌓아 깊은 표현 학습을 가능하게 합니다.\n\n- 이상한 부분의 보충설명\nQ. 어텐션 결과라는 것은 입력값을 멀티-헤드 셀프 어텐션을 거쳐서 나온 결과잖아요. 그런데 그 결과랑 입력 값을 또 더한다구요? 왜 또 더해요?\n\nA. 어텐션의 출력은 입력을 바탕으로 생성된 값인데, 다시 입력과 더하는 것이 이상하게 보일 수 있음. 그 이유는 잔차 연결(Residual Connection)**은 신경망이 깊어질수록 발생하는 학습 어려움을 해결하기 위해 도입된 기법입니다. 만약 어텐션 출력만 다음 레이어로 넘긴다면, 원래 입력 정보가 손실될 수 있음. 따라서, 원래 입력 정보를 유지하면서도, 어텐션이 학습한 새로운 정보(출력)를 추가적으로 반영하기 위해 입력 + 어텐션 출력을 더함.\n\n잔차 연결이 없다면? : 인코더/디코더가 깊어질수록 입력 정보가 왜곡될 가능성이 커진다. 그러므로 입력 정보(원래 값) + 어텐션 결과(새로운 정보)를 함께 유지하는 것이 학습 안정성 측면에서 유리함. 이것이 잔차 연결의 핵심 개념이다.\n\n\n\n\n디코더도 여러 개의 디코더 레이어로 구성되며, 인코더와 다른 점은 인코더-디코더 어텐션 레이어가 추가된다는 것입니다.\n각 디코더 레이어는 다음을 포함합니다.\n\n\n마스크드 멀티-헤드 셀프 어텐션 (Masked Multi-Head Self-Attention)\n\n디코더는 정답 데이터를 예측하는 과정에서 앞쪽 단어만 참고할 수 있도록 마스크를 적용하여 미래 정보를 차단합니다.\n미래 정보(미래에 나올 단어)를 미리 안다면 정답지를 보고 컨닝을 하는 것과 같기에 올바론 학습이 되 지 않는다.\n마스크드 멀티 헤드 어텐션도 인코더의 멀티 헤드 어텐션과 똑같이 Q,K,V 값 계산 -&gt; 어텐션 결과 출력\n\n잔차 연결 + Layer Normalization\n\n인코더 구조에서 설명한 것과 같음.\n\n인코더-디코더 어텐션 (Encoder-Decoder Attention)\n\n인코더에서 출력된 벡터를 사용하여 입력 문장과 현재 디코더 상태 간의 관계를 학습합니다.\n디코더만 사용하는 모델(GPT)는 이 층을 사용할 수 없다. 인코더에서 나온 출력을 이용하기에 인코더와 디코더가 모두 있는 모델에서만 이 층을 사용한다.\n\n잔차 연결 + Layer Normalization\n\n3에서 추가된 인코더-디코더 어텐션 층이 있기에 잔차 연결 + Layer Normalizaiton 층이 추가된다.\n\n피드포워드 네트워크 (FFN)\n잔차 연결 + Layer Normalization\n\n\n\n\n\n선형 변환 (Linear Layer): 디코더에서 나온 출력을 단어 집합 크기만큼의 차원으로 변환합니다.\n소프트맥스 (Softmax): 확률 분포를 구해 최종적으로 가장 확률이 높은 단어를 예측합니다.\n\n\n\n\n\n입력 데이터 처리 (임베딩 + 위치 인코딩)\n인코더 (여러 개의 인코더 레이어) -멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화 → FFN → 잔차 연결 + 정규화\n디코더 (여러 개의 디코더 레이어) -마스크드 멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화 -인코더-디코더 어텐션 → 잔차 연결 + 정규화 -FFN → 잔차 연결 + 정규화\n출력 변환 (선형 레이어 → 소프트맥스 → 예측)"
  },
  {
    "objectID": "posts/About_Transformers.html#입력-데이터-전처리",
    "href": "posts/About_Transformers.html#입력-데이터-전처리",
    "title": "About Transformers",
    "section": "",
    "text": "임베딩 레이어: 입력 데이터를 고정된 차원의 연속형 벡터로 변환합니다.\n위치 인코딩: 단어의 순서를 반영하기 위해 사인 및 코사인 함수를 이용한 위치 인코딩을 추가한다.\n최종 입력: 입력 임베딩과 위치 인코딩을 더한 값을 인코더에 전달"
  },
  {
    "objectID": "posts/About_Transformers.html#인코더-구조-여러-개의-인코더-레이어-스택",
    "href": "posts/About_Transformers.html#인코더-구조-여러-개의-인코더-레이어-스택",
    "title": "About Transformers",
    "section": "",
    "text": "인코더는 여러 개의 인코더 레이어로 이루어지며, 각 인코더 레이어는 다음을 포함한다.\n\n\n멀티-헤드 셀프 어텐션 (Multi-Head Self-Attention)\n\n입력 문장에서 각 단어가 문맥을 반영해 서로 다른 중요도를 가질 수 있도록 어텐션을 적용합니다.\n(입력 임베딩 + 위치 인코딩) -&gt; 쿼리(Q), 키(K), 밸류(V) 생성 -&gt; 어텐션 결과 출력\n\n잔차 연결 (Residual Connection) + Layer Normalization\n\n입력 값(입력 임베딩 + 위치 인코딩)과 어텐션 결과를 더한 후 정규화 적용 (이상하다고 생각 가능 -&gt; 밑에서 설명)\n\n피드포워드 네트워크 (Feedforward Network, FFN)\n\n비선형 변환을 수행하는 두 개의 완전연결층(FC)으로 구성된다.\n비선형 변환이란 ReLU와 같은 활성화 함수를 의미한다.\n비선형 변환을 이용하는 이유는 학습의 다양성과 안정성을 높이기 위해서이다. 선형 변환만으로는 곱셈과 덧셈으로만 나타내기에 함수의 형태가 단순한데 비선형 변환을 통해 더 복잡한 함수와 관계를 학습할 수 있다.\n\n잔차 연결 + Layer Normalization\n\nFFN의 출력을 다시 입력과 더한 후 정규화합니다. (2번과 비슷하게 입력 값을 다시 더해준다)\n그 후 정규화를 한다. 즉, LayerNorm(FFN Output + Attention Output)\n\n\n\n이러한 인코더 레이어를 여러 개 쌓아 깊은 표현 학습을 가능하게 합니다.\n\n- 이상한 부분의 보충설명\nQ. 어텐션 결과라는 것은 입력값을 멀티-헤드 셀프 어텐션을 거쳐서 나온 결과잖아요. 그런데 그 결과랑 입력 값을 또 더한다구요? 왜 또 더해요?\n\nA. 어텐션의 출력은 입력을 바탕으로 생성된 값인데, 다시 입력과 더하는 것이 이상하게 보일 수 있음. 그 이유는 잔차 연결(Residual Connection)**은 신경망이 깊어질수록 발생하는 학습 어려움을 해결하기 위해 도입된 기법입니다. 만약 어텐션 출력만 다음 레이어로 넘긴다면, 원래 입력 정보가 손실될 수 있음. 따라서, 원래 입력 정보를 유지하면서도, 어텐션이 학습한 새로운 정보(출력)를 추가적으로 반영하기 위해 입력 + 어텐션 출력을 더함.\n\n잔차 연결이 없다면? : 인코더/디코더가 깊어질수록 입력 정보가 왜곡될 가능성이 커진다. 그러므로 입력 정보(원래 값) + 어텐션 결과(새로운 정보)를 함께 유지하는 것이 학습 안정성 측면에서 유리함. 이것이 잔차 연결의 핵심 개념이다."
  },
  {
    "objectID": "posts/About_Transformers.html#디코더-구조-여러-개의-디코더-레이어-스택",
    "href": "posts/About_Transformers.html#디코더-구조-여러-개의-디코더-레이어-스택",
    "title": "About Transformers",
    "section": "",
    "text": "디코더도 여러 개의 디코더 레이어로 구성되며, 인코더와 다른 점은 인코더-디코더 어텐션 레이어가 추가된다는 것입니다.\n각 디코더 레이어는 다음을 포함합니다.\n\n\n마스크드 멀티-헤드 셀프 어텐션 (Masked Multi-Head Self-Attention)\n\n디코더는 정답 데이터를 예측하는 과정에서 앞쪽 단어만 참고할 수 있도록 마스크를 적용하여 미래 정보를 차단합니다.\n미래 정보(미래에 나올 단어)를 미리 안다면 정답지를 보고 컨닝을 하는 것과 같기에 올바론 학습이 되 지 않는다.\n마스크드 멀티 헤드 어텐션도 인코더의 멀티 헤드 어텐션과 똑같이 Q,K,V 값 계산 -&gt; 어텐션 결과 출력\n\n잔차 연결 + Layer Normalization\n\n인코더 구조에서 설명한 것과 같음.\n\n인코더-디코더 어텐션 (Encoder-Decoder Attention)\n\n인코더에서 출력된 벡터를 사용하여 입력 문장과 현재 디코더 상태 간의 관계를 학습합니다.\n디코더만 사용하는 모델(GPT)는 이 층을 사용할 수 없다. 인코더에서 나온 출력을 이용하기에 인코더와 디코더가 모두 있는 모델에서만 이 층을 사용한다.\n\n잔차 연결 + Layer Normalization\n\n3에서 추가된 인코더-디코더 어텐션 층이 있기에 잔차 연결 + Layer Normalizaiton 층이 추가된다.\n\n피드포워드 네트워크 (FFN)\n잔차 연결 + Layer Normalization"
  },
  {
    "objectID": "posts/About_Transformers.html#출력-처리",
    "href": "posts/About_Transformers.html#출력-처리",
    "title": "About Transformers",
    "section": "",
    "text": "선형 변환 (Linear Layer): 디코더에서 나온 출력을 단어 집합 크기만큼의 차원으로 변환합니다.\n소프트맥스 (Softmax): 확률 분포를 구해 최종적으로 가장 확률이 높은 단어를 예측합니다."
  },
  {
    "objectID": "posts/About_Transformers.html#요약-순서-정리",
    "href": "posts/About_Transformers.html#요약-순서-정리",
    "title": "About Transformers",
    "section": "",
    "text": "입력 데이터 처리 (임베딩 + 위치 인코딩)\n인코더 (여러 개의 인코더 레이어) -멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화 → FFN → 잔차 연결 + 정규화\n디코더 (여러 개의 디코더 레이어) -마스크드 멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화 -인코더-디코더 어텐션 → 잔차 연결 + 정규화 -FFN → 잔차 연결 + 정규화\n출력 변환 (선형 레이어 → 소프트맥스 → 예측)"
  },
  {
    "objectID": "posts/P_vs_P.html",
    "href": "posts/P_vs_P.html",
    "title": "Position Embedding vs Position Encoding",
    "section": "",
    "text": "1. 위치 임베딩 vs 위치 인코딩\n- 위치 임베딩\n위치별로 학습 가능한 임베딩을 부여하는 방식 (단어들의 위치에 따라 학습 가능한 값을 부여)\n\n모델이 학습을 통해 위치별 임베딩 값을 조정할 수 있다.\n예를 들어, 위치 0번, 1번, 2번, …에 대해 각각 학습 가능한 벡터가 존재한다.\n\n\\(PE_{learned}(pos)=Embedding(pos)\\)\n- 위치 인코딩\n위치 정보를 사인(sin)과 코사인(cos) 함수로 생성하는 방식이야. (단어들의 위치에 따라 학습 불가능한 상수 값을 부여\n\n사전 정의된 함수(수학적 패턴)를 사용하기 때문에 변화하지 않는 상수 값\n학습을 통해 조정되지 않고, 입력 데이터에 대해 고정된 값을 사용한다.\n\n\\(PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\\)\n\\(PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\\)\n\npos는 단어의 위치,\ni는 임베딩 차원의 인덱스\nd는 임베딩 차원 크기\n\n\n\n2. 각 방식의 장단점\nGood 위치 임베딩 장점 1. 학습 가능: 위치 임베딩은 학습이 가능하기에 모델이 데이터에 맞춰 위치 정보를 최적화할 수 있다. 2. 더 높은 유연성: 모델이 데이터를 통해 학습하므로, 특정 문맥이나 도메인에 맞춰 위치 표현을 다르게 할 수 있다.\nBad 위치 임베딩 단점 1. 메모리와 연산 비용: 위치 임베딩은 각 위치마다 임베딩 벡터를 학습해야 하기 때문에, 추가적인 파라미터와 메모리를 사용한다. 2. 오버피팅: 학습 가능한 위치 벡터가 많아지면, 모델이 훈련 데이터에 과적합할 가능성이 커질 수 있다.\nGood 위치 인코딩 장점 1. 메모리 효율성: 파라미터가 없기에 메모리 부담이 적다. 2. 고정된 패턴: 위치 인코딩은 고정된 수학적 패턴을 사용하므로, 특정 위치 간의 관계를 명확하게 정의할 수 있다.\nBad 위치 인코딩 단점 1. 학습할 수 없음: 모델이 학습을 통해 위치 정보를 더 정교하게 조정할 수 없다. 2. 유연성 부족: 모든 문장에서 같은 패턴을 사용하기에 특정 도메인이나 데이터 셋에 대해 최적화된 위치 정보를 표현할 수 없다.\n\n\n3. 결론\n\n위치 인코딩은 일반적인 용도와 효율성이 중요한 경우에 적합하다.\n위치 임베딩은 특정 도메인이나 고유한 데이터셋에 대해 더 정교한 위치 표현이 필요한 경우에 유리하다!"
  },
  {
    "objectID": "posts/Text_generation.html",
    "href": "posts/Text_generation.html",
    "title": "Text generation",
    "section": "",
    "text": "1. 그리디 서치 디코딩\n- 그리디 서치 디코딩의 이해\n처음 문장(\\(x=x_1,...,x_k\\)) 이 주어질 때 텍스트에 등장하는 토큰 시퀀스 (\\(y=y_1,...,y_t\\))의 확률 \\(P(y|x)\\)를 추정하도록 사전 훈련된다.\n하지만 직접 \\(P(y|x)\\)을 추정하려면 방대한 양의 훈련데이터가 필요하므로 연쇄법칙(Chain Rule of Probability) 을 사용해 조건부 확률의 곱으로 나타낸다.\n\\[\nP(y_1, ..., y_t | x) = \\prod_{t=1}^{N} P(y_t | y_{&lt;t}, x)\n\\]\n계산된 확률을 기반으로, 각 시점 t에서 가장 확률이 높은 단어를 선택하여 다음 단어를 예측한다.\n- 아래는 그리디 서치 디코딩의 구현이다.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n2025-03-17 17:53:47.780280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1742234027.798227   18290 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1742234027.803855   18290 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1742234027.817808   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1742234027.817823   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1742234027.817825   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1742234027.817827   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-03-17 17:53:47.822310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nimport pandas as pd\n\ninput_txt = \"Transformers are the\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\niterations = [] # 스텝별로 예측된 단어들을 저장할 리스트\nn_steps = 8 # 최대 8개의 단어를 추가 생성\nchoices_per_step = 5 # 각 스텝에서 가장 높은 확률을 가진 5개의 단어를 저장\n\nwith torch.no_grad(): # 학습이 아니라 예측을 수행하므로 gradient 계산을 비활성화\n    for _ in range(n_steps): \n        iteration = dict() \n        iteration[\"Input\"] = tokenizer.decode(input_ids[0]) # 딕셔너리 생성 후 현재까지의 문장을 저장\n        output = model(input_ids=input_ids) # 현재 문장을 모델에 입력하려 다음 단어의 확률을 얻음.\n        # 첫 번째 배치의 마지막 토큰의 로짓을 선택해 소프트맥스를 적용합니다.\n        next_token_logits = output.logits[0, -1, :]\n        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n        # 가장 높은 확률의 토큰을 저장합니다.\n        for choice_idx in range(choices_per_step):\n            token_id = sorted_ids[choice_idx]\n            token_prob = next_token_probs[token_id].cpu().numpy()\n            token_choice = (\n                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n            )\n            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n        # 예측한 다음 토큰을 입력에 추가합니다.\n        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n        iterations.append(iteration)\n\npd.DataFrame(iterations)\n\n\n\n\n\n\n\n\n\nInput\nChoice 1\nChoice 2\nChoice 3\nChoice 4\nChoice 5\n\n\n\n\n0\nTransformers are the\nmost (8.53%)\nonly (4.96%)\nbest (4.65%)\nTransformers (4.37%)\nultimate (2.16%)\n\n\n1\nTransformers are the most\npopular (16.78%)\npowerful (5.37%)\ncommon (4.96%)\nfamous (3.72%)\nsuccessful (3.20%)\n\n\n2\nTransformers are the most popular\ntoy (10.63%)\ntoys (7.23%)\nTransformers (6.60%)\nof (5.46%)\nand (3.76%)\n\n\n3\nTransformers are the most popular toy\nline (34.38%)\nin (18.20%)\nof (11.71%)\nbrand (6.10%)\nline (2.69%)\n\n\n4\nTransformers are the most popular toy line\nin (46.28%)\nof (15.09%)\n, (4.94%)\non (4.40%)\never (2.72%)\n\n\n5\nTransformers are the most popular toy line in\nthe (65.99%)\nhistory (12.42%)\nAmerica (6.91%)\nJapan (2.44%)\nNorth (1.40%)\n\n\n6\nTransformers are the most popular toy line in the\nworld (69.26%)\nUnited (4.55%)\nhistory (4.29%)\nUS (4.23%)\nU (2.30%)\n\n\n7\nTransformers are the most popular toy line in ...\n, (39.73%)\n. (30.64%)\nand (9.87%)\nwith (2.32%)\ntoday (1.74%)\n\n\n\n\n\n\n\n\n\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\nprint(tokenizer.decode(output[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nTransformers are the most popular toy line in the world,\n\n\n\nmax_length = 128\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput_greedy = model.generate(input_ids, max_length=max_length,\n                               do_sample=False)\nprint(tokenizer.decode(output_greedy[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n\n\nThe researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n\n\nThe researchers were surprised to find that the unicorns were able\n\n\n- 그리디 서치 디코딩은 각 타임스텝에서 확률이 가장 높은 토큰을 탐욕적(greedily)으로 선택하는 방식이다.\n하지만 이것은 사실 최적의 디코딩 방식이 아니다. 가장 최적의 방식은 가능한 모든 경우의 수로 문장을 완성시켜놓고 그 후에 그 문장을 판단하여 가장 좋은 문장을 선택하는 방식이다.\n\n하지만 그것은 너무 비용이 많이 드는 문제가 있어서 그리디 서치 디코딩 방식이 연구되었지만 이 방식 또한 반복적인 출력 시퀀스를 생성하는 경향이 있다. 이로 인해 최적의 솔루션을 만들기는 어렵다.\n\n\n2. 빔 서치 디코딩\n- 빔 서치는 각 스텝에서 확률이 가장 높은 토큰을 디코딩하는 대신, 확률이 가장 높은 상위 b개의 다음 토큰을 추적한다.\n빔 세트는 기존 세트에서 가능한 모든 다음 토큰을 확장한 후 확률이 가장 높은 b개의 확장을 선택하여 구성한다.\n\n이 과정은 최대 길이나 EOS토큰에 도달할 때까지 반복된다.\n- 로그확률을 이용하는 이유\n빔서치는 다음 단어의 확률을 계산할 때 기존의 곱으로 연결되던 확률이 아닌 로그를 취한 확률을 이용한다. 즉 로그확률을 이용한다.\n\n그 이유는 곱셈에서 사용되는 각 조건부 확률은 0과 1사이에 있는 작은 값이다. 이 값들은 문장의 길이가 조금만 길어지면 전체 확률이 0으로 가깝게 되는 underfolow가 쉽게 발생한다.\n\n아주 작은 값을 수치적으로 불안정하기에 확률에 log를 취해주면 곱셈이 덧셈으로 바뀌기에 식이 안정화된다. 이런 값은 다루기 훨씬 쉽다.\n\n추가적으로 log는 단조증가함수로서 확률크기를 비교만하면 되기에 log를 취해도 확률간에 대소관계는 달라지지 않기에 로그확률을 사용해도 상관없다.\n\n# 이 함수는 주어진 logits에서 특정 labels에 해당하는 로그 확률을 추출하는 함수이다.\n\nimport torch.nn.functional as F\n\ndef log_probs_from_logits(logits, labels):\n    logp = F.log_softmax(logits, dim=-1)\n    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n    # unsqueeze(2)를 이용해서 logp과 labels의 차원을 맞춰줌. 그 후에 squeeze를 이용해서 크기가 1인 차원을 없앤다.\n    return logp_label\n\n\n# 이 함수는 모델이 예측한 logits을 사용하여 전체 문장의 로그 확률을 계산하는 함수이다.\n\ndef sequence_logprob(model, labels, input_len=0):\n    with torch.no_grad():\n        output = model(labels)\n        log_probs = log_probs_from_logits(\n            output.logits[:, :-1, :], labels[:, 1:]) # labels[:,1:]는 정답, output.logits[:,:-1,:]은 마지막 단어를 제외한 모든 단어의 로짓\n        seq_log_prob = torch.sum(log_probs[:, input_len:]) #input_len만큼의 확률을 무시하고 더한다.\n    return seq_log_prob.cpu().numpy()\n\n1 그리디 서칭 디코딩으로 만든 시퀀스 로그확률 계산\n\nlogp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_greedy[0]))\nprint(f\"\\n로그 확률: {logp:.2f}\")\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a\nremote, previously unexplored valley, in the Andes Mountains. Even more\nsurprising to the researchers was the fact that the unicorns spoke perfect\nEnglish.\n\n\nThe researchers, from the University of California, Davis, and the University of\nColorado, Boulder, were conducting a study on the Andean cloud forest, which is\nhome to the rare species of cloud forest trees.\n\n\nThe researchers were surprised to find that the unicorns were able to\ncommunicate with each other, and even with humans.\n\n\nThe researchers were surprised to find that the unicorns were able\n\n로그 확률: -87.43\n\n\n2 빔 서치 디코딩으로 만든 시퀀스 로그확률 계산\n\noutput_beam = model.generate(input_ids, max_length=max_length, num_beams=5, # num_beams을 설정하면 빔 서치 디코딩이 활성화 된다. (가장 가능성이 높은 5문장을 동시탐색)\n                             do_sample=False) # do_sample=False는 샘플링을 하지 않고 결정론적 방식으로 단어를 선택(항상 같은 문장이 생성됨)\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_beam[0]))\nprint(f\"\\n로그 확률: {logp:.2f}\")\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a\nremote, previously unexplored valley, in the Andes Mountains. Even more\nsurprising to the researchers was the fact that the unicorns spoke perfect\nEnglish.\n\n\nThe discovery of the unicorns was made by a team of scientists from the\nUniversity of California, Santa Cruz, and the National Geographic Society.\n\n\nThe scientists were conducting a study of the Andes Mountains when they\ndiscovered a herd of unicorns living in a remote, previously unexplored valley,\nin the Andes Mountains. Even more surprising to the researchers was the fact\nthat the unicorns spoke perfect English\n\n로그 확률: -55.23\n\n\n- 빔 서치 디코딩 no_repeat_ngram_size 옵션 활성화\nno_repeat_ngram_size은 빔 서치도 텍스트가 반복되는 문제가 있기에 그 문제를 해결하기 위해 n-그램 페널티를 부과하는 옵션이다.\n\noutput_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n                             do_sample=False, no_repeat_ngram_size=2) # 생성된 문장에서 동일한 연속된 n개의 단어가 반복되지 않도록 제한하는 옵션.\n                             # 빔 서치도 텍스트가 반복되는 문제가 있기에 no_repeat_ngram_size을 설정한다.\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_beam[0]))\nprint(f\"\\n로그 확률: {logp:.2f}\")\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a\nremote, previously unexplored valley, in the Andes Mountains. Even more\nsurprising to the researchers was the fact that the unicorns spoke perfect\nEnglish.\n\n\nThe discovery was made by a team of scientists from the University of\nCalifornia, Santa Cruz, and the National Geographic Society.\n\nAccording to a press release, the scientists were conducting a survey of the\narea when they came across the herd. They were surprised to find that they were\nable to converse with the animals in English, even though they had never seen a\nunicorn in person before. The researchers were\n\n로그 확률: -93.12\n\n\n점수는 낮아졌지만 텍스트가 일관성을 유지하기에 결과는 좋다!\n! 참고로 당연히 각 시점에서의 로그확률은 음수이다 (0과 1사이의 값에 로그를 취하면 음수이기에…)\n하지만 1에 가까울 수록 절댓값은 더 작아지기에 시퀀스의 로그확률(모두 더한 값)은 0에 가까우면 가까울수록 좋은 것이다. (양수는 불가능)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Natural language processing",
    "section": "",
    "text": "Model fine tuning\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nBPE Tokenize\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nBase on Encoder-Decoder models\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nBase on Encoder models\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nBase on Decoder models\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nPosition Embedding vs Position Encoding\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Transformers\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nText generation\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nNLP Sampling\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nText Classification Fine Tuning\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluate\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2025\n\n\n차상진\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Evaluate.html",
    "href": "posts/Evaluate.html",
    "title": "Evaluate",
    "section": "",
    "text": "Evaluate 클래스는 accuracy, F1 score, precision, recall, BLEU, ROUGE 등등 다양한 평가 지표를 간단하게 불러와 활용할 수 있다.\n\nimport evaluate\nacc = evaluate.load('accuracy')\n\n\n\n\n계산을 해보는 예시코드이다.\n\nmetrics = evaluate.combine(['accuracy','f1','precision','recall'])\nmetrics.compute(predictions=[1,0,0,1], references = [0,1,0,1])\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n\nfor y,pred in zip([0,1,0,1],[1,0,0,1]):\n    metrics.add(predictions=pred, references=y)\nmetrics.compute()\n\n# .add 메소드는 입력받는 값들이 스칼라 값이어야 한다. 한 번에 하나의 예측값과 정답을 추가.\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n- zip()을 사용하여 references(정답)과 predictions(예측값)를 묶음\n\nfor y,preds in zip([[0,1],[0,1]],[[1,0],[0,1]]):\n    metrics.add_batch(predictions=preds, references=y)\nmetrics.compute()\n\n# .add_batch 메소드는 입력받는 값들이 리스트(배치단위)여야 한다. 한 번에 여러 개의 예측값과 정답을 추가\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}"
  },
  {
    "objectID": "posts/Evaluate.html#evaluate",
    "href": "posts/Evaluate.html#evaluate",
    "title": "Evaluate",
    "section": "",
    "text": "Evaluate 클래스는 accuracy, F1 score, precision, recall, BLEU, ROUGE 등등 다양한 평가 지표를 간단하게 불러와 활용할 수 있다.\n\nimport evaluate\nacc = evaluate.load('accuracy')\n\n\n\n\n계산을 해보는 예시코드이다.\n\nmetrics = evaluate.combine(['accuracy','f1','precision','recall'])\nmetrics.compute(predictions=[1,0,0,1], references = [0,1,0,1])\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n\nfor y,pred in zip([0,1,0,1],[1,0,0,1]):\n    metrics.add(predictions=pred, references=y)\nmetrics.compute()\n\n# .add 메소드는 입력받는 값들이 스칼라 값이어야 한다. 한 번에 하나의 예측값과 정답을 추가.\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n- zip()을 사용하여 references(정답)과 predictions(예측값)를 묶음\n\nfor y,preds in zip([[0,1],[0,1]],[[1,0],[0,1]]):\n    metrics.add_batch(predictions=preds, references=y)\nmetrics.compute()\n\n# .add_batch 메소드는 입력받는 값들이 리스트(배치단위)여야 한다. 한 번에 여러 개의 예측값과 정답을 추가\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}"
  },
  {
    "objectID": "posts/Evaluate.html#create-custom-metrics",
    "href": "posts/Evaluate.html#create-custom-metrics",
    "title": "Evaluate",
    "section": "Create custom metrics",
    "text": "Create custom metrics\n- 딕셔너리 형태로 반환되는 구조의 함수여야 Trainer 클래스의 매개변수인 compute_metrics에 입력하여 사용할 수 있다.\n\n# 정확도를 계산하는 간단한 함수\ndef simple_accuracy(preds,labels):\n    return {'accuracy': (preds == labels).to(float).mean().item()}"
  },
  {
    "objectID": "posts/Evaluate.html#trainer-적용",
    "href": "posts/Evaluate.html#trainer-적용",
    "title": "Evaluate",
    "section": "Trainer 적용",
    "text": "Trainer 적용\n\n# micro f1 score 사용\nimport evaluate\n\ndef custom_metrics(pred):\n    f1 = evaluate.load('f1')\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(axis = -1) # pred.predictions를 수행하면 logits값이 출력된다. 그 중 가장 큰 값을 가지는 인덱스를 반환하는 함수\n    return f1.compute(predictions = preds, references = labels, average = 'micro')\n          #.compute() 함수는 자동으로 결과를 딕셔너리형태로 출력한다.\n\n- .argmax(?)\naxis = -1을 쓰면 배열의 마지막 축에서 계산하는 것. axis=0은 열(세로), aixis= 1은 행(가로)이다.\n위에서 pred.predcitions을 했을 때 나오는 결과가 2차원이기에 마지막 축이란 행이 되므로 axis = -1과 axis = 1은 같은 코드이다.\n\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    default_data_collator\n)\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)\n\ndataset = load_dataset(\"klue\", \"ynat\")\n\ndef tokenize_function(sample):\n    result = tokenizer(\n        sample[\"title\"],\n        padding=\"max_length\",\n    )\n    return result\n\ndatasets = dataset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=1000,\n    remove_columns=[\"guid\", \"title\", \"url\", \"date\"]\n)\nprint(datasets)\n\nargs = TrainingArguments(\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=5e-5,\n    max_steps=500,\n    evaluation_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    logging_dir=\"/content/logs\",\n    save_strategy=\"steps\",\n    save_steps=50,\n    output_dir=\"/content/ckpt\",\n    report_to=\"tensorboard\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=datasets[\"train\"],\n    eval_dataset=datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=default_data_collator,\n    compute_metrics=custom_metrics, # 이 부분을 바꿔준다.\n)\n\nloading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.11.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\nloading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\nloading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\nloading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\nloading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.11.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.11.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nusing `logging_steps` to initialize `eval_steps` to 50\nPyTorch: setting up devices\nmax_steps is given, it will override any value given in num_train_epochs\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 45678\n    })\n    validation: Dataset({\n        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9107\n    })\n})\n\n\n\ntrainer.train()\n\n***** Running training *****\n  Num examples = 45678\n  Num Epochs = 1\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 500\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-50\nConfiguration saved in /content/ckpt/checkpoint-50/config.json\nModel weights saved in /content/ckpt/checkpoint-50/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-50/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-50/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-100\nConfiguration saved in /content/ckpt/checkpoint-100/config.json\nModel weights saved in /content/ckpt/checkpoint-100/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-100/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-100/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-150\nConfiguration saved in /content/ckpt/checkpoint-150/config.json\nModel weights saved in /content/ckpt/checkpoint-150/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-150/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-150/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-200\nConfiguration saved in /content/ckpt/checkpoint-200/config.json\nModel weights saved in /content/ckpt/checkpoint-200/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-200/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-200/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-250\nConfiguration saved in /content/ckpt/checkpoint-250/config.json\nModel weights saved in /content/ckpt/checkpoint-250/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-250/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-250/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-300\nConfiguration saved in /content/ckpt/checkpoint-300/config.json\nModel weights saved in /content/ckpt/checkpoint-300/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-300/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-300/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-350\nConfiguration saved in /content/ckpt/checkpoint-350/config.json\nModel weights saved in /content/ckpt/checkpoint-350/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-350/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-350/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-400\nConfiguration saved in /content/ckpt/checkpoint-400/config.json\nModel weights saved in /content/ckpt/checkpoint-400/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-400/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-400/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-450\nConfiguration saved in /content/ckpt/checkpoint-450/config.json\nModel weights saved in /content/ckpt/checkpoint-450/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-450/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-450/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-500\nConfiguration saved in /content/ckpt/checkpoint-500/config.json\nModel weights saved in /content/ckpt/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-500/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\n\n\n    \n      \n      \n      [500/500 18:44, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\nF1\n\n\n\n\n50\n1.082100\n0.825163\n0.702427\n\n\n100\n0.571100\n0.593032\n0.810695\n\n\n150\n0.476000\n0.570962\n0.816844\n\n\n200\n0.500600\n0.535641\n0.815966\n\n\n250\n0.454800\n0.501376\n0.833535\n\n\n300\n0.433800\n0.479584\n0.837158\n\n\n350\n0.397700\n0.483717\n0.842868\n\n\n400\n0.442900\n0.449807\n0.851104\n\n\n450\n0.420800\n0.434349\n0.853300\n\n\n500\n0.406100\n0.438009\n0.853080\n\n\n\n\n\n\n\nTrainOutput(global_step=500, training_loss=0.5185918083190918, metrics={'train_runtime': 1125.3103, 'train_samples_per_second': 7.109, 'train_steps_per_second': 0.444, 'total_flos': 2104982937600000.0, 'train_loss': 0.5185918083190918, 'epoch': 0.18})\n\n\n- 결과에서 이전에는 볼 수 없었던 F1 score이 보인다."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html",
    "href": "posts/Base_on_Enco_Deco.html",
    "title": "Base on Encoder-Decoder models",
    "section": "",
    "text": "- 인코더-디코더 기반 모델은 완성된 문장을 이어받아 입력과는 완전히 다른 새로운 문장을 생성하는 것을 목적으로 한다.\n- 디코더 기반 모델의 자연어 생성과 비슷하지만 입력된 문장을 이어 나가는 디코더 기반 모델과는 달리 완전히 새로운 문장을 작성한다는 차이가 있다.\n일반적으로 기계 번역이나 요약에 사용된다.\n\n\n-BART의 학습 방법\n\n\nBERT에서 사용헸던 일반적인 Masked LM과 동일하다.\n\n\n\n랜덤한 토큰을 삭제하고 이를 복구한다. 마스킹 방법은 특정 토큰을 [MASK]로 변경하기에 어떤 위치의 토큰이 사라졌는지 알지만 토큰 삭제는 어떤 위치의 토큰이 사라졌는지 알 수 없다.\n\n\n\n입력 문장 중, 연속되는 토큰 몇 개를 묶어 토큰 뭉치를 생성하여 그 범위를 [MASK] 토큰으로 치환한다. 이때, 토큰 뭉치 길이는 포아송 분포를 따르며 길이가 0 or 2이상이다. 길이가 0인 경우 정상 문장에서 [MASK] 토큰만 생성되고 2 이상인 경우 여러 토큰이 하나의 [MASK] 토큰으로 바뀌게 된다. 따라서 모델이 범위에서 누락된 토큰 수에 대해서도 학습할 수 있도록 한다\n3. 텍스트 채우기 보충 설명\n- 왜 포아송 분포(Poisson Distribution)을 따르는가?\n먼저 포아송 분포에 대한 이해를 해보자.\n포아송 분포: 시간이 지남에 따라 일어나는 특정한 사건 A의 발생횟수의 분포\n즉 포아송 분포는 평균 발생 횟수를 기반으로 사건이 발생할 횟수가 결정된다.\nBART는 입력 문장에서 일부 단어나 토큰을 무작위로 마스킹 처리하는데 정말 무작위로 선택하여 Masking 하는 것이 아니고 포아송 분포를 따르면서 Masking 할 토큰을 찾는데 그 이유는 Masking 할 토큰의 수가 예측 불가능하고, 일정한 평균 빈도수로 선택되도록 하기 위해 사용된다.\n위에서 언급했듯이 포아송 분포는 평균적으로 몇 개의 토큰이 Masking될지 예측할 수 있지만, 실제로 Masking 될 토큰의 수는 확률적으로 결정된다.\n\n길이가 0인 경우: 정상 문장에서 [MASK] 토큰만 생성됨\n길이가 2인 경우: 여러 개의 연속된 토큰이 하나의 [MASK] 토큰으로 치환된다. 이 때!! 마스킹된 범위의 길이는 포아송 분포에 따라 결정된다. (평균이 몇인지에 따라 다르겠지만 대부분 확률은 매우 낮음)\n\n포아송 분포는 모델이 일부 연속적인 토큰들을 마스킹하면서도 문맥 정보를 이해하고, 누락된 토큰을 예측하는 데 도움이 되도록 설계되었다.\n\n\n\n입력 문서를 문장 단위로 분할하고 문장의 순서를 무작위로 섞는다.\n\n\n\n입력 문장 중, 토큰 하나를 무작위로 정해 해당 토큰이 문장의 시작이 되도록 해당 문장 토큰을 밀어낸다. 시작 토큰 앞에 있던 토큰은 맨 뒤로 이동한다."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#bart",
    "href": "posts/Base_on_Enco_Deco.html#bart",
    "title": "Base on Encoder-Decoder models",
    "section": "",
    "text": "-BART의 학습 방법\n\n\nBERT에서 사용헸던 일반적인 Masked LM과 동일하다.\n\n\n\n랜덤한 토큰을 삭제하고 이를 복구한다. 마스킹 방법은 특정 토큰을 [MASK]로 변경하기에 어떤 위치의 토큰이 사라졌는지 알지만 토큰 삭제는 어떤 위치의 토큰이 사라졌는지 알 수 없다.\n\n\n\n입력 문장 중, 연속되는 토큰 몇 개를 묶어 토큰 뭉치를 생성하여 그 범위를 [MASK] 토큰으로 치환한다. 이때, 토큰 뭉치 길이는 포아송 분포를 따르며 길이가 0 or 2이상이다. 길이가 0인 경우 정상 문장에서 [MASK] 토큰만 생성되고 2 이상인 경우 여러 토큰이 하나의 [MASK] 토큰으로 바뀌게 된다. 따라서 모델이 범위에서 누락된 토큰 수에 대해서도 학습할 수 있도록 한다\n3. 텍스트 채우기 보충 설명\n- 왜 포아송 분포(Poisson Distribution)을 따르는가?\n먼저 포아송 분포에 대한 이해를 해보자.\n포아송 분포: 시간이 지남에 따라 일어나는 특정한 사건 A의 발생횟수의 분포\n즉 포아송 분포는 평균 발생 횟수를 기반으로 사건이 발생할 횟수가 결정된다.\nBART는 입력 문장에서 일부 단어나 토큰을 무작위로 마스킹 처리하는데 정말 무작위로 선택하여 Masking 하는 것이 아니고 포아송 분포를 따르면서 Masking 할 토큰을 찾는데 그 이유는 Masking 할 토큰의 수가 예측 불가능하고, 일정한 평균 빈도수로 선택되도록 하기 위해 사용된다.\n위에서 언급했듯이 포아송 분포는 평균적으로 몇 개의 토큰이 Masking될지 예측할 수 있지만, 실제로 Masking 될 토큰의 수는 확률적으로 결정된다.\n\n길이가 0인 경우: 정상 문장에서 [MASK] 토큰만 생성됨\n길이가 2인 경우: 여러 개의 연속된 토큰이 하나의 [MASK] 토큰으로 치환된다. 이 때!! 마스킹된 범위의 길이는 포아송 분포에 따라 결정된다. (평균이 몇인지에 따라 다르겠지만 대부분 확률은 매우 낮음)\n\n포아송 분포는 모델이 일부 연속적인 토큰들을 마스킹하면서도 문맥 정보를 이해하고, 누락된 토큰을 예측하는 데 도움이 되도록 설계되었다.\n\n\n\n입력 문서를 문장 단위로 분할하고 문장의 순서를 무작위로 섞는다.\n\n\n\n입력 문장 중, 토큰 하나를 무작위로 정해 해당 토큰이 문장의 시작이 되도록 해당 문장 토큰을 밀어낸다. 시작 토큰 앞에 있던 토큰은 맨 뒤로 이동한다."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#model",
    "href": "posts/Base_on_Enco_Deco.html#model",
    "title": "Base on Encoder-Decoder models",
    "section": "2-1. model",
    "text": "2-1. model\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"hyunwoongko/kobart\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n2025-04-01 07:39:17.432468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1743493157.447473   60530 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1743493157.452654   60530 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1743493157.466293   60530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743493157.466309   60530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743493157.466311   60530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743493157.466313   60530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-04-01 07:39:17.470692: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\n\nBartForConditionalGeneration(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n)"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#dataset",
    "href": "posts/Base_on_Enco_Deco.html#dataset",
    "title": "Base on Encoder-Decoder models",
    "section": "2-2. Dataset",
    "text": "2-2. Dataset\n유의점\n인코더-디코더 모델은 인코더에 들어가는 입력과 디코더에 들어가는 입력, 총 두 개 입력이 필요하고 이에 대한 정답이 따로 필요하다.\n필수로 인코더 입력, 디코더 입력, 디코더 정답 이렇게 세 가지 데이터 특성이 포함되어야 한다. - 왜 인코더 정답은 필요 없지..?\n우선 인코더 입력과 출력의 정답은 달라야하므로 정답을 text_target 파라미터로 입력해 정답 값까지 한 번에 들어야 한다.\n- 인코더 정답이 필요없는 이유\n결론부터 말하면 정답이 필요한 곳은 디코더 뿐이다! - 인코더는 입력을 벡터로 변환하는 역할만 하기에 정답이 따로 필요없다. - 하지만 디코더는 출력을 생성하므로 정답(labels)이 필요하다.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"msarmi9/korean-english-multitarget-ted-talks-task\")\nprint(dataset)\ndataset['train'][0]\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['korean', 'english'],\n        num_rows: 166215\n    })\n    validation: Dataset({\n        features: ['korean', 'english'],\n        num_rows: 1958\n    })\n    test: Dataset({\n        features: ['korean', 'english'],\n        num_rows: 1982\n    })\n})\n\n\n{'korean': '(박수) 이쪽은 Bill Lange 이고, 저는 David Gallo입니다',\n 'english': \"(Applause) David Gallo: This is Bill Lange. I'm Dave Gallo.\"}\n\n\n\ntokenized_dataset = dataset.map(\n    lambda batch: (\n        tokenizer(\n            batch[\"korean\"],\n            text_target=batch[\"english\"],\n            max_length=512,\n            truncation=True,\n        )\n    ),\n    batched=True,\n    batch_size=1000,\n    num_proc=2,\n    remove_columns=dataset[\"train\"].column_names,\n)\ntokenized_dataset[\"train\"][0]\n\n\n\n\n\n\n\n\n\n\n{'input_ids': [0,\n  14338,\n  10770,\n  11372,\n  240,\n  14025,\n  12471,\n  12005,\n  15085,\n  29490,\n  14676,\n  24508,\n  300,\n  14025,\n  14161,\n  16530,\n  15529,\n  296,\n  317,\n  18509,\n  15464,\n  15585,\n  20858,\n  12049,\n  20211,\n  1],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1],\n 'labels': [0,\n  14338,\n  264,\n  311,\n  311,\n  17422,\n  316,\n  17223,\n  240,\n  15529,\n  296,\n  317,\n  18509,\n  15464,\n  15585,\n  20858,\n  257,\n  15054,\n  303,\n  15868,\n  1700,\n  15868,\n  15085,\n  29490,\n  14676,\n  24508,\n  300,\n  245,\n  14943,\n  238,\n  308,\n  15529,\n  296,\n  21518,\n  15464,\n  15585,\n  20858,\n  245,\n  1]}\n\n\n\ntokenized_dataset\n\n# 데이터를 살펴보면 input_ids : 인코더 입력, labels : 디코더 정답은 존재한다.\n# 디코더 입력인 decoder_input_ids가 없기에 모델에 데이터를 입력하면 오류가 발생한다.\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 166215\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1958\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1982\n    })\n})"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#collator",
    "href": "posts/Base_on_Enco_Deco.html#collator",
    "title": "Base on Encoder-Decoder models",
    "section": "2-3. Collator",
    "text": "2-3. Collator\n디코더 입력값은 결국 정답 값인 labels을 앞으로 한 칸 이동한 데이터이다.\n해당 작업을 패딩과 더불어 간편하게 처리할 수 있도록 DataCollatorForSeq2Seq를 사용한다.\n패딩 작업과 함께 디코더에 입력으로 들어갈 부분까지 자동으로 설정하여 반환한다.\n콜레이터는 batch 데이터를 준비하는데 사용되는 함수이다.\n\nfrom transformers import DataCollatorForSeq2Seq\n\ncollator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"max_length\",\n    max_length=512,\n)\nbatch = collator([tokenized_dataset[\"train\"][i] for i in range(2)]) # 문장 2개만 뽑아서 처리\nbatch\n\n{'input_ids': tensor([[    0, 14338, 10770,  ...,     3,     3,     3],\n        [    0, 15496, 18918,  ...,     3,     3,     3]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    0, 14338,   264,  ...,  -100,  -100,  -100],\n        [    0, 14603,   309,  ...,  -100,  -100,  -100]]), 'decoder_input_ids': tensor([[    1,     0, 14338,  ...,     3,     3,     3],\n        [    1,     0, 14603,  ...,     3,     3,     3]])}"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#generation",
    "href": "posts/Base_on_Enco_Deco.html#generation",
    "title": "Base on Encoder-Decoder models",
    "section": "2-4. Generation",
    "text": "2-4. Generation\n\nimport torch\n\nwith torch.no_grad():\n    logits = model(**batch).logits\n\nlogits\n\ntensor([[[  5.4885,  18.7849,  -0.5489,  ...,   0.0465,   0.5813,  -2.2851],\n         [  3.7287,  18.9676,  -1.1747,  ...,  -0.2600,  -3.4647,  -0.0973],\n         [ -1.2976,   8.6322,  -5.0410,  ...,  -7.0689,  -6.1346,  -4.4141],\n         ...,\n         [ -9.2638,   4.4483,  -8.4506,  ..., -12.6961, -13.2625,  -7.7570],\n         [ -8.4581,   4.9268,  -7.2172,  ..., -11.5650, -11.8799,  -6.8108],\n         [ -8.3191,   5.2101,  -6.8817,  ..., -11.1563, -11.7052,  -6.7644]],\n\n        [[  4.7748,  16.2666,  -3.0011,  ...,  -0.8965,  -3.3187,  -3.1041],\n         [  0.6535,  19.3665,  -1.4506,  ...,   0.1562,  -4.3976,   0.1983],\n         [ -5.0934,  10.8673,  -7.5637,  ...,  -6.3808,  -1.6471,  -7.2105],\n         ...,\n         [ -1.5132,  19.0760,   0.3272,  ...,  -2.6680,  -3.9969,   2.7315],\n         [ -2.3757,  20.0047,  -0.5301,  ...,  -1.7740,  -5.1750,   0.8077],\n         [ -2.2504,  19.9756,  -0.4519,  ...,  -0.6850,  -5.1072,   0.4720]]])\n\n\n\nlogits.shape\n\ntorch.Size([2, 512, 30000])\n\n\n512의 단어 길이를 가지는 2개의 문장에서 30000개의 단어들이 나올 확률을 계산한 것.\n\nfrom transformers import GenerationConfig\n\ngen_cfg = GenerationConfig(\n    max_new_tokens=100,\n    do_sample=True,\n    temperature=1.2,\n    top_k=50,\n    top_p=0.95,\n)\noutputs = model.generate(batch[\"input_ids\"], generation_config=gen_cfg)\nresult = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(result[0])\n\n\n\n\n- 문장이 제대로 생성되지 않은 이유\nmodel.config.eos_token_id = 1로 설정되어 있어서, 모델이 처음 생성하는 토큰이 1번 토큰이면 곧바로 종료된다.\n\nmodel.config.eos_token_id\n\n1"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#evaluate",
    "href": "posts/Base_on_Enco_Deco.html#evaluate",
    "title": "Base on Encoder-Decoder models",
    "section": "2-5. Evaluate",
    "text": "2-5. Evaluate\n문장 생성 태스크는 학습을 진행하며 평가 지표를 확인하기 어렵다.\n따라서 학습 중에 일반적으로 크로스 엔트로피 손실을 사용하여 값이 감소 추이를 살피며 모델 학습이 원활하게 이뤄지는지 확인한다."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#model-1",
    "href": "posts/Base_on_Enco_Deco.html#model-1",
    "title": "Base on Encoder-Decoder models",
    "section": "3-1. model",
    "text": "3-1. model\n이번 실습에서는 인코더와 디코더에 동일한 문장을 입력하여서 문장 분류를 진행하려고 한다.\n문장 구조가 바뀌지 않기에 이전 인코더, 디코더 기반 모델에서 실습했던 것과 같이 동일한 코드로 추론한다.\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"hyunwoongko/kobart\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel\n\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\nSome weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBartForSequenceClassification(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (classification_head): BartClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#dataset-1",
    "href": "posts/Base_on_Enco_Deco.html#dataset-1",
    "title": "Base on Encoder-Decoder models",
    "section": "3-2. Dataset",
    "text": "3-2. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"sts\")\n\ndef process_data(batch):\n  result = tokenizer(batch[\"sentence1\"], text_pair=batch[\"sentence2\"])\n  result[\"labels\"] = [x[\"binary-label\"] for x in batch[\"labels\"]]\n  return result\n\ntokenized_dataset = dataset.map(\n    process_data,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\n\n\n\n\n\n\n\n\n\n\ntokenized_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['labels', 'input_ids', 'attention_mask'],\n        num_rows: 11668\n    })\n    validation: Dataset({\n        features: ['labels', 'input_ids', 'attention_mask'],\n        num_rows: 519\n    })\n})\n\n\ndataset은 인코더에 입력으로 들어가는 input_ids만 포함하고 있지만 큰 문제는 없다.\ndecoder_input_ids가 입력되지 않았을 때, 인코더 입력인 input_ids를 오른쪽으로 한 칸 이동하여 디코더 입력으로 자동으로 사용한다."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#collator-1",
    "href": "posts/Base_on_Enco_Deco.html#collator-1",
    "title": "Base on Encoder-Decoder models",
    "section": "3-3. Collator",
    "text": "3-3. Collator\n위에서도 설명했지만 콜레이터는 모델이 해당 데이터셋을 바로 사용하도록 batch 작업을 해준다.\n\nimport torch\nfrom transformers import DataCollatorWithPadding\n\ncollator = DataCollatorWithPadding(tokenizer)\nbatch = collator([tokenized_dataset['train'][i] for i in range(4)])"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#generation-1",
    "href": "posts/Base_on_Enco_Deco.html#generation-1",
    "title": "Base on Encoder-Decoder models",
    "section": "3-4. Generation",
    "text": "3-4. Generation\n\nwith torch.no_grad():\n    logits = model(**batch).logits\n\nlogits\n\ntensor([[ 0.0255, -0.1499],\n        [ 0.4134, -0.2986],\n        [-0.0575,  0.0541],\n        [ 0.1218, -0.8607]])"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#evaluate-1",
    "href": "posts/Base_on_Enco_Deco.html#evaluate-1",
    "title": "Base on Encoder-Decoder models",
    "section": "3-5. Evaluate",
    "text": "3-5. Evaluate\n\nimport evaluate\n\nf1 = evaluate.load('f1')\nf1.compute(\n    predictions = logits.argmax(axis = -1),\n    references = batch['labels'],\n    average = 'micro'\n)\n\n\n\n\n{'f1': 0.5}\n\n\n- 생성 task가 아니라 분류이므로 평가 가능"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#model-2",
    "href": "posts/Base_on_Enco_Deco.html#model-2",
    "title": "Base on Encoder-Decoder models",
    "section": "4-1. model",
    "text": "4-1. model\n\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nmodel_name = \"hyunwoongko/kobart\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\nmodel\n\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\nSome weights of BartForQuestionAnswering were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBartForQuestionAnswering(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)\n\n\n- out_features=2\n두 값만 출력하면 되기에 out_features = 2이다."
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#dataset-2",
    "href": "posts/Base_on_Enco_Deco.html#dataset-2",
    "title": "Base on Encoder-Decoder models",
    "section": "4-1. Dataset",
    "text": "4-1. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"mrc\") # klue: 데이터셋 mrc: 기계독해 데이터\n\ndef preprocess_function(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=512,\n        truncation=\"only_second\", # 문맥이 길면 문맥만 잘라냄 (질문과 문맥에서 문맥이 길기때문에)\n        return_offsets_mapping=True, # 원본 텍스트에서 각 토큰의 위치 정보 저장 (추출해야 하기에)\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    answers = examples[\"answers\"] # 실제 정답 정보 answer_start , text\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        answer = answers[i]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n        # start, end 위치를 찾음\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n          idx += 1\n        context_start = idx # 문맥이 시작하는 위치\n        while sequence_ids[idx] == 1:\n          idx += 1 \n        context_end = idx - 1 # 문맥이 끝나는 위치\n\n        # If the answer is not fully inside the context, label it (0, 0)\n        if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char:\n            start_positions.append(0) # 정답이 문맥 밖에 있으면 start,end 위치를 0으로 설정\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx &lt;= context_end and offset[idx][0] &lt;= start_char:\n                idx += 1\n            start_positions.append(idx - 1) # 정답의 시작 위치를 저장\n\n            idx = context_end\n            while idx &gt;= context_start and offset[idx][1] &gt;= end_char:\n                idx -= 1\n            end_positions.append(idx + 1) # 정답의 끝 위치를 저장\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\ntokenized_dataset = dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#collator-2",
    "href": "posts/Base_on_Enco_Deco.html#collator-2",
    "title": "Base on Encoder-Decoder models",
    "section": "4-2. Collator",
    "text": "4-2. Collator\n\nfrom transformers import DefaultDataCollator\n\ndata_collator = DefaultDataCollator()\nbatch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])\nbatch\n\n{'input_ids': tensor([[    0, 14337, 26225,  ...,     3,     3,     3],\n         [    0, 25092, 18001,  ..., 11270, 19903,     1],\n         [    0, 25788, 13679,  ..., 19903, 15599,     1],\n         ...,\n         [    0, 20437, 17814,  ...,     3,     3,     3],\n         [    0, 14154, 12061,  ...,     3,     3,     3],\n         [    0, 14295, 14120,  ...,     3,     3,     3]]),\n 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0]]),\n 'start_positions': tensor([233,  27,   0,  78,  60,  68, 202, 319, 306, 271]),\n 'end_positions': tensor([235,  29,   0,  79,  66,  74, 210, 325, 312, 275])}"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#generation-2",
    "href": "posts/Base_on_Enco_Deco.html#generation-2",
    "title": "Base on Encoder-Decoder models",
    "section": "4-3. Generation",
    "text": "4-3. Generation\n\nimport torch\n\nwith torch.no_grad():\n  outputs = model(**batch)\n\nanswer_start_index = outputs.start_logits.argmax()\nanswer_end_index = outputs.end_logits.argmax()\n\npredict_answer_tokens = batch[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n\n''"
  },
  {
    "objectID": "posts/Base_on_Enco_Deco.html#evaluate-2",
    "href": "posts/Base_on_Enco_Deco.html#evaluate-2",
    "title": "Base on Encoder-Decoder models",
    "section": "4-4. Evaluate",
    "text": "4-4. Evaluate\nQA 평가지표는 evaluate.load('squad')를 통해 진행할 수 있었다. 하지만 상당한 양의 후처리가 필요하고 시간이 오래 걸리는 작업이다."
  },
  {
    "objectID": "posts/Based_on_Decoder.html",
    "href": "posts/Based_on_Decoder.html",
    "title": "Base on Decoder models",
    "section": "",
    "text": "디코더 기반 모델은 문장 앞부분 일부만을 입력받아 이를 이어서 작성하는 형태이며 이를 자연어 생성이라고도 말한다.\n\n\n디코더 기반 모델은 셀프 어텐션, 인코더 참조 어텐션, FFNN으로 이루어진 기존의 트랜스포머 디코더에서 인코더 참조 부분을 제거하여 두 단계로 구성된다.\n이를 여러 개 레이어로 층층이 쌓은 형태의 모델이 디코더 기반 모델이다.\n인코더 기반 모델의 주요 task는 완성된 문장을 분석하는 것이기에 이미 완성된 문장이 input으로 입력된다.\n하지만 디코더 기반 모델의 주요 task는 미완성의 문장을 이어서 작성하는 생성태스크이다.\n미완성 상태로 작성중인 문장을 실시간으로 확인하며 직접 이어 나가야 하기에 문장 일부만으로 자연스럽게 다음 단어를 예측하는 단방향 형태로 분석한다.\n- 단방향 형태?\n디코더 기반 모델은 뒤쪽 단어가 앞쪽 단어에 영향을 미칠 수 없다…더 깊은 이해를 위해 BERT와 GPT를 비교해보자.\n\n\n\n\n\nBERT는 Transformer의 인코더 블록을 기반으로 만들어진 모델이다.\n핵심 특징은 양방향(Bidirectional)으로 문맥을 고려한다는 점이다. 즉, 모든 단어가 앞쪽 + 뒤쪽의 단어를 동시에 참고할 수 있음.\n예를 들어, 문장이 \"The cat sat on the mat\"라면, \"cat\"은 \"The\"도 보고 \"sat\"도 보면서 문맥을 이해할 수 있음(???). 즉, 첫 번째 단어도 마지막 단어에 영향을 미칠 수 있음\nBERT는 문장의 모든 단어를 한 번에 입력받아 전체 문맥을 학습하기에 첫 번째 단어의 벡터가 마지막 단어 벡터에까지 영향을 미칠 수 있다.\n즉, BERT는 문장의 모든 단어가 서로 영향을 주고받을 수 있는 구조다.\n- \"cat\"은 \"The\"도 보고 \"sat\"도 보면서 문맥을 이해할 수 있음 (???)\nQ. cat이 \"The\"도 보고 \"sat\"도 보면서 문맥을 이해한다고? 그냥 하나의 단어 아닌가? A. cat만 본다면 당연히 고양이라고 생각하지만 어떤 문장에서는 다른 의미 혹은 이름등 함축하고 있는 내용은 고양이와 다를 수 있다. 여기서 self-attention 메커니즘을 적용하면 sat, the, on 등을 보면서 cat의 내포 의미를 더 확실하게 파악할 수 있다. 그래서 cat이 다른 단어를 보며 문맥을 이해한다고 표현한 것이다.\n\n\n\nGPT는 Transformer의 디코더(Decoder) 블록을 기반으로 만들어진 모델이다.\n핵심 특징은 단방향(Unidirectional)으로 문맥을 고려한다는 점이다. Self-Attention을 단방향으로 수행하므로 현재 단어보다 앞쪽(이전)에 있는 단어들만 참고할 수 있다.\n예를 들어, \"The cat sat on the mat\"에서 \"mat\"을 예측할 때 \"The cat sat on the\"까지만 보고 \"mat\"을 결정해야 함.\n반대로 \"mat\"이 \"sat\"에 영향을 미칠 수는 없음. 즉, 미래 정보는 볼 수 없음\nGPT는 생성 모델이기 때문에, 문장을 왼쪽에서 오른쪽으로 한 단어씩 생성해야 한다. 만약 오른쪽(뒤쪽)의 단어를 안다면 이미 정답을 알고 있기에 cheating으로 간주된다.\n따라서 첫 번째 단어는 마지막 단어를 전혀 모름.\n즉, GPT는 앞쪽 단어들이 뒤쪽 단어에 영향을 줄 수 있지만, 반대로 뒤쪽 단어가 앞쪽 단어에 영향을 미칠 수 없음!"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#기본-구조",
    "href": "posts/Based_on_Decoder.html#기본-구조",
    "title": "Base on Decoder models",
    "section": "",
    "text": "디코더 기반 모델은 셀프 어텐션, 인코더 참조 어텐션, FFNN으로 이루어진 기존의 트랜스포머 디코더에서 인코더 참조 부분을 제거하여 두 단계로 구성된다.\n이를 여러 개 레이어로 층층이 쌓은 형태의 모델이 디코더 기반 모델이다.\n인코더 기반 모델의 주요 task는 완성된 문장을 분석하는 것이기에 이미 완성된 문장이 input으로 입력된다.\n하지만 디코더 기반 모델의 주요 task는 미완성의 문장을 이어서 작성하는 생성태스크이다.\n미완성 상태로 작성중인 문장을 실시간으로 확인하며 직접 이어 나가야 하기에 문장 일부만으로 자연스럽게 다음 단어를 예측하는 단방향 형태로 분석한다.\n- 단방향 형태?\n디코더 기반 모델은 뒤쪽 단어가 앞쪽 단어에 영향을 미칠 수 없다…더 깊은 이해를 위해 BERT와 GPT를 비교해보자."
  },
  {
    "objectID": "posts/Based_on_Decoder.html#bert-vs-gpt",
    "href": "posts/Based_on_Decoder.html#bert-vs-gpt",
    "title": "Base on Decoder models",
    "section": "",
    "text": "BERT는 Transformer의 인코더 블록을 기반으로 만들어진 모델이다.\n핵심 특징은 양방향(Bidirectional)으로 문맥을 고려한다는 점이다. 즉, 모든 단어가 앞쪽 + 뒤쪽의 단어를 동시에 참고할 수 있음.\n예를 들어, 문장이 \"The cat sat on the mat\"라면, \"cat\"은 \"The\"도 보고 \"sat\"도 보면서 문맥을 이해할 수 있음(???). 즉, 첫 번째 단어도 마지막 단어에 영향을 미칠 수 있음\nBERT는 문장의 모든 단어를 한 번에 입력받아 전체 문맥을 학습하기에 첫 번째 단어의 벡터가 마지막 단어 벡터에까지 영향을 미칠 수 있다.\n즉, BERT는 문장의 모든 단어가 서로 영향을 주고받을 수 있는 구조다.\n- \"cat\"은 \"The\"도 보고 \"sat\"도 보면서 문맥을 이해할 수 있음 (???)\nQ. cat이 \"The\"도 보고 \"sat\"도 보면서 문맥을 이해한다고? 그냥 하나의 단어 아닌가? A. cat만 본다면 당연히 고양이라고 생각하지만 어떤 문장에서는 다른 의미 혹은 이름등 함축하고 있는 내용은 고양이와 다를 수 있다. 여기서 self-attention 메커니즘을 적용하면 sat, the, on 등을 보면서 cat의 내포 의미를 더 확실하게 파악할 수 있다. 그래서 cat이 다른 단어를 보며 문맥을 이해한다고 표현한 것이다.\n\n\n\nGPT는 Transformer의 디코더(Decoder) 블록을 기반으로 만들어진 모델이다.\n핵심 특징은 단방향(Unidirectional)으로 문맥을 고려한다는 점이다. Self-Attention을 단방향으로 수행하므로 현재 단어보다 앞쪽(이전)에 있는 단어들만 참고할 수 있다.\n예를 들어, \"The cat sat on the mat\"에서 \"mat\"을 예측할 때 \"The cat sat on the\"까지만 보고 \"mat\"을 결정해야 함.\n반대로 \"mat\"이 \"sat\"에 영향을 미칠 수는 없음. 즉, 미래 정보는 볼 수 없음\nGPT는 생성 모델이기 때문에, 문장을 왼쪽에서 오른쪽으로 한 단어씩 생성해야 한다. 만약 오른쪽(뒤쪽)의 단어를 안다면 이미 정답을 알고 있기에 cheating으로 간주된다.\n따라서 첫 번째 단어는 마지막 단어를 전혀 모름.\n즉, GPT는 앞쪽 단어들이 뒤쪽 단어에 영향을 줄 수 있지만, 반대로 뒤쪽 단어가 앞쪽 단어에 영향을 미칠 수 없음!"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#model",
    "href": "posts/Based_on_Decoder.html#model",
    "title": "Base on Decoder models",
    "section": "2-1. model",
    "text": "2-1. model\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\nmodel_name = \"skt/kogpt2-base-v2\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    bos_token=\"&lt;/s&gt;\",\n    eos_token=\"&lt;/s&gt;\",\n    unk_token=\"&lt;unk&gt;\",\n    pad_token=\"&lt;pad&gt;\",\n    mask_token=\"&lt;mask&gt;\"\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel\n\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(51200, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n)\n\n\n- 모델을 잘 살펴보면 첫 줄의 Embedding(51200, 768)과 마지막 줄의 (lm_head): Linear(in_features=768, out_features=51200, bias=False)에서 51200으로 숫자가 같은데 이게 우연이 아니다.\n! Embedding(vocab_size , Embedding_dim)으로 이루어지는데 51200개의 단어를 768 길이의 벡터에서 임베딩을 통해 단어를 숫자로 표현하겠다는 의미이다. Embedding_dim이 커질 수록 연산은 증가하지만 더 세심하게 단어들끼리의 차이를 둘 수 있다.\n위에서 말했듯이 출력 class는 vocab size와 같기에 그래서 51200의 수가 겹치는 것이다."
  },
  {
    "objectID": "posts/Based_on_Decoder.html#dataset",
    "href": "posts/Based_on_Decoder.html#dataset",
    "title": "Base on Decoder models",
    "section": "2-2. Dataset",
    "text": "2-2. Dataset\n- 위키 데이터라서 양이 매우 많다. 일부만 뽑아서 사용하자.\n\nfrom datasets import load_dataset\n\nsplit_dict = {\n    \"train\": \"train[:8000]\",\n    \"test\": \"train[8000:10000]\",\n    \"unused\": \"train[10000:]\",\n}\ndataset = load_dataset(\"heegyu/kowikitext\", split=split_dict)\ndel dataset[\"unused\"]\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'revid', 'url', 'title', 'text'],\n        num_rows: 8000\n    })\n    test: Dataset({\n        features: ['id', 'revid', 'url', 'title', 'text'],\n        num_rows: 2000\n    })\n})\n\n\n\ntokenized_dataset = dataset.map(\n    lambda batch: tokenizer([f\"{ti}\\n{te}\" for ti, te in zip(batch[\"title\"], batch[\"text\"])]),\n    batched=True, # 배치 단위로 데이터를 처리하도록 설정\n    num_proc=2, # 병렬 처리 개수를 설정 (CPU 코어 2개 사용)\n    remove_columns=dataset[\"train\"].column_names, # 기존 데이터셋의 컬럼 삭제 -&gt; 토큰화 데이터만 남김\n)\ntokenized_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 8000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 2000\n    })\n})"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#preprocessing",
    "href": "posts/Based_on_Decoder.html#preprocessing",
    "title": "Base on Decoder models",
    "section": "2-3. Preprocessing",
    "text": "2-3. Preprocessing\n\nmax_length = 512 # 문장의 최대길이\ndef group_texts(batched_sample):\n    sample = {k: v[0] for k, v in batched_sample.items()}\n\n    if sample[\"input_ids\"][-1] != tokenizer.eos_token_id: # sample['input_ids']의 마지막 토큰이 eos 토큰이 아닌 경우에만 실행된다.\n        for k in sample.keys():\n            sample[k].append(\n                tokenizer.eos_token_id if k == \"input_ids\" else sample[k][-1] # input_ids 키인 경우에는 eos_token_id를 추가한다.\n            )\n\n    result = {k: [v[i: i + max_length] for i in range(0, len(v), max_length)] for k, v in sample.items()}\n    return result\n\ngrouped_dataset = tokenized_dataset.map(\n    group_texts,\n    batched=True, # 입력 데이터가 배치 단위로 전달됨\n    batch_size=1, # 한 번에 1개의 데이터만 처리\n    num_proc=2,\n)\nprint(len(grouped_dataset[\"train\"][0][\"input_ids\"]))\nprint(grouped_dataset)\n\n512\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 18365\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 2400\n    })\n})\n\n\n- 데이터에 정답 데이터인 labels 칼럼이 존재하지 않는다. (디코더 기반이니까)\n디코더 기반 모델 생성태스크에서는 labels 데이터를 따로 작성하지 않고 input_ids에 모두 포함한다.\n-&gt; 입력된 문장을 한 칸 이동시킨 값을 정답으로 사용함. (기술적 이유로 labels 칼럼은 추가해야하기에 콜레이터를 사용해 정답 label이 없는 것을 해결한다.)"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#collator",
    "href": "posts/Based_on_Decoder.html#collator",
    "title": "Base on Decoder models",
    "section": "2-4. Collator",
    "text": "2-4. Collator\nDataCollator는 배치 내에서 sample을 패딩하거나, 마스킹을 적용하는 역할을 한다.\n\nfrom transformers import DataCollatorForLanguageModeling\n\ncollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # mlml = False: 마스크드 언어 모델링을 사용하지 않겠다는 의미\n# mlm=True로 설정한다면 일부 토큰을 [MASK]로 변환하여 원래 단어를 맞추도록 학습하는 방식이다. 이것은 인코더 기반의 BERT가 학습하는 방식이므로 지금은 False로 설정한다.\nsample = collator([grouped_dataset[\"train\"][i] for i in range(1)])\n# 위에서 데이터 전처리할 때 batch_size = 1이므로 range(1)로 설정했다. 만약 batch를 늘리고싶다면 range(???) ???을 늘리면 된다."
  },
  {
    "objectID": "posts/Based_on_Decoder.html#evaluate",
    "href": "posts/Based_on_Decoder.html#evaluate",
    "title": "Base on Decoder models",
    "section": "2-5. Evaluate",
    "text": "2-5. Evaluate\n문장 생성 태스크는 일반적으로 학습 도중 평가를 진행하기 어렵다.\n\n이유\n\n일반적인 모델은 입력 -&gt; 출력 1번의 과정이다. 하지만 생성모델은 한 문장을 만들기 위해 여러 번 추론해야함.\n\n\n일반적 모델: I go school -&gt; (긍정)\n생성모델: ’I-&gt; 'I go' -&gt; 'I go school (여러번 추론이 필요!)\n\n\n생성 모델은 한 번에 몇 번 추론할지 정해져 있지 않음!\n\n\n당연하다. 문장이 언제 끝날지 생성모델은 알 수 없다. (미래를 모르기 때문에)\n\n\n정답과 비교하는 방식이 다르다.\n\n\n일반적인 모델은 실수 값을 출력하므로 산술 연산을 이용해 정답과 비교가 가능하다.\n하지만 생성 모델은 정수 리스트로(토큰 ID 리스트)를 출력하기에 비교 방식이 다르다.\n모델 출력: [12,34,56,78] 정답: [12,34,90,66]\n두 리스트를 논리 연산으로 맞는지 평가해야 한다.\n\n\n길이가 다 달라서 한 번에 처리하기 어렵다.\n\n\n감정 분석같은 일반적 모델은 항상 일정한 출력이 나오므로 한 번에 배치 처리가 가능하다.\n하지만 생성 모델은 출력되는 문장의 길이가 다 다르다.\n\nI go school\nI go shcool to meet my friend\nnice to meet you\n\n이렇게 길이가 다 다르면 한 번에 배치로 비교하기 어려움"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#generate-sentence",
    "href": "posts/Based_on_Decoder.html#generate-sentence",
    "title": "Base on Decoder models",
    "section": "2-6. Generate sentence",
    "text": "2-6. Generate sentence\n\ninputs = tokenizer(\"지난해 7월, \", return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(inputs.input_ids, max_new_tokens=100)\nresult = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(result[0])\n\n지난해 7월, 롯데백화점 본점 지하 1층 식품매장에서 판매된 '롯\n\n\nmax_new_tokens = 100 때문에 길이를 초과할 수 없음.\n\nimport torch\n\ninput_ids = tokenizer(\"지난해 7월, \", return_tensors=\"pt\").to(model.device).input_ids\n\nwith torch.no_grad():\n    for _ in range(100):\n        next_token = model(input_ids).logits[0, -1:].argmax(-1)\n        input_ids = torch.cat((input_ids[0], next_token), -1).unsqueeze(0)\n\nprint(tokenizer.decode(input_ids[0].tolist()))\n\n지난해 7월, 롯데백화점 본점 지하 1층 식품매장에서 판매된 '롯데 햄버거' 제품에서 대장균이 검출돼 판매 중단된 바 있다.\n롯데백화점 측은 \"햄버거 판매 중단은 롯데백화점 본점 식품매장의 위생과 안전관리에 대한 고객들의 신뢰가 크게 훼손된 데 따른 것\"이라며 \"롯데백화점 본점 식품매장은 햄버거 판매 중단을 즉각 중단하고, 롯데백화점 본점 식품\n\n\nEOS 토큰을 만나도 100번 반복. for _ in range(100)은 100번 실행된다는 보장만 있을 뿐, 문장이 길어지는 걸 막지 않는다. 실제로 생성된 토큰 개수가 100개를 초과할 수 있음.\n결론\n첫 번째 코드는 강제 제한이 있지만 두 번째 코드는 100번 반복하면서 문장이 더 길어질 가능성이 높음."
  },
  {
    "objectID": "posts/Based_on_Decoder.html#model-1",
    "href": "posts/Based_on_Decoder.html#model-1",
    "title": "Base on Decoder models",
    "section": "3-1. model",
    "text": "3-1. model\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"skt/kogpt2-base-v2\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    bos_token=\"&lt;/s&gt;\",\n    eos_token=\"&lt;/s&gt;\",\n    unk_token=\"&lt;unk&gt;\",\n    pad_token=\"&lt;pad&gt;\",\n    mask_token=\"&lt;mask&gt;\"\n)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel\n\nSome weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nGPT2ForSequenceClassification(\n  (transformer): GPT2Model(\n    (wte): Embedding(51200, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=2, bias=False)\n)\n\n\n- out_features = 2 \\(\\rightarrow\\) 이진분류"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#dataset-1",
    "href": "posts/Based_on_Decoder.html#dataset-1",
    "title": "Base on Decoder models",
    "section": "3-2. Dataset",
    "text": "3-2. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"sts\")\n\ndef process_data(batch):\n  result = tokenizer(batch[\"sentence1\"], text_pair=batch[\"sentence2\"])\n  result[\"labels\"] = [x[\"binary-label\"] for x in batch[\"labels\"]]\n  return result\n\ndataset = dataset.map(process_data, batched=True, remove_columns=dataset[\"train\"].column_names)"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#collator-1",
    "href": "posts/Based_on_Decoder.html#collator-1",
    "title": "Base on Decoder models",
    "section": "3-3. Collator",
    "text": "3-3. Collator\n\nfrom transformers import DataCollatorWithPadding # 패딩만을 진행하는 DataCollatorWithPadding 사용\n\ncollator = DataCollatorWithPadding(tokenizer)\nbatch = collator([dataset[\"train\"][i] for i in range(4)])"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#predictions",
    "href": "posts/Based_on_Decoder.html#predictions",
    "title": "Base on Decoder models",
    "section": "3-4. Predictions",
    "text": "3-4. Predictions\n\nwith torch.no_grad():\n    logits = model(**batch).logits\n\nlogits\n\ntensor([[ 0.5085, -0.4168],\n        [-0.1568, -0.9897],\n        [ 1.4718, -0.7141],\n        [ 0.5799, -0.3399]])"
  },
  {
    "objectID": "posts/Based_on_Decoder.html#evaluate-1",
    "href": "posts/Based_on_Decoder.html#evaluate-1",
    "title": "Base on Decoder models",
    "section": "3-5. Evaluate",
    "text": "3-5. Evaluate\n\nimport evaluate\n\nf1 = evaluate.load('f1')\nf1.compute(predictions = logits.argmax(-1), references = batch['labels'], average = 'micro')\n\n{'f1': 0.75}"
  },
  {
    "objectID": "posts/Text_Classification.html",
    "href": "posts/Text_Classification.html",
    "title": "Text Classification Fine Tuning",
    "section": "",
    "text": "- colab에서 실습하길 바랍니다.\n# !git clone https://github.com/rickiepark/nlp-with-transformers.git\n# %cd nlp-with-transformers\n# from install import *\n# install_requirements(chapter=2)"
  },
  {
    "objectID": "posts/Text_Classification.html#data-loading-emotion-encoding",
    "href": "posts/Text_Classification.html#data-loading-emotion-encoding",
    "title": "Text Classification Fine Tuning",
    "section": "1. Data loading & Emotion encoding",
    "text": "1. Data loading & Emotion encoding\n\n# 허깅페이스 데이터셋을 사용하기\nfrom huggingface_hub import list_datasets\nfrom datasets import load_dataset\nfrom datasets import ClassLabel\n\nemotions = load_dataset(\"emotion\")\n\nfrom transformers import AutoTokenizer\nemotions['train'].features['label'] = ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])\nmodel_ckpt = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n\nemotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n\n- 코드 설명 1. emotion 데이터를 불러온다. 2. emotion 데이터에서 train에 있는 레이블을 6개의 감정으로 할당해준다. 3. model을 설정하고 tokenizer도 모델에 맞게 불러온다. 4. tokenize 함수를 선언하고 문장 길이를 맞추기 위해 padding과 truncation을 True로 설정한다. 5. emotion을 토크나이징 한다."
  },
  {
    "objectID": "posts/Text_Classification.html#text-tokenizing",
    "href": "posts/Text_Classification.html#text-tokenizing",
    "title": "Text Classification Fine Tuning",
    "section": "Text tokenizing",
    "text": "Text tokenizing\n\nfrom transformers import AutoModel\nimport torch\nfrom sklearn.metrics import accuracy_score, f1_score\n\ntext = \"this is a test\"\ninputs = tokenizer(text, return_tensors=\"pt\")\ninputs['input_ids'].size()\n\n- 코드 설명 1. 임의의 테스트 text를 생성 후 토크나이징을 해준다. 2. tokenizer가 반환하는 데이터를 PyTorch 텐서(torch.Tensor) 형식으로 변환하기 위해서 return_tensors=“pt”를 설정한다."
  },
  {
    "objectID": "posts/Text_Classification.html#hf-login",
    "href": "posts/Text_Classification.html#hf-login",
    "title": "Text Classification Fine Tuning",
    "section": "3. HF login",
    "text": "3. HF login\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
    "objectID": "posts/Text_Classification.html#model",
    "href": "posts/Text_Classification.html#model",
    "title": "Text Classification Fine Tuning",
    "section": "4. model",
    "text": "4. model\n\nfrom transformers import AutoModelForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_labels = 6\nmodel_ckpt = \"distilbert-base-uncased\"\n\n# distilbert-base-uncased가 바디이고 AutoModelForSequenceClassification가 헤드이다.\n# num_label이 6이므로 6개의 감정 클래스를 분류하는 헤드 하나가 추가된 것이다.\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, num_labels=num_labels)\n         .to(device))\n\n- 코드 설명 1. GPU를 사용하기 위해 device 설정. 2. label의 개수는 위에서 할당한 대로 6개이고 model도 선언해준다. 3. 여기서 distilbert-base-uncased은 바디이고 AutoModelForSequenceClassification은 헤드이다. 사전학습된 bert모델에 감정 클래스 분류를 위해서 헤드를 추가했다."
  },
  {
    "objectID": "posts/Text_Classification.html#learning",
    "href": "posts/Text_Classification.html#learning",
    "title": "Text Classification Fine Tuning",
    "section": "5. Learning",
    "text": "5. Learning\n\nfrom transformers import Trainer, TrainingArguments\n\nbatch_size = 64\nlogging_steps = len(emotions_encoded[\"train\"]) // batch_size\nmodel_name = f\"{model_ckpt}-finetuned-emotion\"\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=2,\n                                  learning_rate=2e-5,\n                                  per_device_train_batch_size=batch_size,\n                                  per_device_eval_batch_size=batch_size,\n                                  weight_decay=0.01,\n                                  evaluation_strategy=\"epoch\",\n                                  disable_tqdm=False,\n                                  logging_steps=logging_steps,\n                                  push_to_hub=True,\n                                  save_strategy=\"epoch\",\n                                  load_best_model_at_end=True,\n                                  log_level=\"error\",\n                                  report_to=\"none\")\n\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n\ntrainer = Trainer(model=model, args=training_args,\n                  compute_metrics=compute_metrics,\n                  train_dataset=emotions_encoded[\"train\"],\n                  eval_dataset=emotions_encoded[\"validation\"],\n                  tokenizer=tokenizer)\ntrainer.train()\n\n- 코드 설명 1. training argument를 설정해준다. 2. 학습을 하고 결과를 보니 Loss, Accuracy, F1 들이 전부 향상된 것을 볼 수 있다. 즉 Fine tuning이 잘 이루어 졌다고 볼 수 있다."
  },
  {
    "objectID": "posts/Text_Classification.html#prediction",
    "href": "posts/Text_Classification.html#prediction",
    "title": "Text Classification Fine Tuning",
    "section": "6. Prediction",
    "text": "6. Prediction\n\noutput = trainer.predict(emotions_encoded[\"validation\"])\noutput.metrics\n\n\nimport numpy as np\nyy = np.argmax(output.predictions,axis=1)\nyy"
  },
  {
    "objectID": "posts/Text_Classification.html#error-analyze",
    "href": "posts/Text_Classification.html#error-analyze",
    "title": "Text Classification Fine Tuning",
    "section": "7. Error analyze",
    "text": "7. Error analyze\n\nfrom torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    # 모든 입력 텐서를 모델과 같은 장치로 이동합니다.\n    inputs = {k:v.to(device) for k,v in batch.items()\n              if k in tokenizer.model_input_names}\n\n    with torch.no_grad(): # 역전파를 사용하지 않음 (평가 단계이므로)\n        output = model(**inputs) # 입력 데이터를 모델에 전달\n        pred_label = torch.argmax(output.logits, axis=-1) # 가장 높은 점수를 가진 클래스 선택\n        loss = cross_entropy(output.logits, batch[\"label\"].to(device), # loss 계산\n                             reduction=\"none\") # 평균을 내지 않고 개별 샘플의 손실을 반환\n\n    return {\"loss\": loss.cpu().numpy(), # 결과를 CPU로 이동 및 numpy 배열로 변환 # PyTorch 텐서는 dataset에서 다루기 어렵다.\n            \"predicted_label\": pred_label.cpu().numpy()}\n\n\n# 데이터셋을 다시 파이토치 텐서로 변환\nemotions_encoded.set_format(\"torch\",\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n# 손실 값을 계산\nemotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n    forward_pass_with_label, batched=True, batch_size=16)\n\n- 코드 설명 1. 모든 입력 텐서가 모델과 같아야 계산이 가능하기에 같은 장치로 이동 2. 입력 데이터를 **inputs으로 모델에 전달 후 가장 높은 logits값을 가진 클래스를 선택한다. 3. 이제 loss를 계산하고 평균을 내지 않는 이유는 label마다 loss값의 편차가 있는 것을 확인하기 위해 평균을 내지 않는다. 4. 결과를 numpy로 변환. (datasets.map() 함수는 PyTorch 텐서 대신 리스트나 NumPy 배열을 반환해야 함.) 5. 손실값을 계산하기 위해 PyTorch 텐서로 전환한다. (batch형태로 계산하기 위해서)"
  },
  {
    "objectID": "posts/Text_Classification.html#int---str-변환",
    "href": "posts/Text_Classification.html#int---str-변환",
    "title": "Text Classification Fine Tuning",
    "section": "8. int -> str 변환",
    "text": "8. int -&gt; str 변환\n\ndef label_int2str(row):\n    return emotions[\"train\"].features[\"label\"].int2str(row)\n\nemotions_encoded.set_format(\"pandas\")\ncols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\ndf_test = emotions_encoded[\"validation\"][:][cols]\ndf_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\ndf_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n                              .apply(label_int2str))\n\n\ndf_test.sort_values(\"loss\", ascending=False).head(10)\n\n\ndf_test.sort_values(\"loss\", ascending=True).head(10)\n\n- 코드 설명 1. label에 있는 int형 값들을 사람이 알아보기 쉽게 str형태로 바꿔준다. 2. 결과를 살펴보면 sadness 레이블들은 loss도 적고 잘 맞추는 것을 알 수 있다."
  },
  {
    "objectID": "posts/Text_Classification.html#save-model-publish",
    "href": "posts/Text_Classification.html#save-model-publish",
    "title": "Text Classification Fine Tuning",
    "section": "9. Save model & Publish",
    "text": "9. Save model & Publish\n\ntrainer.push_to_hub(commit_message=\"Training completed!\")\n\n\nfrom transformers import pipeline\n\nmodel_id = \"SangJinCha/distilbert-base-uncased-finetuned-emotion\"\nclassifier = pipeline(\"text-classification\", model=model_id)\n\n이제 모델에 hugging face 사용자 이름을 붙혀서 push 해주면 된다."
  },
  {
    "objectID": "posts/model_fine_tuning.html",
    "href": "posts/model_fine_tuning.html",
    "title": "Model fine tuning",
    "section": "",
    "text": "아래 코드를 실행하고 런타임 재시작\n\n# pip install datasets==2.20.0 transformers==4.41.2 peft==0.10.0 evaluate==0.4.2 scikit-learn==1.4.2 accelerate -U\n\n이전 chapter에서는 전체적으로 미세조정을 하지 않고 모델과 각 태스크의 대략적인 구조만 학습했다.\n아무래도 학습을 하지 않고 바로 predict를 하니 결과가 만족스럽지 않았다.\n이번 chapter에서는 미세조정을 해보는 코드를 배워볼 것이다."
  },
  {
    "objectID": "posts/model_fine_tuning.html#model",
    "href": "posts/model_fine_tuning.html#model",
    "title": "Model fine tuning",
    "section": "2-1 Model",
    "text": "2-1 Model\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n\n\n\n/root/anaconda3/envs/asdf/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
  },
  {
    "objectID": "posts/model_fine_tuning.html#dataset",
    "href": "posts/model_fine_tuning.html#dataset",
    "title": "Model fine tuning",
    "section": "2-2. Dataset",
    "text": "2-2. Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"klue\", \"sts\")\n\ndef process_data(batch):\n  result = tokenizer(\n      batch[\"sentence1\"],\n      text_pair=batch[\"sentence2\"],\n      max_length=128,\n      padding=\"max_length\",\n      truncation=True,\n      return_tensors=\"np\",\n  )\n  result[\"labels\"] = [x[\"binary-label\"] for x in batch[\"labels\"]]\n  return result\n\ntokenized_dataset = dataset.map(\n    process_data,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n)\ntokenized_dataset[\"train\"].column_names\n\n\n\n\n\n\n\n['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
  },
  {
    "objectID": "posts/model_fine_tuning.html#train",
    "href": "posts/model_fine_tuning.html#train",
    "title": "Model fine tuning",
    "section": "2-3 Train",
    "text": "2-3 Train\n\nfrom transformers import (\n    Trainer, # 모델 학습을 위한 훈련 도구\n    TrainingArguments, # 학습을 위한 하이퍼파라미터 및 설정을 정의하는 클래스\n    default_data_collator, # 콜레이터\n    EarlyStoppingCallback # early stop 함수\n)\nimport evaluate \n\n\ndef custom_metrics(pred): # micro f1 score 평가 지표를 로드하는 함수\n  f1 = evaluate.load(\"f1\")\n  labels = pred.label_ids\n  preds = pred.predictions.argmax(-1)\n\n  return f1.compute(predictions=preds, references=labels, average=\"micro\")\n\ntraining_args = TrainingArguments( # 학습 argument 설정\n    per_device_train_batch_size=64, # 학습할 때 배치 크기\n    per_device_eval_batch_size=64, # 평가할 때 배치 크기\n    learning_rate=5e-6,\n    max_grad_norm=1, # 그래디언트 클리핑 (그래디언트 폭발 방지)\n    num_train_epochs=10, \n    evaluation_strategy=\"steps\", # 일정 step마다 검증 실행\n    logging_strategy=\"steps\", # 일정 스텝마다 로그 저장\n    logging_steps=100, # 100 step마다 로그 출력\n    logging_dir=\"data/logs\", # 로그 저장 경로 (현재 실행중인 폴더를 기준으로 저장되므로 현재 폴더를 잘 확인해야함)\n    save_strategy=\"steps\", # 일정 step마다 체크포인트 저장\n    save_steps=100, # 100 step마다 체크포인트 저장\n    output_dir=\"data/ckpt\", # 로그 저장 경로 (현재 실행중인 폴더를 기준으로 저장되므로 현재 폴더를 잘 확인해야함)\n    load_best_model_at_end = True, # 학습 종료 후 가장 좋은 모델 로드\n    report_to='tensorboard', # tensorboard에 학습 로그 기록\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=default_data_collator,\n    compute_metrics=custom_metrics,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)] # 2번 연속으로 검증 성능이 향상되지 않으면 학습 중단\n)\n\ntrainer.train()\n\n/root/anaconda3/envs/asdf/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n\n\n\n\n    \n      \n      \n      [ 400/1830 04:27 &lt; 16:01, 1.49 it/s, Epoch 2/10]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\nF1\n\n\n\n\n100\n0.045700\n0.930365\n0.786127\n\n\n200\n0.083700\n0.718717\n0.805395\n\n\n300\n0.071400\n0.800480\n0.786127\n\n\n400\n0.060000\n0.828407\n0.799615\n\n\n\n\n\n\n\nTrainOutput(global_step=400, training_loss=0.06518504738807679, metrics={'train_runtime': 268.0555, 'train_samples_per_second': 435.283, 'train_steps_per_second': 6.827, 'total_flos': 1678122311086080.0, 'train_loss': 0.06518504738807679, 'epoch': 2.185792349726776})"
  },
  {
    "objectID": "posts/model_fine_tuning.html#predict",
    "href": "posts/model_fine_tuning.html#predict",
    "title": "Model fine tuning",
    "section": "2-4. Predict",
    "text": "2-4. Predict\n미세조정으로 400 step에서 체크포인트로 저장한 경로에서 모델과 토크나이저를 불러와 검증 게이터 중 10개 샘플을 입력해 간단한 추론을 진행한다.\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    BertForSequenceClassification,\n    DataCollatorWithPadding\n)\n\n# tokenizer, model\nmodel_name = \"data/ckpt/checkpoint-400\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name)\n\ncollator = DataCollatorWithPadding(tokenizer) # 콜레이터 설정\nbatch = collator([tokenized_dataset[\"validation\"][i] for i in range(10)])\n\n# inference\nwith torch.no_grad():\n  logits = model(**batch).logits\nlogits\n\ntensor([[-3.8785,  3.4420],\n        [ 3.2443, -2.9451],\n        [ 1.9444, -1.9935],\n        [-3.5267,  3.0033],\n        [ 0.2386, -0.7375],\n        [ 3.3531, -2.7962],\n        [-3.2954,  2.9353],\n        [ 3.8899, -3.2844],\n        [ 4.2035, -3.6925],\n        [ 4.1651, -3.5504]])"
  },
  {
    "objectID": "posts/model_fine_tuning.html#evaluate",
    "href": "posts/model_fine_tuning.html#evaluate",
    "title": "Model fine tuning",
    "section": "2-5. Evaluate",
    "text": "2-5. Evaluate\n\nimport evaluate\n\npred_labels = logits.argmax(dim=1).cpu().numpy()\ntrue_labels = batch[\"labels\"].numpy()\n\nf1 = evaluate.load(\"f1\")\nf1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")\n\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n\n\n{'f1': 1.0}\n\n\n‘klue/bert-base’ 모델을 불러와서 AutoModelForSequenceClassification 헤드를 붙혔다.\n그 후 학습을 진행하였고 학습된 모델을 불러와서 예측을 하였으므로 미세조정(fine tuning)이다."
  },
  {
    "objectID": "posts/model_fine_tuning.html#model-1",
    "href": "posts/model_fine_tuning.html#model-1",
    "title": "Model fine tuning",
    "section": "3-1. model",
    "text": "3-1. model\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\nmodel_name = \"skt/kogpt2-base-v2\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    bos_token=\"&lt;/s&gt;\",\n    eos_token=\"&lt;/s&gt;\",\n    unk_token=\"&lt;unk&gt;\",\n    pad_token=\"&lt;pad&gt;\",\n    mask_token=\"&lt;mask&gt;\"\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)"
  },
  {
    "objectID": "posts/model_fine_tuning.html#dataset-1",
    "href": "posts/model_fine_tuning.html#dataset-1",
    "title": "Model fine tuning",
    "section": "3-2. Dataset",
    "text": "3-2. Dataset\n\nfrom datasets import load_dataset\n\n# 데이터셋 구조화\nsplit_dict = {\n    \"train\": \"train[:8000]\",\n    \"test\": \"train[8000:10000]\",\n    \"unused\": \"train[10000:]\",\n}\ndataset = load_dataset(\"heegyu/kowikitext\", split=split_dict)\ndel dataset[\"unused\"] # 사용하지 않는 unused 데이터를 삭제\n\n# 토큰화\ntokenized_dataset = dataset.map(\n    lambda batch: tokenizer([f\"{ti}\\n{te}\" for ti, te in zip(batch[\"title\"], batch[\"text\"])]),\n    batched=True, # 배치로 묶어서 처리\n    num_proc=2, # 2개 프로세스 병렬 실행\n    remove_columns=dataset[\"train\"].column_names, # 기존의 'title','text' 컬럼을 삭제 후 토큰화 결과만 남김\n)\n\n# 최대 길이로 그룹화\nmax_length = 512 \ndef group_texts(batched_sample):\n    sample = {k: v[0] for k, v in batched_sample.items()} # 데이터셋에서 각 key의 첫 번째 값만 가져오기.\n\n    if sample[\"input_ids\"][-1] != tokenizer.eos_token_id: # 마지막 토큰이 &lt;eos&gt;가 아니라면 &lt;eos&gt; 추가.\n        for k in sample.keys():\n            sample[k].append(\n                tokenizer.eos_token_id if k == \"input_ids\" else sample[k][-1]\n                # sample['input_ids']라면 &lt;eos&gt; 토큰을 추가. / sample['input_ids']가 아니라면 기존 값 유지.(sample[k][-1])\n            )\n\n    result = {\n        k: [v[i: i + max_length] for i in range(0, len(v), max_length)] # 문장이 길면 여러 개의 샘플로 분할\n        for k, v in sample.items()\n    }\n    return result\n\ngrouped_dataset = tokenized_dataset.map(\n    group_texts,\n    batched=True,\n    batch_size=1,\n    num_proc=2,\n)\ngrouped_dataset[\"train\"].column_names\n\n\n\n\n\n\n\n['input_ids', 'attention_mask']"
  },
  {
    "objectID": "posts/model_fine_tuning.html#fine-tuning",
    "href": "posts/model_fine_tuning.html#fine-tuning",
    "title": "Model fine tuning",
    "section": "3-3. Fine tuning",
    "text": "3-3. Fine tuning\n\nfrom transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=5e-6,\n    max_grad_norm=1,\n    num_train_epochs=3,\n    evaluation_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    logging_dir=\"data/logs\",\n    output_dir=\"data/ckpt\",\n    report_to=\"tensorboard\",\n)\n\ncollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=grouped_dataset[\"train\"],\n    eval_dataset=grouped_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=collator,\n)\n\ntrainer.train()\ntrainer.save_model(\"data/model\")\n\n/root/anaconda3/envs/asdf/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [13776/13776 1:59:37, Epoch 3/3]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n500\n4.284300\n4.471501\n\n\n1000\n4.219000\n3.416660\n\n\n1500\n4.206900\n2.995688\n\n\n2000\n4.185800\n2.903164\n\n\n2500\n4.168300\n2.876508\n\n\n3000\n4.151000\n2.805254\n\n\n3500\n4.151500\n2.801232\n\n\n4000\n4.160500\n2.774476\n\n\n4500\n4.133300\n2.774085\n\n\n5000\n4.085200\n2.745091\n\n\n5500\n4.062300\n2.747127\n\n\n6000\n4.066100\n2.767898\n\n\n6500\n4.037000\n2.726495\n\n\n7000\n4.048100\n2.735216\n\n\n7500\n4.060500\n2.741515\n\n\n8000\n4.031300\n2.703305\n\n\n8500\n4.050100\n2.725095\n\n\n9000\n4.062700\n2.712202\n\n\n9500\n4.020300\n2.688988\n\n\n10000\n3.996800\n2.699451\n\n\n10500\n4.008100\n2.701477\n\n\n11000\n3.989600\n2.698209\n\n\n11500\n3.984000\n2.686813\n\n\n12000\n4.007600\n2.688273\n\n\n12500\n3.992500\n2.678961\n\n\n13000\n3.982600\n2.687267\n\n\n13500\n4.009200\n2.686797"
  },
  {
    "objectID": "posts/model_fine_tuning.html#predict-by-before-fine-tuning-model",
    "href": "posts/model_fine_tuning.html#predict-by-before-fine-tuning-model",
    "title": "Model fine tuning",
    "section": "3-4. Predict by before fine tuning model",
    "text": "3-4. Predict by before fine tuning model\n\nimport torch\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\n\n# 미세조정 이전\norigin_name = \"skt/kogpt2-base-v2\"\norigin_tokenizer = AutoTokenizer.from_pretrained(\n    origin_name,\n    bos_token=\"&lt;/s&gt;\",\n    eos_token=\"&lt;/s&gt;\",\n    unk_token=\"&lt;unk&gt;\",\n    pad_token=\"&lt;pad&gt;\",\n    mask_token=\"&lt;mask&gt;\"\n)\norigin_model = GPT2LMHeadModel.from_pretrained(origin_name)\n\ninputs1 = origin_tokenizer(\n    \"우리는 누구나 희망을 가지고\",\n    return_tensors=\"pt\"\n).to(origin_model.device)\noutputs1 = origin_model.generate(inputs1.input_ids, max_length=128, repetition_penalty=2.0)\nresult1 = origin_tokenizer.batch_decode(outputs1, skip_special_tokens=True)\nprint(result1[0])\n\n/root/anaconda3/envs/asdf/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n우리는 누구나 희망을 가지고 살아갈 수 있는 사회를 만들어야 한다\"고 강조했다.\n이날 행사에는 박근혜 대통령, 황우여 새누리당 대표 등 여권 지도부와 김무성 전 대표가 참석해 축사를 했다.\n김영삼 정부 시절인 지난 2007년 대선 당시 이명박 후보의 당선을 위해 '국민통합21'을 이끌었던 이 후보는 \"우리나라에서 가장 큰 문제는 경제\"라며 \"이명박은 경제를 살리고 서민을 위한 정치를 하겠다고 약속했지만 현실은 그렇지 못했다\"며 이같이 말했다.\n그는 이어 \"나는 지금 대한민국을 걱정하고 있다, 경제가 어렵다면 우리 모두 힘을 모아 위기를 극복해야 한다고 생각한다"
  },
  {
    "objectID": "posts/model_fine_tuning.html#predict-by-after-fine-tuning-model",
    "href": "posts/model_fine_tuning.html#predict-by-after-fine-tuning-model",
    "title": "Model fine tuning",
    "section": "3-5. Predict by after fine tuning model",
    "text": "3-5. Predict by after fine tuning model\n\n# 미세조정 이후\nfinetuned_name = \"data/model\"\nfinetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_name)\nfinetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_name)\n\ninputs2 = finetuned_tokenizer(\n    \"우리는 누구나 희망을 가지고\",\n    return_tensors=\"pt\").to(finetuned_model.device)\noutputs2 = finetuned_model.generate(\n    inputs2.input_ids,\n    max_length=128,\n    repetition_penalty=2.0\n)\nresult2 = finetuned_tokenizer.batch_decode(outputs2, skip_special_tokens=True)\nprint(result2[0])\n\n우리는 누구나 희망을 가지고 살아갈 수 있는 사회를 만들자는 것이었다. 그러나 그 희망은 결국 좌절되고 말았다. 이 절망적인 상황 속에서, 사람들은 자신들의 삶을 포기하고 다른 사람들의 삶으로 돌아가고자 하였다. 이러한 상황에서 그들은 자신의 삶에 대한 책임을 회피하고 자기 자신을 희생하는 선택을 하게 되었다. 그리하여 많은 사람들이 자살을 선택하게 되었고, 이는 곧 자살로 이어지게 되었다.\n이러한 상황에 대해서, 현대 사회는 개인의 존엄성을 존중하지 않는 사회라고 비판하였다. 또한 개인들이 스스로 목숨을 끊는 것을 막기 위해 노력하기도 했다. 하지만 그러한 극단적인 행동들은 오히려 사람들을 죽음으로 내몰게 만드는 결과를 가져왔다. 예를 들어, 한 개인이 사망할 경우 가족들의 동의 없이 강제로 죽음을"
  },
  {
    "objectID": "posts/model_fine_tuning.html#model-2",
    "href": "posts/model_fine_tuning.html#model-2",
    "title": "Model fine tuning",
    "section": "4-1. model",
    "text": "4-1. model\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = 'hyunwoongko/kobart'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2."
  },
  {
    "objectID": "posts/model_fine_tuning.html#dataset-2",
    "href": "posts/model_fine_tuning.html#dataset-2",
    "title": "Model fine tuning",
    "section": "4-2. Dataset",
    "text": "4-2. Dataset\n번역 모델에서는 text_target이라는 인자를 사용하고 한국어를 입력했을 때 영어를 출력으로 생성하는 목적으로 text_target = batch['english']를 사용한다\n\nfrom datasets import load_dataset\n\ndataset = load_dataset('msarmi9/korean-english-multitarget-ted-talks-task')\ntokenized_dataset = dataset.map(\n    lambda batch: (tokenizer(batch['korean'], text_target = batch['english'], max_length=128, truncation = True)\n                  ),batched = True, batch_size = 1000, num_proc = 2, remove_columns = dataset['train'].column_names)\n\ntokenized_dataset['train'].column_names\n\n\n\n\n\n\n\n\n\n\n['input_ids', 'attention_mask', 'labels']\n\n\ninput_ids : 입력 문장이 토큰화 되어서 사전에 매핑된 숫자로 변환된다.\nattention mask: 진짜 입력과 패딩을 구별하도록 도와준다. input_ids의 길이가 모두 같아야 모델이 한 꺼번에 처리할 수 있기에 문장이 짧은 경우엔 뒤에 0 또는 [PAD]를 넣어서 길이를 맞춘다. 이때, 어떤 부분이 실제 문장이고 어떤 부분이 패딩인지 알려주는 게 바로 attention mask이다.\nlabels: 정답 토큰 시퀀스이다. Seq2Seq 모델에서 모델이 예측해야 할 목표 문장을 의미한다. labels도 길이를 맞추기 위해 padding을 넣을 수 있는데 padding 부분은 Loss 계산에서 무시해야하므로 보통 -100으로 채운다.\nlabels가 있다는 건 정답이 있다는 의미!\n번역 Task에서 정답은 번역하고자 하는 언어로 쓰여진 문장이므로 여기서는 영어 문장이다.\n영어 문장은 Dataset에 이미 포함되어있고 학습을 위해 Loss 계산을 할 때 사용된다."
  },
  {
    "objectID": "posts/model_fine_tuning.html#fine-tuning-1",
    "href": "posts/model_fine_tuning.html#fine-tuning-1",
    "title": "Model fine tuning",
    "section": "4-3. fine tuning",
    "text": "4-3. fine tuning\n\nfrom transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    learning_rate=2e-5,\n    max_grad_norm=1,\n    num_train_epochs=2,\n    evaluation_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    logging_dir=\"data/logs\",\n    save_strategy=\"steps\",\n    save_steps=1000,\n    output_dir=\"data/ckpt\",\n    report_to=\"tensorboard\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n)\n\n\ntrainer.train()\ntrainer.save_model(\"data/model\")\n\n- GPU 사용할 때 학습 시간은 2시간이 조금 넘기에 학습은 생략했습니다."
  },
  {
    "objectID": "posts/model_fine_tuning.html#predict-1",
    "href": "posts/model_fine_tuning.html#predict-1",
    "title": "Model fine tuning",
    "section": "4-4. predict",
    "text": "4-4. predict\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    GenerationConfig\n)\n\nmodel_name = \"data/model\" # 미세조정한 model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ncollator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"max_length\",\n    max_length=512,\n)\nbatch = collator([tokenized_dataset[\"test\"][i] for i in range(2)])\n\noutputs = model.generate(batch[\"input_ids\"], max_length=128, do_sample=False)\nresult = tokenizer.batch_decode(outputs, skip_special_tokens=True)\norigin = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\nprint(f\"원본 : {origin[0]} -&gt; 영어 : {result[0]}\")\nprint(f\"원본 : {origin[1]} -&gt; 영어 : {result[1]}\")"
  }
]