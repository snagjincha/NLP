[
  {
    "objectID": "posts/Text_Classification.html",
    "href": "posts/Text_Classification.html",
    "title": "Text Classification Fine Tuning",
    "section": "",
    "text": "- colab에서 실습하길 바랍니다.\n# !git clone https://github.com/rickiepark/nlp-with-transformers.git\n# %cd nlp-with-transformers\n# from install import *\n# install_requirements(chapter=2)"
  },
  {
    "objectID": "posts/Text_Classification.html#data-loading-emotion-encoding",
    "href": "posts/Text_Classification.html#data-loading-emotion-encoding",
    "title": "Text Classification Fine Tuning",
    "section": "1. Data loading & Emotion encoding",
    "text": "1. Data loading & Emotion encoding\n\n# 허깅페이스 데이터셋을 사용하기\nfrom huggingface_hub import list_datasets\nfrom datasets import load_dataset\nfrom datasets import ClassLabel\n\nemotions = load_dataset(\"emotion\")\n\nfrom transformers import AutoTokenizer\nemotions['train'].features['label'] = ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])\nmodel_ckpt = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n\nemotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n\n- 코드 설명 1. emotion 데이터를 불러온다. 2. emotion 데이터에서 train에 있는 레이블을 6개의 감정으로 할당해준다. 3. model을 설정하고 tokenizer도 모델에 맞게 불러온다. 4. tokenize 함수를 선언하고 문장 길이를 맞추기 위해 padding과 truncation을 True로 설정한다. 5. emotion을 토크나이징 한다."
  },
  {
    "objectID": "posts/Text_Classification.html#text-tokenizing",
    "href": "posts/Text_Classification.html#text-tokenizing",
    "title": "Text Classification Fine Tuning",
    "section": "Text tokenizing",
    "text": "Text tokenizing\n\nfrom transformers import AutoModel\nimport torch\nfrom sklearn.metrics import accuracy_score, f1_score\n\ntext = \"this is a test\"\ninputs = tokenizer(text, return_tensors=\"pt\")\ninputs['input_ids'].size()\n\n- 코드 설명 1. 임의의 테스트 text를 생성 후 토크나이징을 해준다. 2. tokenizer가 반환하는 데이터를 PyTorch 텐서(torch.Tensor) 형식으로 변환하기 위해서 return_tensors=“pt”를 설정한다."
  },
  {
    "objectID": "posts/Text_Classification.html#hf-login",
    "href": "posts/Text_Classification.html#hf-login",
    "title": "Text Classification Fine Tuning",
    "section": "3. HF login",
    "text": "3. HF login\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
    "objectID": "posts/Text_Classification.html#model",
    "href": "posts/Text_Classification.html#model",
    "title": "Text Classification Fine Tuning",
    "section": "4. model",
    "text": "4. model\n\nfrom transformers import AutoModelForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_labels = 6\nmodel_ckpt = \"distilbert-base-uncased\"\n\n# distilbert-base-uncased가 바디이고 AutoModelForSequenceClassification가 헤드이다.\n# num_label이 6이므로 6개의 감정 클래스를 분류하는 헤드 하나가 추가된 것이다.\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, num_labels=num_labels)\n         .to(device))\n\n- 코드 설명 1. GPU를 사용하기 위해 device 설정. 2. label의 개수는 위에서 할당한 대로 6개이고 model도 선언해준다. 3. 여기서 distilbert-base-uncased은 바디이고 AutoModelForSequenceClassification은 헤드이다. 사전학습된 bert모델에 감정 클래스 분류를 위해서 헤드를 추가했다."
  },
  {
    "objectID": "posts/Text_Classification.html#learning",
    "href": "posts/Text_Classification.html#learning",
    "title": "Text Classification Fine Tuning",
    "section": "5. Learning",
    "text": "5. Learning\n\nfrom transformers import Trainer, TrainingArguments\n\nbatch_size = 64\nlogging_steps = len(emotions_encoded[\"train\"]) // batch_size\nmodel_name = f\"{model_ckpt}-finetuned-emotion\"\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=2,\n                                  learning_rate=2e-5,\n                                  per_device_train_batch_size=batch_size,\n                                  per_device_eval_batch_size=batch_size,\n                                  weight_decay=0.01,\n                                  evaluation_strategy=\"epoch\",\n                                  disable_tqdm=False,\n                                  logging_steps=logging_steps,\n                                  push_to_hub=True,\n                                  save_strategy=\"epoch\",\n                                  load_best_model_at_end=True,\n                                  log_level=\"error\",\n                                  report_to=\"none\")\n\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n\ntrainer = Trainer(model=model, args=training_args,\n                  compute_metrics=compute_metrics,\n                  train_dataset=emotions_encoded[\"train\"],\n                  eval_dataset=emotions_encoded[\"validation\"],\n                  tokenizer=tokenizer)\ntrainer.train()\n\n- 코드 설명 1. training argument를 설정해준다. 2. 학습을 하고 결과를 보니 Loss, Accuracy, F1 들이 전부 향상된 것을 볼 수 있다. 즉 Fine tuning이 잘 이루어 졌다고 볼 수 있다."
  },
  {
    "objectID": "posts/Text_Classification.html#prediction",
    "href": "posts/Text_Classification.html#prediction",
    "title": "Text Classification Fine Tuning",
    "section": "6. Prediction",
    "text": "6. Prediction\n\noutput = trainer.predict(emotions_encoded[\"validation\"])\noutput.metrics\n\n\nimport numpy as np\nyy = np.argmax(output.predictions,axis=1)\nyy"
  },
  {
    "objectID": "posts/Text_Classification.html#error-analyze",
    "href": "posts/Text_Classification.html#error-analyze",
    "title": "Text Classification Fine Tuning",
    "section": "7. Error analyze",
    "text": "7. Error analyze\n\nfrom torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    # 모든 입력 텐서를 모델과 같은 장치로 이동합니다.\n    inputs = {k:v.to(device) for k,v in batch.items()\n              if k in tokenizer.model_input_names}\n\n    with torch.no_grad(): # 역전파를 사용하지 않음 (평가 단계이므로)\n        output = model(**inputs) # 입력 데이터를 모델에 전달\n        pred_label = torch.argmax(output.logits, axis=-1) # 가장 높은 점수를 가진 클래스 선택\n        loss = cross_entropy(output.logits, batch[\"label\"].to(device), # loss 계산\n                             reduction=\"none\") # 평균을 내지 않고 개별 샘플의 손실을 반환\n\n    return {\"loss\": loss.cpu().numpy(), # 결과를 CPU로 이동 및 numpy 배열로 변환 # PyTorch 텐서는 dataset에서 다루기 어렵다.\n            \"predicted_label\": pred_label.cpu().numpy()}\n\n\n# 데이터셋을 다시 파이토치 텐서로 변환\nemotions_encoded.set_format(\"torch\",\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n# 손실 값을 계산\nemotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n    forward_pass_with_label, batched=True, batch_size=16)\n\n- 코드 설명 1. 모든 입력 텐서가 모델과 같아야 계산이 가능하기에 같은 장치로 이동 2. 입력 데이터를 **inputs으로 모델에 전달 후 가장 높은 logits값을 가진 클래스를 선택한다. 3. 이제 loss를 계산하고 평균을 내지 않는 이유는 label마다 loss값의 편차가 있는 것을 확인하기 위해 평균을 내지 않는다. 4. 결과를 numpy로 변환. (datasets.map() 함수는 PyTorch 텐서 대신 리스트나 NumPy 배열을 반환해야 함.) 5. 손실값을 계산하기 위해 PyTorch 텐서로 전환한다. (batch형태로 계산하기 위해서)"
  },
  {
    "objectID": "posts/Text_Classification.html#int---str-변환",
    "href": "posts/Text_Classification.html#int---str-변환",
    "title": "Text Classification Fine Tuning",
    "section": "8. int -> str 변환",
    "text": "8. int -&gt; str 변환\n\ndef label_int2str(row):\n    return emotions[\"train\"].features[\"label\"].int2str(row)\n\nemotions_encoded.set_format(\"pandas\")\ncols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\ndf_test = emotions_encoded[\"validation\"][:][cols]\ndf_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\ndf_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n                              .apply(label_int2str))\n\n\ndf_test.sort_values(\"loss\", ascending=False).head(10)\n\n\ndf_test.sort_values(\"loss\", ascending=True).head(10)\n\n- 코드 설명 1. label에 있는 int형 값들을 사람이 알아보기 쉽게 str형태로 바꿔준다. 2. 결과를 살펴보면 sadness 레이블들은 loss도 적고 잘 맞추는 것을 알 수 있다."
  },
  {
    "objectID": "posts/Text_Classification.html#save-model-publish",
    "href": "posts/Text_Classification.html#save-model-publish",
    "title": "Text Classification Fine Tuning",
    "section": "9. Save model & Publish",
    "text": "9. Save model & Publish\n\ntrainer.push_to_hub(commit_message=\"Training completed!\")\n\n\nfrom transformers import pipeline\n\nmodel_id = \"SangJinCha/distilbert-base-uncased-finetuned-emotion\"\nclassifier = pipeline(\"text-classification\", model=model_id)\n\n이제 모델에 hugging face 사용자 이름을 붙혀서 push 해주면 된다."
  },
  {
    "objectID": "posts/P_vs_P.html",
    "href": "posts/P_vs_P.html",
    "title": "Position Embedding vs Position Encoding",
    "section": "",
    "text": "1. 위치 임베딩 vs 위치 인코딩\n- 위치 임베딩\n위치별로 학습 가능한 임베딩을 부여하는 방식 (단어들의 위치에 따라 학습 가능한 값을 부여)\n\n모델이 학습을 통해 위치별 임베딩 값을 조정할 수 있다.\n예를 들어, 위치 0번, 1번, 2번, …에 대해 각각 학습 가능한 벡터가 존재한다.\n\n\\(PE_{learned}(pos)=Embedding(pos)\\)\n- 위치 인코딩\n위치 정보를 사인(sin)과 코사인(cos) 함수로 생성하는 방식이야. (단어들의 위치에 따라 학습 불가능한 상수 값을 부여\n\n사전 정의된 함수(수학적 패턴)를 사용하기 때문에 변화하지 않는 상수 값\n학습을 통해 조정되지 않고, 입력 데이터에 대해 고정된 값을 사용한다.\n\n\\(PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\\)\n\\(PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\\)\n\npos는 단어의 위치,\ni는 임베딩 차원의 인덱스\nd는 임베딩 차원 크기\n\n\n\n2. 각 방식의 장단점\nGood 위치 임베딩 장점 1. 학습 가능: 위치 임베딩은 학습이 가능하기에 모델이 데이터에 맞춰 위치 정보를 최적화할 수 있다. 2. 더 높은 유연성: 모델이 데이터를 통해 학습하므로, 특정 문맥이나 도메인에 맞춰 위치 표현을 다르게 할 수 있다.\nBad 위치 임베딩 단점 1. 메모리와 연산 비용: 위치 임베딩은 각 위치마다 임베딩 벡터를 학습해야 하기 때문에, 추가적인 파라미터와 메모리를 사용한다. 2. 오버피팅: 학습 가능한 위치 벡터가 많아지면, 모델이 훈련 데이터에 과적합할 가능성이 커질 수 있다.\nGood 위치 인코딩 장점 1. 메모리 효율성: 파라미터가 없기에 메모리 부담이 적다. 2. 고정된 패턴: 위치 인코딩은 고정된 수학적 패턴을 사용하므로, 특정 위치 간의 관계를 명확하게 정의할 수 있다.\nBad 위치 인코딩 단점 1. 학습할 수 없음: 모델이 학습을 통해 위치 정보를 더 정교하게 조정할 수 없다. 2. 유연성 부족: 모든 문장에서 같은 패턴을 사용하기에 특정 도메인이나 데이터 셋에 대해 최적화된 위치 정보를 표현할 수 없다.\n\n\n3. 결론\n\n위치 인코딩은 일반적인 용도와 효율성이 중요한 경우에 적합하다.\n위치 임베딩은 특정 도메인이나 고유한 데이터셋에 대해 더 정교한 위치 표현이 필요한 경우에 유리하다."
  },
  {
    "objectID": "posts/Text_generation.html",
    "href": "posts/Text_generation.html",
    "title": "Text generation",
    "section": "",
    "text": "1. 그리디 서치 디코딩\n- 그리디 서치 디코딩의 이해\n처음 문장(\\(x=x_1,...,x_k\\)) 이 주어질 때 텍스트에 등장하는 토큰 시퀀스 (\\(y=y_1,...,y_t\\))의 확률 \\(P(y|x)\\)를 추정하도록 사전 훈련된다.\n하지만 직접 \\(P(y|x)\\)을 추정하려면 방대한 양의 훈련데이터가 필요하므로 연쇄법칙(Chain Rule of Probability) 을 사용해 조건부 확률의 곱으로 나타낸다.\n\\[\nP(y_1, ..., y_t | x) = \\prod_{t=1}^{N} P(y_t | y_{&lt;t}, x)\n\\]\n계산된 확률을 기반으로, 각 시점 t에서 가장 확률이 높은 단어를 선택하여 다음 단어를 예측한다.\n- 아래는 그리디 서치 디코딩의 구현이다.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n2025-03-17 17:53:47.780280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1742234027.798227   18290 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1742234027.803855   18290 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1742234027.817808   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1742234027.817823   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1742234027.817825   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1742234027.817827   18290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-03-17 17:53:47.822310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nimport pandas as pd\n\ninput_txt = \"Transformers are the\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\niterations = [] # 스텝별로 예측된 단어들을 저장할 리스트\nn_steps = 8 # 최대 8개의 단어를 추가 생성\nchoices_per_step = 5 # 각 스텝에서 가장 높은 확률을 가진 5개의 단어를 저장\n\nwith torch.no_grad(): # 학습이 아니라 예측을 수행하므로 gradient 계산을 비활성화\n    for _ in range(n_steps): \n        iteration = dict() \n        iteration[\"Input\"] = tokenizer.decode(input_ids[0]) # 딕셔너리 생성 후 현재까지의 문장을 저장\n        output = model(input_ids=input_ids) # 현재 문장을 모델에 입력하려 다음 단어의 확률을 얻음.\n        # 첫 번째 배치의 마지막 토큰의 로짓을 선택해 소프트맥스를 적용합니다.\n        next_token_logits = output.logits[0, -1, :]\n        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n        # 가장 높은 확률의 토큰을 저장합니다.\n        for choice_idx in range(choices_per_step):\n            token_id = sorted_ids[choice_idx]\n            token_prob = next_token_probs[token_id].cpu().numpy()\n            token_choice = (\n                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n            )\n            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n        # 예측한 다음 토큰을 입력에 추가합니다.\n        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n        iterations.append(iteration)\n\npd.DataFrame(iterations)\n\n\n\n\n\n\n\n\n\nInput\nChoice 1\nChoice 2\nChoice 3\nChoice 4\nChoice 5\n\n\n\n\n0\nTransformers are the\nmost (8.53%)\nonly (4.96%)\nbest (4.65%)\nTransformers (4.37%)\nultimate (2.16%)\n\n\n1\nTransformers are the most\npopular (16.78%)\npowerful (5.37%)\ncommon (4.96%)\nfamous (3.72%)\nsuccessful (3.20%)\n\n\n2\nTransformers are the most popular\ntoy (10.63%)\ntoys (7.23%)\nTransformers (6.60%)\nof (5.46%)\nand (3.76%)\n\n\n3\nTransformers are the most popular toy\nline (34.38%)\nin (18.20%)\nof (11.71%)\nbrand (6.10%)\nline (2.69%)\n\n\n4\nTransformers are the most popular toy line\nin (46.28%)\nof (15.09%)\n, (4.94%)\non (4.40%)\never (2.72%)\n\n\n5\nTransformers are the most popular toy line in\nthe (65.99%)\nhistory (12.42%)\nAmerica (6.91%)\nJapan (2.44%)\nNorth (1.40%)\n\n\n6\nTransformers are the most popular toy line in the\nworld (69.26%)\nUnited (4.55%)\nhistory (4.29%)\nUS (4.23%)\nU (2.30%)\n\n\n7\nTransformers are the most popular toy line in ...\n, (39.73%)\n. (30.64%)\nand (9.87%)\nwith (2.32%)\ntoday (1.74%)\n\n\n\n\n\n\n\n\n\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\nprint(tokenizer.decode(output[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nTransformers are the most popular toy line in the world,\n\n\n\nmax_length = 128\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput_greedy = model.generate(input_ids, max_length=max_length,\n                               do_sample=False)\nprint(tokenizer.decode(output_greedy[0]))\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n\n\nThe researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n\n\nThe researchers were surprised to find that the unicorns were able\n\n\n- 그리디 서치 디코딩은 각 타임스텝에서 확률이 가장 높은 토큰을 탐욕적(greedily)으로 선택하는 방식이다.\n하지만 이것은 사실 최적의 디코딩 방식이 아니다. 가장 최적의 방식은 가능한 모든 경우의 수로 문장을 완성시켜놓고 그 후에 그 문장을 판단하여 가장 좋은 문장을 선택하는 방식이다.\n\n하지만 그것은 너무 비용이 많이 드는 문제가 있어서 그리디 서치 디코딩 방식이 연구되었지만 이 방식 또한 반복적인 출력 시퀀스를 생성하는 경향이 있다. 이로 인해 최적의 솔루션을 만들기는 어렵다.\n\n\n2. 빔 서치 디코딩\n- 빔 서치는 각 스텝에서 확률이 가장 높은 토큰을 디코딩하는 대신, 확률이 가장 높은 상위 b개의 다음 토큰을 추적한다.\n빔 세트는 기존 세트에서 가능한 모든 다음 토큰을 확장한 후 확률이 가장 높은 b개의 확장을 선택하여 구성한다.\n\n이 과정은 최대 길이나 EOS토큰에 도달할 때까지 반복된다.\n- 로그확률을 이용하는 이유\n빔서치는 다음 단어의 확률을 계산할 때 기존의 곱으로 연결되던 확률이 아닌 로그를 취한 확률을 이용한다. 즉 로그확률을 이용한다.\n\n그 이유는 곱셈에서 사용되는 각 조건부 확률은 0과 1사이에 있는 작은 값이다. 이 값들은 문장의 길이가 조금만 길어지면 전체 확률이 0으로 가깝게 되는 underfolow가 쉽게 발생한다.\n\n아주 작은 값을 수치적으로 불안정하기에 확률에 log를 취해주면 곱셈이 덧셈으로 바뀌기에 식이 안정화된다. 이런 값은 다루기 훨씬 쉽다.\n\n추가적으로 log는 단조증가함수로서 확률크기를 비교만하면 되기에 log를 취해도 확률간에 대소관계는 달라지지 않기에 로그확률을 사용해도 상관없다.\n\n# 이 함수는 주어진 logits에서 특정 labels에 해당하는 로그 확률을 추출하는 함수이다.\n\nimport torch.nn.functional as F\n\ndef log_probs_from_logits(logits, labels):\n    logp = F.log_softmax(logits, dim=-1)\n    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n    # unsqueeze(2)를 이용해서 logp과 labels의 차원을 맞춰줌. 그 후에 squeeze를 이용해서 크기가 1인 차원을 없앤다.\n    return logp_label\n\n\n# 이 함수는 모델이 예측한 logits을 사용하여 전체 문장의 로그 확률을 계산하는 함수이다.\n\ndef sequence_logprob(model, labels, input_len=0):\n    with torch.no_grad():\n        output = model(labels)\n        log_probs = log_probs_from_logits(\n            output.logits[:, :-1, :], labels[:, 1:]) # labels[:,1:]는 정답, output.logits[:,:-1,:]은 마지막 단어를 제외한 모든 단어의 로짓\n        seq_log_prob = torch.sum(log_probs[:, input_len:]) #input_len만큼의 확률을 무시하고 더한다.\n    return seq_log_prob.cpu().numpy()\n\n1 그리디 서칭 디코딩으로 만든 시퀀스 로그확률 계산\n\nlogp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_greedy[0]))\nprint(f\"\\n로그 확률: {logp:.2f}\")\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a\nremote, previously unexplored valley, in the Andes Mountains. Even more\nsurprising to the researchers was the fact that the unicorns spoke perfect\nEnglish.\n\n\nThe researchers, from the University of California, Davis, and the University of\nColorado, Boulder, were conducting a study on the Andean cloud forest, which is\nhome to the rare species of cloud forest trees.\n\n\nThe researchers were surprised to find that the unicorns were able to\ncommunicate with each other, and even with humans.\n\n\nThe researchers were surprised to find that the unicorns were able\n\n로그 확률: -87.43\n\n\n2 빔 서치 디코딩으로 만든 시퀀스 로그확률 계산\n\noutput_beam = model.generate(input_ids, max_length=max_length, num_beams=5, # num_beams을 설정하면 빔 서치 디코딩이 활성화 된다. (가장 가능성이 높은 5문장을 동시탐색)\n                             do_sample=False) # do_sample=False는 샘플링을 하지 않고 결정론적 방식으로 단어를 선택(항상 같은 문장이 생성됨)\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_beam[0]))\nprint(f\"\\n로그 확률: {logp:.2f}\")\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a\nremote, previously unexplored valley, in the Andes Mountains. Even more\nsurprising to the researchers was the fact that the unicorns spoke perfect\nEnglish.\n\n\nThe discovery of the unicorns was made by a team of scientists from the\nUniversity of California, Santa Cruz, and the National Geographic Society.\n\n\nThe scientists were conducting a study of the Andes Mountains when they\ndiscovered a herd of unicorns living in a remote, previously unexplored valley,\nin the Andes Mountains. Even more surprising to the researchers was the fact\nthat the unicorns spoke perfect English\n\n로그 확률: -55.23\n\n\n- 빔 서치 디코딩 no_repeat_ngram_size 옵션 활성화\nno_repeat_ngram_size은 빔 서치도 텍스트가 반복되는 문제가 있기에 그 문제를 해결하기 위해 n-그램 페널티를 부과하는 옵션이다.\n\noutput_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n                             do_sample=False, no_repeat_ngram_size=2) # 생성된 문장에서 동일한 연속된 n개의 단어가 반복되지 않도록 제한하는 옵션.\n                             # 빔 서치도 텍스트가 반복되는 문제가 있기에 no_repeat_ngram_size을 설정한다.\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_beam[0]))\nprint(f\"\\n로그 확률: {logp:.2f}\")\n\nIn a shocking finding, scientist discovered a herd of unicorns living in a\nremote, previously unexplored valley, in the Andes Mountains. Even more\nsurprising to the researchers was the fact that the unicorns spoke perfect\nEnglish.\n\n\nThe discovery was made by a team of scientists from the University of\nCalifornia, Santa Cruz, and the National Geographic Society.\n\nAccording to a press release, the scientists were conducting a survey of the\narea when they came across the herd. They were surprised to find that they were\nable to converse with the animals in English, even though they had never seen a\nunicorn in person before. The researchers were\n\n로그 확률: -93.12\n\n\n점수는 낮아졌지만 텍스트가 일관성을 유지하기에 결과는 좋다!\n! 참고로 당연히 각 시점에서의 로그확률은 음수이다 (0과 1사이의 값에 로그를 취하면 음수이기에…)\n하지만 1에 가까울 수록 절댓값은 더 작아지기에 시퀀스의 로그확률(모두 더한 값)은 0에 가까우면 가까울수록 좋은 것이다. (양수는 불가능)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": ".",
    "section": "",
    "text": "Position Embedding vs Position Encoding\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Transformers\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nText generation\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nText Classification Fine Tuning\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluate\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2025\n\n\n차상진\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Evaluate.html",
    "href": "posts/Evaluate.html",
    "title": "Evaluate",
    "section": "",
    "text": "Evaluate 클래스는 accuracy, F1 score, precision, recall, BLEU, ROUGE 등등 다양한 평가 지표를 간단하게 불러와 활용할 수 있다.\n\nimport evaluate\nacc = evaluate.load('accuracy')\n\n\n\n\n계산을 해보는 예시코드이다.\n\nmetrics = evaluate.combine(['accuracy','f1','precision','recall'])\nmetrics.compute(predictions=[1,0,0,1], references = [0,1,0,1])\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n\nfor y,pred in zip([0,1,0,1],[1,0,0,1]):\n    metrics.add(predictions=pred, references=y)\nmetrics.compute()\n\n# .add 메소드는 입력받는 값들이 스칼라 값이어야 한다. 한 번에 하나의 예측값과 정답을 추가.\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n- zip()을 사용하여 references(정답)과 predictions(예측값)를 묶음\n\nfor y,preds in zip([[0,1],[0,1]],[[1,0],[0,1]]):\n    metrics.add_batch(predictions=preds, references=y)\nmetrics.compute()\n\n# .add_batch 메소드는 입력받는 값들이 리스트(배치단위)여야 한다. 한 번에 여러 개의 예측값과 정답을 추가\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}"
  },
  {
    "objectID": "posts/Evaluate.html#evaluate",
    "href": "posts/Evaluate.html#evaluate",
    "title": "Evaluate",
    "section": "",
    "text": "Evaluate 클래스는 accuracy, F1 score, precision, recall, BLEU, ROUGE 등등 다양한 평가 지표를 간단하게 불러와 활용할 수 있다.\n\nimport evaluate\nacc = evaluate.load('accuracy')\n\n\n\n\n계산을 해보는 예시코드이다.\n\nmetrics = evaluate.combine(['accuracy','f1','precision','recall'])\nmetrics.compute(predictions=[1,0,0,1], references = [0,1,0,1])\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n\nfor y,pred in zip([0,1,0,1],[1,0,0,1]):\n    metrics.add(predictions=pred, references=y)\nmetrics.compute()\n\n# .add 메소드는 입력받는 값들이 스칼라 값이어야 한다. 한 번에 하나의 예측값과 정답을 추가.\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n\n\n- zip()을 사용하여 references(정답)과 predictions(예측값)를 묶음\n\nfor y,preds in zip([[0,1],[0,1]],[[1,0],[0,1]]):\n    metrics.add_batch(predictions=preds, references=y)\nmetrics.compute()\n\n# .add_batch 메소드는 입력받는 값들이 리스트(배치단위)여야 한다. 한 번에 여러 개의 예측값과 정답을 추가\n\n{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}"
  },
  {
    "objectID": "posts/Evaluate.html#create-custom-metrics",
    "href": "posts/Evaluate.html#create-custom-metrics",
    "title": "Evaluate",
    "section": "Create custom metrics",
    "text": "Create custom metrics\n- 딕셔너리 형태로 반환되는 구조의 함수여야 Trainer 클래스의 매개변수인 compute_metrics에 입력하여 사용할 수 있다.\n\n# 정확도를 계산하는 간단한 함수\ndef simple_accuracy(preds,labels):\n    return {'accuracy': (preds == labels).to(float).mean().item()}"
  },
  {
    "objectID": "posts/Evaluate.html#trainer-적용",
    "href": "posts/Evaluate.html#trainer-적용",
    "title": "Evaluate",
    "section": "Trainer 적용",
    "text": "Trainer 적용\n\n# micro f1 score 사용\nimport evaluate\n\ndef custom_metrics(pred):\n    f1 = evaluate.load('f1')\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(axis = -1) # pred.predictions를 수행하면 logits값이 출력된다. 그 중 가장 큰 값을 가지는 인덱스를 반환하는 함수\n    return f1.compute(predictions = preds, references = labels, average = 'micro')\n          #.compute() 함수는 자동으로 결과를 딕셔너리형태로 출력한다.\n\n- .argmax(?)\naxis = -1을 쓰면 배열의 마지막 축에서 계산하는 것. axis=0은 열(세로), aixis= 1은 행(가로)이다.\n위에서 pred.predcitions을 했을 때 나오는 결과가 2차원이기에 마지막 축이란 행이 되므로 axis = -1과 axis = 1은 같은 코드이다.\n\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    default_data_collator\n)\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)\n\ndataset = load_dataset(\"klue\", \"ynat\")\n\ndef tokenize_function(sample):\n    result = tokenizer(\n        sample[\"title\"],\n        padding=\"max_length\",\n    )\n    return result\n\ndatasets = dataset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=1000,\n    remove_columns=[\"guid\", \"title\", \"url\", \"date\"]\n)\nprint(datasets)\n\nargs = TrainingArguments(\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=5e-5,\n    max_steps=500,\n    evaluation_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    logging_dir=\"/content/logs\",\n    save_strategy=\"steps\",\n    save_steps=50,\n    output_dir=\"/content/ckpt\",\n    report_to=\"tensorboard\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=datasets[\"train\"],\n    eval_dataset=datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=default_data_collator,\n    compute_metrics=custom_metrics, # 이 부분을 바꿔준다.\n)\n\nloading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.11.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\nloading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\nloading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\nloading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\nloading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.11.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.11.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\nSome weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nusing `logging_steps` to initialize `eval_steps` to 50\nPyTorch: setting up devices\nmax_steps is given, it will override any value given in num_train_epochs\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 45678\n    })\n    validation: Dataset({\n        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9107\n    })\n})\n\n\n\ntrainer.train()\n\n***** Running training *****\n  Num examples = 45678\n  Num Epochs = 1\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 500\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-50\nConfiguration saved in /content/ckpt/checkpoint-50/config.json\nModel weights saved in /content/ckpt/checkpoint-50/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-50/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-50/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-100\nConfiguration saved in /content/ckpt/checkpoint-100/config.json\nModel weights saved in /content/ckpt/checkpoint-100/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-100/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-100/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-150\nConfiguration saved in /content/ckpt/checkpoint-150/config.json\nModel weights saved in /content/ckpt/checkpoint-150/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-150/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-150/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-200\nConfiguration saved in /content/ckpt/checkpoint-200/config.json\nModel weights saved in /content/ckpt/checkpoint-200/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-200/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-200/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-250\nConfiguration saved in /content/ckpt/checkpoint-250/config.json\nModel weights saved in /content/ckpt/checkpoint-250/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-250/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-250/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-300\nConfiguration saved in /content/ckpt/checkpoint-300/config.json\nModel weights saved in /content/ckpt/checkpoint-300/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-300/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-300/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-350\nConfiguration saved in /content/ckpt/checkpoint-350/config.json\nModel weights saved in /content/ckpt/checkpoint-350/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-350/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-350/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-400\nConfiguration saved in /content/ckpt/checkpoint-400/config.json\nModel weights saved in /content/ckpt/checkpoint-400/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-400/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-400/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-450\nConfiguration saved in /content/ckpt/checkpoint-450/config.json\nModel weights saved in /content/ckpt/checkpoint-450/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-450/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-450/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 9107\n  Batch size = 16\nSaving model checkpoint to /content/ckpt/checkpoint-500\nConfiguration saved in /content/ckpt/checkpoint-500/config.json\nModel weights saved in /content/ckpt/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in /content/ckpt/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in /content/ckpt/checkpoint-500/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\n\n\n    \n      \n      \n      [500/500 18:44, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\nF1\n\n\n\n\n50\n1.082100\n0.825163\n0.702427\n\n\n100\n0.571100\n0.593032\n0.810695\n\n\n150\n0.476000\n0.570962\n0.816844\n\n\n200\n0.500600\n0.535641\n0.815966\n\n\n250\n0.454800\n0.501376\n0.833535\n\n\n300\n0.433800\n0.479584\n0.837158\n\n\n350\n0.397700\n0.483717\n0.842868\n\n\n400\n0.442900\n0.449807\n0.851104\n\n\n450\n0.420800\n0.434349\n0.853300\n\n\n500\n0.406100\n0.438009\n0.853080\n\n\n\n\n\n\n\nTrainOutput(global_step=500, training_loss=0.5185918083190918, metrics={'train_runtime': 1125.3103, 'train_samples_per_second': 7.109, 'train_steps_per_second': 0.444, 'total_flos': 2104982937600000.0, 'train_loss': 0.5185918083190918, 'epoch': 0.18})\n\n\n- 결과에서 이전에는 볼 수 없었던 F1 score이 보인다."
  },
  {
    "objectID": "posts/About_Transformers.html",
    "href": "posts/About_Transformers.html",
    "title": "About Transformers",
    "section": "",
    "text": "임베딩 레이어: 입력 데이터를 고정된 차원의 연속형 벡터로 변환합니다.\n위치 인코딩: 단어의 순서를 반영하기 위해 사인 및 코사인 함수를 이용한 위치 인코딩을 추가한다.\n최종 입력: 입력 임베딩과 위치 인코딩을 더한 값을 인코더에 전달\n\n\n\n\n\n인코더는 여러 개의 인코더 레이어로 이루어지며, 각 인코더 레이어는 다음을 포함한다.\n\n\n멀티-헤드 셀프 어텐션 (Multi-Head Self-Attention)\n\n입력 문장에서 각 단어가 문맥을 반영해 서로 다른 중요도를 가질 수 있도록 어텐션을 적용합니다.\n(입력 임베딩 + 위치 인코딩) -&gt; 쿼리(Q), 키(K), 밸류(V) 생성 -&gt; 어텐션 결과 출력\n\n잔차 연결 (Residual Connection) + Layer Normalization\n\n입력 값(입력 임베딩 + 위치 인코딩)과 어텐션 결과를 더한 후 정규화 적용 (이상하다고 생각 가능 -&gt; 밑에서 설명)\n\n피드포워드 네트워크 (Feedforward Network, FFN)\n\n비선형 변환을 수행하는 두 개의 완전연결층(FC)으로 구성된다.\n비선형 변환이란 ReLU와 같은 활성화 함수를 의미한다.\n비선형 변환을 이용하는 이유는 학습의 다양성과 안정성을 높이기 위해서이다. 선형 변환만으로는 곱셈과 덧셈으로만 나타내기에 함수의 형태가 단순한데 비선형 변환을 통해 더 복잡한 함수와 관계를 학습할 수 있다.\n\n잔차 연결 + Layer Normalization\n\nFFN의 출력을 다시 입력과 더한 후 정규화합니다. (2번과 비슷하게 입력 값을 다시 더해준다)\n그 후 정규화를 한다. 즉, LayerNorm(FFN Output + Attention Output)\n\n\n\n이러한 인코더 레이어를 여러 개 쌓아 깊은 표현 학습을 가능하게 합니다.\n\n- 이상한 부분의 보충설명\nQ. 어텐션 결과라는 것은 입력값을 멀티-헤드 셀프 어텐션을 거쳐서 나온 결과잖아요. 그런데 그 결과랑 입력 값을 또 더한다구요? 왜 또 더해요?\n\nA. 어텐션의 출력은 입력을 바탕으로 생성된 값인데, 다시 입력과 더하는 것이 이상하게 보일 수 있음. 그 이유는 잔차 연결(Residual Connection)**은 신경망이 깊어질수록 발생하는 학습 어려움을 해결하기 위해 도입된 기법입니다. 만약 어텐션 출력만 다음 레이어로 넘긴다면, 원래 입력 정보가 손실될 수 있음. 따라서, 원래 입력 정보를 유지하면서도, 어텐션이 학습한 새로운 정보(출력)를 추가적으로 반영하기 위해 입력 + 어텐션 출력을 더함.\n\n잔차 연결이 없다면? : 인코더/디코더가 깊어질수록 입력 정보가 왜곡될 가능성이 커진다. 그러므로 입력 정보(원래 값) + 어텐션 결과(새로운 정보)를 함께 유지하는 것이 학습 안정성 측면에서 유리함. 이것이 잔차 연결의 핵심 개념이다.\n\n\n\n\n디코더도 여러 개의 디코더 레이어로 구성되며, 인코더와 다른 점은 인코더-디코더 어텐션 레이어가 추가된다는 것입니다.\n각 디코더 레이어는 다음을 포함합니다.\n\n\n마스크드 멀티-헤드 셀프 어텐션 (Masked Multi-Head Self-Attention)\n\n디코더는 정답 데이터를 예측하는 과정에서 앞쪽 단어만 참고할 수 있도록 마스크를 적용하여 미래 정보를 차단합니다.\n미래 정보(미래에 나올 단어)를 미리 안다면 정답지를 보고 컨닝을 하는 것과 같기에 올바론 학습이 되 지 않는다.\n마스크드 멀티 헤드 어텐션도 인코더의 멀티 헤드 어텐션과 똑같이 Q,K,V 값 계산 -&gt; 어텐션 결과 출력\n\n잔차 연결 + Layer Normalization\n\n인코더 구조에서 설명한 것과 같음.\n\n인코더-디코더 어텐션 (Encoder-Decoder Attention)\n\n인코더에서 출력된 벡터를 사용하여 입력 문장과 현재 디코더 상태 간의 관계를 학습합니다.\n디코더만 사용하는 모델(GPT)는 이 층을 사용할 수 없다. 인코더에서 나온 출력을 이용하기에 인코더와 디코더가 모두 있는 모델에서만 이 층을 사용한다.\n\n잔차 연결 + Layer Normalization\n\n3에서 추가된 인코더-디코더 어텐션 층이 있기에 잔차 연결 + Layer Normalizaiton 층이 추가된다.\n\n피드포워드 네트워크 (FFN)\n잔차 연결 + Layer Normalization\n\n\n\n\n\n선형 변환 (Linear Layer): 디코더에서 나온 출력을 단어 집합 크기만큼의 차원으로 변환합니다.\n소프트맥스 (Softmax): 확률 분포를 구해 최종적으로 가장 확률이 높은 단어를 예측합니다.\n\n\n\n\n\n입력 데이터 처리 (임베딩 + 위치 인코딩)\n인코더 (여러 개의 인코더 레이어) -멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화 → FFN → 잔차 연결 + 정규화\n디코더 (여러 개의 디코더 레이어) -마스크드 멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화 -인코더-디코더 어텐션 → 잔차 연결 + 정규화 -FFN → 잔차 연결 + 정규화\n출력 변환 (선형 레이어 → 소프트맥스 → 예측)"
  },
  {
    "objectID": "posts/About_Transformers.html#입력-데이터-전처리",
    "href": "posts/About_Transformers.html#입력-데이터-전처리",
    "title": "About Transformers",
    "section": "",
    "text": "임베딩 레이어: 입력 데이터를 고정된 차원의 연속형 벡터로 변환합니다.\n위치 인코딩: 단어의 순서를 반영하기 위해 사인 및 코사인 함수를 이용한 위치 인코딩을 추가한다.\n최종 입력: 입력 임베딩과 위치 인코딩을 더한 값을 인코더에 전달"
  },
  {
    "objectID": "posts/About_Transformers.html#인코더-구조-여러-개의-인코더-레이어-스택",
    "href": "posts/About_Transformers.html#인코더-구조-여러-개의-인코더-레이어-스택",
    "title": "About Transformers",
    "section": "",
    "text": "인코더는 여러 개의 인코더 레이어로 이루어지며, 각 인코더 레이어는 다음을 포함한다.\n\n\n멀티-헤드 셀프 어텐션 (Multi-Head Self-Attention)\n\n입력 문장에서 각 단어가 문맥을 반영해 서로 다른 중요도를 가질 수 있도록 어텐션을 적용합니다.\n(입력 임베딩 + 위치 인코딩) -&gt; 쿼리(Q), 키(K), 밸류(V) 생성 -&gt; 어텐션 결과 출력\n\n잔차 연결 (Residual Connection) + Layer Normalization\n\n입력 값(입력 임베딩 + 위치 인코딩)과 어텐션 결과를 더한 후 정규화 적용 (이상하다고 생각 가능 -&gt; 밑에서 설명)\n\n피드포워드 네트워크 (Feedforward Network, FFN)\n\n비선형 변환을 수행하는 두 개의 완전연결층(FC)으로 구성된다.\n비선형 변환이란 ReLU와 같은 활성화 함수를 의미한다.\n비선형 변환을 이용하는 이유는 학습의 다양성과 안정성을 높이기 위해서이다. 선형 변환만으로는 곱셈과 덧셈으로만 나타내기에 함수의 형태가 단순한데 비선형 변환을 통해 더 복잡한 함수와 관계를 학습할 수 있다.\n\n잔차 연결 + Layer Normalization\n\nFFN의 출력을 다시 입력과 더한 후 정규화합니다. (2번과 비슷하게 입력 값을 다시 더해준다)\n그 후 정규화를 한다. 즉, LayerNorm(FFN Output + Attention Output)\n\n\n\n이러한 인코더 레이어를 여러 개 쌓아 깊은 표현 학습을 가능하게 합니다.\n\n- 이상한 부분의 보충설명\nQ. 어텐션 결과라는 것은 입력값을 멀티-헤드 셀프 어텐션을 거쳐서 나온 결과잖아요. 그런데 그 결과랑 입력 값을 또 더한다구요? 왜 또 더해요?\n\nA. 어텐션의 출력은 입력을 바탕으로 생성된 값인데, 다시 입력과 더하는 것이 이상하게 보일 수 있음. 그 이유는 잔차 연결(Residual Connection)**은 신경망이 깊어질수록 발생하는 학습 어려움을 해결하기 위해 도입된 기법입니다. 만약 어텐션 출력만 다음 레이어로 넘긴다면, 원래 입력 정보가 손실될 수 있음. 따라서, 원래 입력 정보를 유지하면서도, 어텐션이 학습한 새로운 정보(출력)를 추가적으로 반영하기 위해 입력 + 어텐션 출력을 더함.\n\n잔차 연결이 없다면? : 인코더/디코더가 깊어질수록 입력 정보가 왜곡될 가능성이 커진다. 그러므로 입력 정보(원래 값) + 어텐션 결과(새로운 정보)를 함께 유지하는 것이 학습 안정성 측면에서 유리함. 이것이 잔차 연결의 핵심 개념이다."
  },
  {
    "objectID": "posts/About_Transformers.html#디코더-구조-여러-개의-디코더-레이어-스택",
    "href": "posts/About_Transformers.html#디코더-구조-여러-개의-디코더-레이어-스택",
    "title": "About Transformers",
    "section": "",
    "text": "디코더도 여러 개의 디코더 레이어로 구성되며, 인코더와 다른 점은 인코더-디코더 어텐션 레이어가 추가된다는 것입니다.\n각 디코더 레이어는 다음을 포함합니다.\n\n\n마스크드 멀티-헤드 셀프 어텐션 (Masked Multi-Head Self-Attention)\n\n디코더는 정답 데이터를 예측하는 과정에서 앞쪽 단어만 참고할 수 있도록 마스크를 적용하여 미래 정보를 차단합니다.\n미래 정보(미래에 나올 단어)를 미리 안다면 정답지를 보고 컨닝을 하는 것과 같기에 올바론 학습이 되 지 않는다.\n마스크드 멀티 헤드 어텐션도 인코더의 멀티 헤드 어텐션과 똑같이 Q,K,V 값 계산 -&gt; 어텐션 결과 출력\n\n잔차 연결 + Layer Normalization\n\n인코더 구조에서 설명한 것과 같음.\n\n인코더-디코더 어텐션 (Encoder-Decoder Attention)\n\n인코더에서 출력된 벡터를 사용하여 입력 문장과 현재 디코더 상태 간의 관계를 학습합니다.\n디코더만 사용하는 모델(GPT)는 이 층을 사용할 수 없다. 인코더에서 나온 출력을 이용하기에 인코더와 디코더가 모두 있는 모델에서만 이 층을 사용한다.\n\n잔차 연결 + Layer Normalization\n\n3에서 추가된 인코더-디코더 어텐션 층이 있기에 잔차 연결 + Layer Normalizaiton 층이 추가된다.\n\n피드포워드 네트워크 (FFN)\n잔차 연결 + Layer Normalization"
  },
  {
    "objectID": "posts/About_Transformers.html#출력-처리",
    "href": "posts/About_Transformers.html#출력-처리",
    "title": "About Transformers",
    "section": "",
    "text": "선형 변환 (Linear Layer): 디코더에서 나온 출력을 단어 집합 크기만큼의 차원으로 변환합니다.\n소프트맥스 (Softmax): 확률 분포를 구해 최종적으로 가장 확률이 높은 단어를 예측합니다."
  },
  {
    "objectID": "posts/About_Transformers.html#요약-순서-정리",
    "href": "posts/About_Transformers.html#요약-순서-정리",
    "title": "About Transformers",
    "section": "",
    "text": "입력 데이터 처리 (임베딩 + 위치 인코딩)\n인코더 (여러 개의 인코더 레이어) -멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화 → FFN → 잔차 연결 + 정규화\n디코더 (여러 개의 디코더 레이어) -마스크드 멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화 -인코더-디코더 어텐션 → 잔차 연결 + 정규화 -FFN → 잔차 연결 + 정규화\n출력 변환 (선형 레이어 → 소프트맥스 → 예측)"
  }
]