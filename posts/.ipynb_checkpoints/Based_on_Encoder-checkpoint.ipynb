{
 "cells": [
  {
   "cell_type": "raw",
   "id": "440b1b3a-9c6f-4ae0-9000-412a87b9260e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Base on Encoder models\"\n",
    "author: \"차상진\"\n",
    "date: \"2025-03-30\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737dc88-b341-4c88-9803-cd421b1d9a50",
   "metadata": {},
   "source": [
    "# 1. Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9441f648-a536-4fee-8b65-a7ff39fa3680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fe8c9-ca58-4dab-bd19-fc0192be1698",
   "metadata": {},
   "source": [
    "`-` 클래스가 잘 설정되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a193b1-9251-48dc-9383-a178c0a5c430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'LABEL_0', 1: 'LABEL_1'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7077a-e864-4bb2-93c0-e6769eb1f490",
   "metadata": {},
   "source": [
    "# 1-2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dbdb6f3-02b0-46d6-a99b-d1d841344b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['guid', 'source', 'sentence1', 'sentence2', 'labels'],\n",
       "    num_rows: 11668\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('klue','sts')\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6229a8-e787-4e02-a90b-1ba080939361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(batch):\n",
    "    result = tokenizer(batch['sentence1'], text_pair=batch['sentence2'])\n",
    "    # 데이터 셋이 sentence1,sentence2으로 이루어져 있다는 것을 정확하게 알아야 이런 코드를 작성할 수 있다.\n",
    "    # text_pair은 두 문장간의 문장 관계 분석을 위해서 추가되는 옵션이다.\n",
    "    result['labels'] = [l['binary-label'] for l in batch['labels']]\n",
    "    return result\n",
    "\n",
    "dataset = dataset.map(\n",
    "    process_data,\n",
    "    batched = True,\n",
    "    remove_columns = dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2663781-76b6-43b1-9f49-37d96332545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "batch = collator([dataset['train'][l] for l in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e695e5d-80ed-476b-bd9f-1ff7417e7d54",
   "metadata": {},
   "source": [
    "## 1-3. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1cf260e-4a87-479c-a5fe-b82807265d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6647,  0.5699],\n",
       "        [ 0.2459, -0.5885],\n",
       "        [-0.3337,  0.2817],\n",
       "        [ 0.1649,  0.0235],\n",
       "        [-0.5286,  0.4380],\n",
       "        [ 0.7408,  0.0513],\n",
       "        [-0.3665,  0.6204],\n",
       "        [ 0.6414, -0.6416],\n",
       "        [ 0.1279, -0.2553],\n",
       "        [-0.3801,  0.3907]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**batch).logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf6e990c-04bc-4121-a1db-b9813fd099f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 1 0 0 1]\n",
      "[1 0 0 0 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "pred_labels = logits.argmax(dim=1).cpu().numpy()\n",
    "true_labels = batch['labels'].numpy()\n",
    "print(pred_labels)\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8df8d71d-556a-434a-a6ab-6553836eee2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.9}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "f1 = evaluate.load('f1')\n",
    "f1.compute(predictions = pred_labels, references = true_labels, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250885d-49a4-4b33-b5d7-089ba645b17f",
   "metadata": {},
   "source": [
    "# 2. Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0c665-ec26-4821-8e74-60ffd477dd9e",
   "metadata": {},
   "source": [
    "`-` 분류 모델이 반환하는 것은 '확률'이 아니라 '로짓(logits)'이다.\n",
    "\n",
    "- 일반적으로 분류모델은 마지막 층에서 softmax를 적용하지 않는다. 대신 logits, 즉 스케일링되지 않은 점수 값을 반환한다.\n",
    "    위에서 말한 스케일링의 의미는 통계학에서의 표준화와는 다른 개념이다.\n",
    "    여기서 말하는 스케일링은 0과 1 사이의 확률값으로 변환하는 softmax이다. 통계학에서 표준화는 평균이 0이고 분산이 1이 되도록 만드는 것이다.\n",
    "    머신러닝에서는 sciket-learn의 MinMaxScaler는 softmax이고, StandardScaler은 통계학의 표준화이다.\n",
    "\n",
    "그렇다면 왜 softmax를 적용하지 않고 logits 값만 반환할까?\n",
    "- 수치적으로 더 안정적이고 loss 계산이 더 쉽기 때문이다. \n",
    "- 만약 로짓 값이 매우 큰 경우(예: 1000, 2000)라면, 소프트맥스 계산 중에 **지수 함수(exp(x))**로 변환하면 매우 큰 숫자가 나오고, 이는 컴퓨터에서 처리할 수 있는 범위를 넘을 수 있다.\n",
    "- 손실함수(ex.Cross Entropy Loss)에 logits이 아니라 softmax로 계산된 확률값을 넣는다면 값이 너무 작아지는 underflow가 일어날 수 있기 때문이다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415561a-b140-47fa-b490-2eeb9df3e22c",
   "metadata": {},
   "source": [
    "`-` Sequence Classification에서 num_labels = 1으로 설정하면 연속적인 실수를 예측하는 회귀(Regression) 문제에 사용이 가능하다.\n",
    "\n",
    "- 출력 차원이 1인 로짓 값이 나오는데, 이 값은 회귀 문제의 예측값이 될 수 있다.\n",
    "- num_labels=1으로 설정하면 자동으로 회귀 태스크로 인식하여 크로스 엔트로피가 아닌 MSE를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79dfac7c-7c11-4179-86c0-cb13b1ccbb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7c3d5-6c0d-4dbb-bbc1-9c925bded24c",
   "metadata": {},
   "source": [
    "## 2-2. Predcition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb51a3b-9d76-42fe-9a48-b8eb32e06f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0012],\n",
       "        [-0.2672],\n",
       "        [ 0.1028],\n",
       "        [-0.3430],\n",
       "        [-0.0204],\n",
       "        [-0.2547],\n",
       "        [ 0.0901],\n",
       "        [-0.6130],\n",
       "        [-0.4369],\n",
       "        [ 0.0066]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**batch).logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b67e2-1c45-49ae-8f00-bad292c453c0",
   "metadata": {},
   "source": [
    "# 3. Multiple Choice model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8641d64-2455-4123-8451-f2ec190d0eb8",
   "metadata": {},
   "source": [
    "`-` 여러 개의 입력이 주어졌을 때, 주어진 문장 중 옳은 문장을 고르는 객관식 문제."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bdf83-5449-479e-a530-99d09ce47e72",
   "metadata": {},
   "source": [
    "ex) 일반적인 객관식 문제\n",
    "\n",
    "Q: 뉴턴의 운동 법칙 중 첫 번째 법칙은 무엇인가?\n",
    "\n",
    "(A) 힘은 질량과 가속도의 곱이다.\n",
    "\n",
    "(B) 모든 물체는 외부에서 힘이 가해지지 않는 한 정지 또는 등속 운동을 유지한다.\n",
    "\n",
    "(C) 모든 작용에는 크기가 같고 반대 방향인 반작용이 있다.\n",
    "\n",
    "(D) 에너지는 생성되거나 소멸되지 않고 변환될 뿐이다.\n",
    "\n",
    "정답: (B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee2e69-259a-4cdf-aa36-30d6b12a63e2",
   "metadata": {},
   "source": [
    "Multiple Choice에서 트랜스포머 모델은 강력하다.\n",
    "\n",
    "트랜스포머는 문장이 길어질수록 연산량이 제곱으로 증가한다 (self-attention 과정)\n",
    "\n",
    "즉 트랜스포머는 긴 문장을 처리하는 것보다 여러 개의 짧은 문장을 처리하는 게 연산량 측면에서 유리하다.\n",
    "\n",
    "그런데 Multiple Choice은 짧은 답변이 여러 개여서 계산이 쉽다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "196e0e40-4f1a-4508-8d90-56e5de0710c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMultipleChoice: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMultipleChoice(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "\n",
    "model_name = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ef739-44ff-4f26-be15-c7809afd8bda",
   "metadata": {},
   "source": [
    "`-` 여러 개 중 하나를 고르는 분류임에도 num_labels을 설정하지 않았다.. 왜?\n",
    "\n",
    "- 다른 task와 동일하게 모델 마지막 부분이 classifier 레이어가 하나 추가됐는데, Multiple Choice에서는 sample(문제)당 여러 개의 후보를 각각 문장으로 입력받기에 임베딩 과정을 거치면서 후보 개수, 문장 길이, 임베딩 사이즈로 총 3차원으로 이루어진다.\n",
    "- 여기서 배치처리까지 하면 4차원 데이터를 가지므로 사용이 힘들어진다.\n",
    "- 그래서 flatten을 적용하여 문장을 3차원으로 바꾸고 추론을 하고 다시 원상태 (4차원)로 복구한다.\n",
    "- flatten된 데이터는 문장당 0~1 사이 확률 값을 하나만 가지면 되므로 자동으로 num_labels 수는 1로 고정된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e4c15-8a3b-4ac4-a791-769a8ca04af2",
   "metadata": {},
   "source": [
    "`할 수 있는 질문`\n",
    "\n",
    "질문 1. flatten() 하면 1차원이 되는 거 아닌가? 어떻게 3차원이 돼?\n",
    "- 일반적으로 flatten()은 1차원이 되지만 여기서는 특정 축을 기준으로 차원을 줄이는 방식을 사용.\n",
    "- `(batch_size, num_choices, seq_length, hidden_dim)` -> `(batch_size * num_choices, seq_length, hidden_dim)`\n",
    "\n",
    "질문 2. 문장당 0~1 사이 확률값을 하나만 가지면 되므로 num_labels은 1이다?\n",
    "- 보통 분류 태스크에서 num_labels = 3이라고 한다면 softmax를 적용해서 여러 클래스 중 하나를 선택 (클래스 별로 확률을 출력함)\n",
    "- Multiple Choice는 정답일 확률만 출력하면 된다. 그 중 가장 큰 것을 선택하면 됨. 즉, 문장 하나에 대해 스칼라 값 하나만 출력하면 되므로 num_labes = 1이다. (회귀 문제와 같다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a04424-c1a7-42ac-a60a-46ea51f1f09d",
   "metadata": {},
   "source": [
    "## 문장분류 vs 다중 분류\n",
    "\n",
    "문장 분류: **문장 한 개당 N개의 확률 출력** (N = 클래스의 수)\n",
    "\n",
    "다중 분류: **N개의 문장을 입력받아 문장당 한 개씩, 총 N개 확률 추출** (N = 객관식 보기 개수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883aff72-e8de-4fe6-8907-38eaf18c68dd",
   "metadata": {},
   "source": [
    "## 3-2. Dataset\n",
    "\n",
    "수능 국어 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01c82a38-7659-4667-a367-2e02a5e9e520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ' 이 이야기에서 얻을 수 있는 교훈으로 가장 적절한 것은?', 'context': '이제 한 편의 이야기를 들려 드립니다. 잘 듣고 물음에 답하십시오.\\n자, 여러분! 안녕하십니까? 오늘은 제가 어제 꾼 꿈 이야기 하날 들려 드리겠습니다. 전 꿈속에서 낯선 거리를 걷고 있었습니다. 그러다가 홍미로운 간판을 발견했답니다. 행 복을 파는 가게. 그렇게 쓰여 있었습니다. 전 호기심으로 문을 열고 들어갔답니다. 그곳 에서는 한 노인이 물건을 팔고 있었습니다. 전 잠시 머뭇거리다가 노인에게 다가가서 물 었습니다. 여기서는 무슨 물건을 파느냐고요. 노인은 미소를 지으며, 원하는 것은 뭐든 다 살 수 있다고 말했습니다. 저는 제 귀를 의심했습니다. \\'무엇이든 다?\\' 전 무엇을 사야 할까 생각하다가 말했답니다. \"사랑, 부귀 그리고 지혜하고 건강도 사고 싶습니다. 저 자신뿐 아니라 우리 가족 모두 를 위해서요. 지금 바로 살 수 있나요?\" 그러자 노인은 빙긋이 웃으며 대답했습니다. \"젊은이, 한번 잘 보게나. 여기에서 팔고 있는 것은 무르익은 과일이 아니라 씨앗이라 네. 앞으로 좋은 열매를 맺으려면 이 씨앗들을 잘 가꾸어야 할 걸세.\"', 'option#1': '새로운 세계에 대한 열망을 가져야 한다.', 'option#2': '주어진 기회를 능동적으로 활용해야 한다.', 'option#3': '큰 것을 얻으려면 작은 것은 버려야 한다.', 'option#4': '물질적 가치보다 정신적 가치를 중시해야 한다.', 'option#5': '소망하는 바를 성취하기 위해서는 노력을 해야 한다.', 'gold': 5, 'category': 'N/A', 'human_performance': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f29dd109c54b5c93f9513ca0cfb089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HAERAE-HUB/csatqa\", \"full\")\n",
    "print(dataset[\"test\"][0])\n",
    "\n",
    "ending_names = [\"option#1\", \"option#2\", \"option#3\", \"option#4\", \"option#5\"]\n",
    "\n",
    "def preprocess_function(examples): # examples 자리에 dataset의 batch가 들어간다.\n",
    "  first_sentences = [\n",
    "      [context] * 5 for context in examples[\"context\"] # 각 문항에 5개의 선택지가 있다. 각 선택지마다 동일한 context를 사용해야함.\n",
    "  ]\n",
    "  question_headers = examples[\"question\"]\n",
    "  second_sentences = [\n",
    "      [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers) # 각 질문과 선택지를 하나로 합치는 역할\n",
    "      # 1. enumerate(question_headers) → question_headers에서 (index, question_text) 쌍을 가져옴.\n",
    "      # 2. for end in ending_names → \"option#1\" ~ \"option#5\"까지 돌면서 해당 선택지를 가져옴.\n",
    "      # 3. f\"{header} {examples[end][i]}\" → 각 질문(header)과 해당 선택지를 합친 새로운 문장을 생성.\n",
    "  ]\n",
    "  # 토큰화를 위해 1차원으로 평활화\n",
    "  first_sentences = sum(first_sentences, []) # flatten()과 같은 효과. flatten()은 numpy에서 동작하므로 리스트에서는 sum(리스트, []) 사용\n",
    "  second_sentences = sum(second_sentences, [])\n",
    "\n",
    "  # None 데이터 처리\n",
    "  first_sentences = [i if i else \"\" for i in first_sentences] # sentences에서 None을 공백으로 바꾸는 코드. 즉, None 데이터를 처리해서 모델이 학습할 수 있게 함\n",
    "  second_sentences = [i if i else \"\" for i in second_sentences]\n",
    "\n",
    "  tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "  # Multiple Choice 문제에서는 질문 + 각 답변을 결합해야한다. 결합하고 싶은 문장을 이어서 작성한다면 알아서 결합된다.\n",
    "\n",
    "  # 토큰화 후 다시 2차원으로 재배열\n",
    "  result = {\n",
    "      k: [v[i:i+5] for i in range(0, len(v), 5)] for k, v in tokenized_examples.items()\n",
    "  }\n",
    "  result[\"labels\"] = [i-1 for i in examples[\"gold\"]]  # k는 문제(문제와 보기), v는 선택지 5개이다. 보기 좋은 2차원 배열로 재배열\n",
    "\n",
    "  return result\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a3d285-5fc3-4fc6-a712-d6217e49ef65",
   "metadata": {},
   "source": [
    "다중 분류 task에서는 일반적으로 사용하는 DataCollatorWithPadding을 사용하기 어렵다.\n",
    "\n",
    "이를 위해 패딩 등 필요한 작업을 진행하는 콜레이터를 직접 작성해야한다.\n",
    "\n",
    "그 전 작성된 콜레이터를 이해하기 위해선 아래의 문법을 알아야한다. 간략하게 설명할테니 숙지하고 넘어가도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3c259-b4f9-419d-9d5c-74c6975f5856",
   "metadata": {},
   "source": [
    "## 번외1. 파이썬 문법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f1c2d-6020-4f3e-8415-01bd5d74dadf",
   "metadata": {},
   "source": [
    "1. `__init__()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b11e8-12c6-42ae-88e3-b0f03e4924e5",
   "metadata": {},
   "source": [
    "객체 지향 프로그래밍에서 클래스를 만들면 해당 클래스의 **객체(인스턴스)**를 생성할 때 자동으로 호출되는 메서드가 `__init__()`이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4268b27b-1416-4efb-95d8-dcdc723a2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4d0bf-b86b-41c1-a89b-5d2be2b8b392",
   "metadata": {},
   "source": [
    "예를 들어 위와 같은 클래스를 만든다고 했을 때, `__init__()` 메서드는 클래스를 처음 만들 때 자동으로 실행된다.\n",
    "\n",
    "`self.a = a` -> `a`값을 객체 내부에 저장\n",
    "\n",
    "`self.b = b` -> `b`값을 객체 내부에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cfcf3fb-bd87-4e6f-b4e5-b3290a54d7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "obj = Example(3,5)\n",
    "print(obj.a)\n",
    "print(obj.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53988f4a-39eb-4b9f-a431-9cef699ff642",
   "metadata": {},
   "source": [
    "즉 `__init__()`은 클래스를 만들 때 필요한 변수를 초기화하는 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0147d1b-3e38-4d8e-b85f-eae183a34ab8",
   "metadata": {},
   "source": [
    "2. `데코레이터` + `dataclass`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d380a-625b-449c-8736-dadca5e3fbd0",
   "metadata": {},
   "source": [
    "**데코레이터(Decorator)** 는 함수나 클래스를 꾸며주는(변형하는) 함수이다. `@`을 붙혀서 사용한다.\n",
    "`@dataclass`는 클래스에서 `__init__()`을 자동으로 만들어주는 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a678b46b-02db-4b2b-ad55-ab2be161e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Example:\n",
    "    a: int\n",
    "    b: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8992915-87ad-4222-b8d8-fe55690adee1",
   "metadata": {},
   "source": [
    "위의 코드에서 `__init__()`을 따로 만들지 않았음에도 자동 생성되었고 내부적으로는 아래의 코드와 같은 방식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d04031e7-4e91-4c9c-ae87-9ba80b8e5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, a:int, b:int):\n",
    "    self.a = a\n",
    "    self.b = b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d1fba-a367-4717-909c-c78c61a9b981",
   "metadata": {},
   "source": [
    "추가적으로 `a: int, b: int` 와 같이 쓴 이유는 a와 b는 int 타입을 기대한다는 것을 알리기 위해 사용한 것이다.\n",
    "\n",
    "하지만 **a는 정수여야 한다** 는 아니므로 float을 입력해도 에러는 나지 않는다.\n",
    "\n",
    "즉, 권장사항이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fc7cc-feca-48e4-a256-63901e170416",
   "metadata": {},
   "source": [
    "`@dataclass` 말고도 `@attrs` 등 많은 기능을 제공하는 다른 라이브러리들이 많다. 필요한 것을 골라서 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515ff59c-ff88-490a-a6ae-c0460cab91a7",
   "metadata": {},
   "source": [
    "## 번외 2.`Union`, `Optional`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3b1e9-c1c6-4956-b334-79afc3a97939",
   "metadata": {},
   "source": [
    "1. `Union`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e203ce69-ca49-40c6-bc4c-1246041fab7d",
   "metadata": {},
   "source": [
    "`Union`과 `Optional`은 **타입 힌트** 에서 사용되는 개념이다. Python의 타입 시스템에서 변수나 함수가 가질 수 있는 값을 더 명확하게 지정하는데 사용된다.\n",
    "\n",
    "- `Union`\n",
    "    - `Union`은 \"이 변수는 여러 타입 중 하나일 수 있다\" 는 뜻이다. 예를 들어 `Union[int,float]`이라면 해당 변수나 값이 `int`일 수도 있고 `float`일 수도 있다는 것을 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21e3ac1a-c894-4dbf-993c-78922d8d82ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "None\n",
      "<class 'float'>\n",
      "None\n",
      "<class 'bool'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def foo(x: Union[int, float]) -> None:\n",
    "    print(x)\n",
    "\n",
    "print(foo(int)) # 당연히 가능\n",
    "print(foo(float)) # 당연히 가능\n",
    "print(foo(bool)) # int,float이 제한사항이 아니라 권장사항이므로 bool도 당연히 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c02e1-34c2-44c1-b654-220ce829eee3",
   "metadata": {},
   "source": [
    "2. `Optional`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4b640-587b-416f-a4c1-199b397da510",
   "metadata": {},
   "source": [
    "- `Optional`\n",
    "    - 'Optional[X]' = `Union[X, None]` 즉, 해당 값이 `X`일 수도 있고, `None`일 수도 있다는 의미이다.\n",
    "  - 'Optional'을 사용하면 값이 `None`일 수 있다는 것을 명시적으로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a34e9ce-e10a-4208-9c68-60ab74270d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "None\n",
      "<class 'float'>\n",
      "None\n",
      "<class 'bool'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def foo(x: Optional[int]) -> None:\n",
    "    print(x)\n",
    "\n",
    "print(foo(int)) # 당연히 가능\n",
    "print(foo(float)) # 당연히 가능\n",
    "print(foo(bool)) # int,float이 제한사항이 아니라 권장사항이므로 bool도 당연히 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f575c-1a24-4cce-a0b5-4a0756452917",
   "metadata": {},
   "source": [
    "## 3-3. Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c046e-b4d7-48a8-9bb7-6db76a094929",
   "metadata": {},
   "source": [
    "`Collator`는 배치를 만들기 위한 객체이고 `batch`는 그 결과물이다.\n",
    "\n",
    "`batcg`를 `model(**batch)`로 넣으면 콜레이터에서 변경된 데이터 형식도 그대로 반영된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0765d3f-0cfc-4418-8ce6-c86afa94f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass # 데코레이터\n",
    "# @dataclass -> __init__을 자동으로 생성해주는 데이터 클래스\n",
    "# Collator는 data loader에서 batch를 구성할 때 사용. 일반적인 Collator를 padding과 tensor변환 담당\n",
    "# 하지만 다중 선택 문제에서는 input_ids가 2차원 구조이기에 일반적인 Collator를 사용할 수 없음.\n",
    "class DataCollatorForMultipleChoice:\n",
    "  tokenizer: PreTrainedTokenizerBase # 실제로 모델에 입력되는 데이터를 토크나이저로 변환하는 도구.\n",
    "  padding: Union[bool, str, PaddingStrategy] = True \n",
    "  # 입력 데이터가 고정 길이를 가지도록 패딩을 추가하는 방법을 정의한다. 기본 값은 True, 필요하면 패딩을 추가한다.\n",
    "  max_length: Optional[int] = None # 입력 시 최대 길이를 설정한다. max_length를 초과하는 토큰은 잘린다.\n",
    "  pad_to_multiple_of: Optional[int] = None # 이 값은 패딩 길이가 특정 수의 배수가 되도록 설정할 수 있다.\n",
    "  # 이 변수들은 클래스를 초기화할 때 설정할 값들로\n",
    "  def __call__(self, features): # 클래스의 인스턴스를 함수처럼 호출할 수 있도록 만듦\n",
    "    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\" # label이나 labels중 하나를 사용해서 레이블 이름을 결정한다.\n",
    "    labels = [feature.pop(label_name) for feature in features] # 각 샘플에서 레이블을 꺼내고 pop()으로 레이블을 분리\n",
    "\n",
    "    batch_size = len(features) # features의 길이를 통해 한 번에 처리하는 샘플 수(batch_size)를 결정한다\n",
    "    num_choices = len(features[0][\"input_ids\"]) # 각 샘플에 포함된 선택지 수를 결정.\n",
    "\n",
    "    # multiple choice에서 여러 개의 선택지를 평탄화(flatten)하는 과정\n",
    "    # 첫 번째 리스트 컴프리헨션은 각 샘플에 대해 선택지별로 분리한다.\n",
    "    # 두 번째 리스트 컴프리헨션은 각 샘플에 대해 평탄화하여 하나의 리스트로 만든다.\n",
    "    flattened_features = [\n",
    "        [\n",
    "            {k: v[i] for k, v in feature.items()}\n",
    "            for i in range(num_choices)\n",
    "        ]\n",
    "        for feature in features\n",
    "    ]\n",
    "    flattened_features = sum(flattened_features, []) # 중첩된 리스트를 하나로 합친다.\n",
    "\n",
    "    # 토큰화를 적용하고 다시 2차원 구조로 변환한다.\n",
    "    # flattened_features 리스트를 self.tokenizer.pad(...)에 넣어서 토큰화 수행, return_tensors = 'pt'를 이용해 파이토치 형식으로 변환\n",
    "    batch = self.tokenizer.pad(\n",
    "        flattened_features,\n",
    "        padding=self.padding,\n",
    "        max_length=self.max_length,\n",
    "        pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "        return_tensors=\"pt\",\n",
    "    ) # 이렇게 하면 각 선택지가 개별적으로 패딩되어, 입력 길이가 맞춰진다.\n",
    "\n",
    "    # 다시 배치 크기 * 선택지 개수형태로 복구한다.\n",
    "    batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()} # batch_size(문제) * num_choices(선택지)로 맞추고 -1으로 나머지는 자동으로 맞춘다.\n",
    "    batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64) # 레이블을 추가하여 정답이 몇 번째 선택지인지 알 수 있게 한다.\n",
    "    return batch\n",
    "\n",
    "collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "batch = collator([tokenized_dataset[\"test\"][i] for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ee204585-fddd-48aa-b035-99b3365dfb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0498, 0.0333, 0.2016, 0.0107, 0.1579],\n",
       "        [0.0852, 0.0705, 0.0632, 0.0507, 0.0745],\n",
       "        [0.1740, 0.1215, 0.2006, 0.2101, 0.2531],\n",
       "        [0.1829, 0.2058, 0.1865, 0.1838, 0.3799],\n",
       "        [0.2215, 0.2357, 0.2723, 0.2856, 0.3356]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  logits = model(**batch).logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36512c30-0a4f-4971-8ab3-9484b226bcb3",
   "metadata": {},
   "source": [
    "모델이 Dropout과 같은 랜덤 연산을 포함한다면 같은 모델에 같은 입력을 넣어도 logits 값은 달라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e19f2-c2aa-4123-be84-ccb58ddcbd87",
   "metadata": {},
   "source": [
    "## 3-4. evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "00e5a7a8-59a6-4f4c-905f-2c8128e58be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 4 4 4]\n",
      "[4 4 0 3 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.0}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "pred_labels = logits.argmax(dim=1).cpu().numpy()\n",
    "true_labels = batch[\"labels\"].numpy()\n",
    "print(pred_labels)\n",
    "print(true_labels)\n",
    "\n",
    "f1 = evaluate.load(\"f1\")\n",
    "f1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8253c-c345-4214-bac7-bd67ba8c075f",
   "metadata": {},
   "source": [
    "# 4.Token Classifiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7b760-8655-4ec2-9c10-438a5a618d79",
   "metadata": {},
   "source": [
    "`-` 말 그대로 토큰 단위로 분류를 진행한다. 주로 문장 내에서 유호한 개체를 추출해 내는 개체명 인식 태스크에서 가장 많이 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f92aa0-e2e0-4a38-baec-45770a16ad2d",
   "metadata": {},
   "source": [
    "## 4-1. model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d1fc5-43ea-4b4b-99d6-d3fff9bb8296",
   "metadata": {},
   "source": [
    "`-` 베이스 모델은 기본 모델인 모델명PreTrainedModel을 상속하며 `모델명ForTokenClassification`을 사용한다.\n",
    "\n",
    "다만 문장 벡터 차원을 축소하는 풀링 작업을 진행하지 않고 입력된 각 토큰에 모두 출력 헤더를 달아 독립적으로 분류를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d480cdf0-9c79-48ca-9854-ec6b66b056f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7710b5-0706-43f2-bccf-7091d03fd9e7",
   "metadata": {},
   "source": [
    "`-` (classifier): Linear(in_features=768, out_features=2, bias=True)에서 out_features=2인 이유는 분류되는 클래스의 개수가 2개이기 때문이다. 만약 더 세분화하여 구분되어야한다먼 out_features=?? ??의 수가 더 늘어나야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ebf4e-d422-44a9-9e67-da4b6dee06d4",
   "metadata": {},
   "source": [
    "## 4-2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c66d3568-3318-4c7d-8d58-36f3a3cd04d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8288307d1c914cbbb60957a90126558e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/4.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fed4eec60f34d2ea0a617855acef887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3bad53f0724968b636438bc00f46f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/21008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93808980079545f38bcf8cb9c8648d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens :  ['특', '히', ' ', '영', '동', '고', '속', '도', '로', ' ', '강', '릉', ' ', '방', '향', ' ', '문', '막', '휴', '게']\n",
      "ner tags :  [12, 12, 12, 2, 3, 3, 3, 3, 3, 12, 2, 3, 12, 12, 12, 12, 2, 3, 3, 3]\n",
      "(66, 66)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"ner\")\n",
    "\n",
    "sample = dataset[\"train\"][0]\n",
    "print(\"tokens : \", sample[\"tokens\"][: 20])\n",
    "print(\"ner tags : \", sample[\"ner_tags\"][: 20])\n",
    "print((len(sample[\"tokens\"]), len(sample[\"tokens\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ec26e51-3ced-43f5-9c14-b19985e24f7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특 \t 12\n",
      "히 \t 12\n",
      "  \t 12\n",
      "영 \t 2\n",
      "동 \t 3\n",
      "고 \t 3\n",
      "속 \t 3\n",
      "도 \t 3\n",
      "로 \t 3\n",
      "  \t 12\n",
      "강 \t 2\n",
      "릉 \t 3\n",
      "  \t 12\n",
      "방 \t 12\n",
      "향 \t 12\n",
      "  \t 12\n",
      "문 \t 2\n",
      "막 \t 3\n",
      "휴 \t 3\n",
      "게 \t 3\n",
      "소 \t 3\n",
      "에 \t 12\n",
      "서 \t 12\n",
      "  \t 12\n",
      "만 \t 2\n",
      "종 \t 3\n",
      "분 \t 3\n",
      "기 \t 3\n",
      "점 \t 3\n",
      "까 \t 12\n",
      "지 \t 12\n",
      "  \t 12\n",
      "5 \t 8\n",
      "㎞ \t 9\n",
      "  \t 12\n",
      "구 \t 12\n",
      "간 \t 12\n",
      "에 \t 12\n",
      "는 \t 12\n",
      "  \t 12\n",
      "승 \t 12\n",
      "용 \t 12\n",
      "차 \t 12\n",
      "  \t 12\n",
      "전 \t 12\n",
      "용 \t 12\n",
      "  \t 12\n",
      "임 \t 12\n",
      "시 \t 12\n",
      "  \t 12\n",
      "갓 \t 12\n",
      "길 \t 12\n",
      "차 \t 12\n",
      "로 \t 12\n",
      "제 \t 12\n",
      "를 \t 12\n",
      "  \t 12\n",
      "운 \t 12\n",
      "영 \t 12\n",
      "하 \t 12\n",
      "기 \t 12\n",
      "로 \t 12\n",
      "  \t 12\n",
      "했 \t 12\n",
      "다 \t 12\n",
      ". \t 12\n"
     ]
    }
   ],
   "source": [
    "for l in range(len(sample['ner_tags'])):\n",
    "    print(sample['tokens'][l], '\\t', sample['ner_tags'][l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a49f1-358c-4277-8f29-e65ecbb0de77",
   "metadata": {},
   "source": [
    "`-` 문자 단위로 분할된 tokens 칼럼은 이미 '토큰화'되었다고 할 수 있다. 따라서 문장 인코딩을 진행할 때 평소처럼 토큰화 - 정수 인코딩 과정을 거치지 않고 정수 인코딩 과정만 거치도록 코드를 작성해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8fa1a273-17ef-4013-be5d-9482dbe10303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e772489993e461b9535cb67b55dd035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8e05782e19406098347b49143e5cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토큰화 x , 정수 인코딩 o\n",
    "def tokenize_and_align_labels(examples): # examples : dataset.map()을 통해 받을 배치 데이터\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) # s_split_into_words=True면 토크나이저는 토큰화가 이미 진행됐다고 인식함.\n",
    "    # example['tokens'] -> [['Hello','world],['My','name','is','John']]\n",
    "    # example['ner_tags'] -> [[0,0],[0,1,0,2]] (각 단어의 라벨)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]): \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # 토큰을 해당 단어에 매핑, 추가적으로 word_ids 메서드는 word index의 줄임말이다.\n",
    "        previous_word_idx = None # 이전 단어 인덱스를 저장하여 첫 번째 토큰인지 확인\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # 스페셜 토큰을 -100으로 세팅\n",
    "            if word_idx is None: # 토큰이 None이라는 것은 현재 토큰이 특별한 토큰인 것을 나타내는 것이다. [CLS],[SEP],[PAD]일 때 None으로 출력되기 때문이다.\n",
    "                label_ids.append(12) # 12는 의미없는 토큰이라는 의미, -100은 손실계산을 하지 않기 위함 즉 12, -100 모두 자주 사용되는 값이다.\n",
    "                # label_ids.append(-100)\n",
    "                # 그런데! None이라는 건 특별한 거라면서? 왜 12나 -100을 추가해서 손실계산에서 빼?\n",
    "                # -> None은 단어에 속하지 않는 스페셜 토큰을 나타낸다. 실제 단어가 아니기에(실제 문장의 의미를 담지 않기에) 토큰화 후 해당 토큰들이 학습에서 계산에 포함되는 것은 부적절하다.\n",
    "            elif word_idx != previous_word_idx:  # 주어진 단어의 첫 번째 토큰에만 레이블을 지정\n",
    "                label_ids.append(label[word_idx])\n",
    "            else: # playing에서 play , ##ing으로 나뉜다면 첫 번째 토큰인 play는 elif 구문에서 레이블을 넣고 ##ing은 -100으로 처리한다.\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "        # if-elif-else 구조가 필요한 이유\n",
    "        # word_idx is None (스페셜 토큰 처리)\n",
    "        # [CLS], [SEP], [PAD] 같은 특별한 토큰을 손실 계산에서 제외\n",
    "        # word_idx != previous_word_idx (단어의 첫 번째 토큰)\n",
    "        # 단어의 첫 번째 토큰에만 레이블을 할당\n",
    "        # else (단어의 나머지 토큰들)\n",
    "        # 단어의 나머지 토큰들은 손실 계산에서 제외 (-100 사용)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1188ca5d-6f42-440f-ae27-7c8289d08662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8428d643-4578-41c8-8af9-6573b39da066",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"B-DT\",\n",
    "    1: \"I-DT\",\n",
    "    2: \"B-LC\",\n",
    "    3: \"I-LC\",\n",
    "    4: \"B-OG\",\n",
    "    5: \"I-OG\",\n",
    "    6: \"B-PS\",\n",
    "    7: \"I-PS\",\n",
    "    8: \"B-QT\",\n",
    "    9: \"I-QT\",\n",
    "    10: \"B-TI\",\n",
    "    11: \"I-TI\",\n",
    "    12: \"O\",\n",
    "}\n",
    "label2id = {v:k for k,v in id2label.items()} # k,v 뒤집기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2ddf97ec-6110-420f-b9d5-f4d76c2bf302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'klue/bert-base',\n",
    "    num_labels = 13,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "631259df-5109-49db-80bc-a915ef1bc400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-OG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PS',\n",
       " 'O',\n",
       " 'B-TI',\n",
       " 'B-TI',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'I-DT',\n",
       " 'B-OG',\n",
       " 'B-TI',\n",
       " 'O',\n",
       " 'I-DT',\n",
       " 'O',\n",
       " 'I-OG',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'I-QT',\n",
       " 'I-OG',\n",
       " 'B-OG',\n",
       " 'I-QT',\n",
       " 'O',\n",
       " 'B-TI',\n",
       " 'I-QT',\n",
       " 'O',\n",
       " 'I-DT',\n",
       " 'O',\n",
       " 'B-TI',\n",
       " 'B-TI',\n",
       " 'B-OG',\n",
       " 'B-TI',\n",
       " 'B-TI',\n",
       " 'I-QT',\n",
       " 'I-DT',\n",
       " 'B-OG',\n",
       " 'I-DT',\n",
       " 'B-OG',\n",
       " 'B-QT',\n",
       " 'B-DT',\n",
       " 'B-TI',\n",
       " 'B-TI',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'I-OG',\n",
       " 'O',\n",
       " 'B-DT',\n",
       " 'I-TI',\n",
       " 'O',\n",
       " 'B-TI',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PS',\n",
       " 'B-OG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'O',\n",
       " 'I-QT',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-OG',\n",
       " 'B-OG',\n",
       " 'B-OG']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  logits = model(**batch).logits\n",
    "\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "predicted_token_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005be0c-f3a1-4948-bb49-88570d1c20e7",
   "metadata": {},
   "source": [
    "## 4-3. evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3c04ac0d-53b7-472c-b5d6-b7275db61d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.06923076923076923}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "pred_labels = logits.argmax(dim=2).flatten().cpu().numpy() # logits.argmax(dim=2)의 결과를 1차원 벡터로 변환\n",
    "true_labels = batch[\"labels\"].flatten().numpy() # batch의 레이블을 1차원 벡터로 변환\n",
    "\n",
    "# evaluate 할 때는 데이터들을 1차원 텐서로 바꿔야한다.\n",
    "f1 = evaluate.load(\"f1\")\n",
    "f1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345bbcab-3c7f-47b9-869b-fe7525edd9de",
   "metadata": {},
   "source": [
    "# 5. Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acddf36d-7750-48cf-943c-182937021038",
   "metadata": {},
   "source": [
    "추출: 주어진 context에서 답변을 추출한다.\n",
    " - 추출 질의 응답은 질문에 대한 답변을 입력된 context에서 말 그대로 추출하는 방식이다.\n",
    "\n",
    "생성: 질문에 정확하게 답하는 맥락에서 답을 생성한다.\n",
    " - 문제에 대한 답을 입력 context를 참고하여 새로 작성하는 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3149ee-76af-47fa-85d6-9fc9d251d91e",
   "metadata": {},
   "source": [
    "## 5-1. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb12e59-3e18-4433-885d-27f781b6da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 02:48:07.896997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743130087.914973   41991 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743130087.920553   41991 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743130087.934591   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743130087.934608   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743130087.934609   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743130087.934611   41991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-28 02:48:07.939078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d08b31-4fe2-48ed-bf07-802e2ccc7326",
   "metadata": {},
   "source": [
    "## 5-2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1fa83e8-7b92-4572-9526-e2ec8e624501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fdbbe0d6d94c22b10b4649a8d8a68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4c41534d4340de9b6793210e3f5d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d7769613bc4954aa3b632a9839d68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/17554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61214d96a167487cb38fc424efbf5079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내용 : 올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 \n",
      "질문 : 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\n",
      "답변 : {'answer_start': [478, 478], 'text': ['한 달가량', '한 달']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"mrc\")\n",
    "sample = dataset[\"train\"][0]\n",
    "\n",
    "print(f\"내용 : {sample['context'][:50]}\") # context: 모델이 답변을 추출할 때, 필요한 배경 정보\n",
    "print(f\"질문 : {sample['question']}\") # question: 모델이 대답해야 하는 질문\n",
    "print(f\"답변 : {sample['answers']}\") # answers: 답변 토큰과 답변 텍스트 시작 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dffc823-f85a-44e9-9182-867ba9cfb6ac",
   "metadata": {},
   "source": [
    "## 5-3. Data preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff24bd60-ce35-4673-a569-e691d9255e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76d6503b6f942b092484f52417c7a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1d579077a24424916ce526497954ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\", \n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03fbc0c-d61a-498f-9e8d-dcfd12937bd5",
   "metadata": {},
   "source": [
    "## 보충 설명\n",
    "\n",
    "`truncation=\"only_second\"`\n",
    "\n",
    "- truncation을 only_second로 설정하면 두 번째 문장에 대해서만 max_length보다 긴 부분을 잘라낸다.\n",
    "- QA task에서는 보통 question과 context를 함께 모델에 입력함. 보통 context가 길기고 question은 짧기에 context가 max_length를 넘으면 자른다.\n",
    "    \n",
    "`return_offsets_mapping=True`\n",
    "\n",
    "- 인코됭된 토큰이 원본 문장에서 몇 번째 글자인지를 알 수 있도록 인덱스를 반환하도록 설정하는 옵션이다.\n",
    "- QA task에서 answert이 context에서 추출되는 방식이다. 즉 answer 시작과 끝이 context 내에서 특정한 위치에 존재한다. 하지만 토큰화 과정에서 단어가 쪼개지기에 원본 문장에서 정확한 위치를 찾기 힘들다.\n",
    "- 그래서 return_offsets_mapping=True를 설정하면 각 토큰이 원본 문장의 몇 번째 글자 범위에 해당하는지 매핑해줘서 모델이 정답을 원본 문장에서 찾을 수 있도록 도와준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801c1ea-f43d-480c-bf44-322bd83d7cdd",
   "metadata": {},
   "source": [
    "## 5-4. Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba2134-c6d7-4735-b6b0-17eaeb0e9b3d",
   "metadata": {},
   "source": [
    "input_ids, token_type_ids, attention_mask 칼럼을 입력 문장으로 만들고 각각 답변 시작과 끝 인덱스를 가리키는 start_positions과 end_positions이 출력(정답)이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd24310-6306-4d30-8cab-1b95ec8647d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  1174, 18956,  ...,  2170,  2259,     3],\n",
       "        [    2,  3920, 31221,  ...,  8055,  2867,     3],\n",
       "        [    2,  8813,  2444,  ...,  3691,  4538,     3],\n",
       "        ...,\n",
       "        [    2,  6860, 19364,  ...,  2532,  6370,     3],\n",
       "        [    2, 27463, 23413,  ..., 21786,  2069,     3],\n",
       "        [    2,  3659,  2170,  ...,  2470,  3703,     3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'start_positions': tensor([260,  31,   0,  80,  72,  81, 216, 348, 323, 348]), 'end_positions': tensor([263,  33,   0,  81,  78,  87, 221, 352, 328, 353])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "batch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6250f-fd35-46b6-a1e1-3bebfc88e151",
   "metadata": {},
   "source": [
    "## 5-5. prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a503c701-64f4-452d-9e37-c4e7ccbd7149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다. 17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2 ~ 3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다. 장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20 ~ 21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24 ~ 25일이었으며 장마기간은 32일, 강수일수는 17. 2일이었다. 기상청은 올해 장마기간의 평균 강수량이 350 ~ 400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = batch[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2a292-ebc8-4b80-944d-2dc8edb6c390",
   "metadata": {},
   "source": [
    "## 5-6. evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdd6d2-b6bc-4102-91fd-ed5497ac7799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate.load('sqaud')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d5574-9764-4558-8a92-3d47e73726a0",
   "metadata": {},
   "source": [
    "위의 코드로 진행이 가능하지만 상당한 양의 후처리가 필요하고 시간이 많이 걸리기에 생략한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
