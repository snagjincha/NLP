{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7eed493c-50fe-47ae-a798-08262894ca22",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Tokenizer training\"\n",
    "author: \"차상진\"\n",
    "date: \"2025-05-02\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a0c36-019e-48ba-b9dd-56d478e74f6d",
   "metadata": {},
   "source": [
    "# Tokenizers 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312baaa-96dd-40ff-bfaa-f641f6bddb6c",
   "metadata": {},
   "source": [
    "**목표: Transformers 라이브러리에서 사용되는 토크나이저를 만들어보자**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c506c36-0398-47ac-9db4-37d6bddd0170",
   "metadata": {},
   "source": [
    "`-` Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11a4a7c-3efc-4d84-ace5-388656a58aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nlp/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('klue','ynat')\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf5e03-c6a2-4dca-80cc-42b87263e586",
   "metadata": {},
   "source": [
    "`-` 제목만 따로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e35fc-00fd-474a-a635-6a8786687b4b",
   "metadata": {},
   "source": [
    "tokenizer 학습을 위해서 제목만 사용할 것이다. 굳이 메모리를 낭비할 필요가 없기 때문에 제목만 따로 저장하여 파일로 저장 후 그 파일을 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e23af91-6f15-4890-8ac2-71b9ae8a016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_key = 'title'\n",
    "for key in dataset.column_names.keys():\n",
    "    with open(f'./tokenizer_data_{key}.txt', 'w') as f:\n",
    "        f.write('\\n'.join(dataset[key][target_key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e196d-c380-4e58-b903-2a1cf546bca4",
   "metadata": {},
   "source": [
    "`-` 특수 토큰 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a611ace3-44eb-4ef7-8d3c-60d14b977ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '[UNUSED0', '[UNUSED1', '[UNUSED2', '[UNUSED3', '[UNUSED4']\n"
     ]
    }
   ],
   "source": [
    "user_defined_symbols = [\n",
    "    '[PAD]', # 문장길이 맞추는 토큰\n",
    "    '[UNK]', # 토크나이저가 인식할 수 없는 토큰\n",
    "    '[CLS]', # BERT계열 모델에서 문장 전체 정보를 저장하는 토큰\n",
    "    '[SEP]', # BERT계열 모델에서 문장 구분을 위해 사용하는 토큰\n",
    "    '[MASK]' # Masked LM에서 토큰 마스킹을 위해 사용하는 토큰\n",
    "]\n",
    "\n",
    "unused_token_num = 100\n",
    "unused_list = [f'[UNUSED{i}' for i in range(100)] # 사전학습시 어휘에 없는 토큰을 추가하기 위한 빈 공간\n",
    "whole_user_defined_symbols = user_defined_symbols + unused_list\n",
    "print(whole_user_defined_symbols[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283f6e3-02b3-41d5-8cd3-ab946bdb44d4",
   "metadata": {},
   "source": [
    "`-` 베이스 토크나이저 불러오기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54635a24-deca-4948-ac7a-d8ff70ef82fd",
   "metadata": {},
   "source": [
    "현재 선언된 토크나이저는 아직 학습되지 않은 WordPiece라는 규칙만 지정되고 빈 상태이다.\n",
    "\n",
    "- BERT 토크나이저는 WordPiece를 기반으로 하는 모델이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "deba09a0-a28a-45e4-baf5-a04c3e68d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token = '[UNK]')) # 토크나이저 객체 생성(WordPiece토크나이저 사용(사전에 없는 단어는 UNK로 처리하도록 설정))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22448af5-739b-4d0c-8291-a98609efbf2f",
   "metadata": {},
   "source": [
    "WordPiece 규칙만 지정되어있고 vocab는 전혀 구축되지 않은 상태이다.\n",
    "\n",
    "- vocab size가 0이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fa5b326-1e86-4363-bb6f-1db2b5a68a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.get_vocab_size()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ee5d4-504f-4f1a-bdee-7efe7a4cf0a9",
   "metadata": {},
   "source": [
    "`-` Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d77e95-58bd-4738-b6b6-f134b14a4a5a",
   "metadata": {},
   "source": [
    "tokenizer 성능을 높이기 위한 title 정규화를 진행합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5387b4f2-a7a5-4fb8-9c17-5fce96d9f4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are u? '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import normalizers\n",
    "\n",
    "normalizer = normalizers.BertNormalizer()\n",
    "bert_tokenizer.normalizer = normalizer\n",
    "\n",
    "# 정규화 예시\n",
    "normalizer.normalize_str(\"Héllò hôw\\nare ü? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f908e6-5e3c-4cc3-8260-244f1bd36ad3",
   "metadata": {},
   "source": [
    "`-` Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94baad0c-2d2f-43fd-b46a-b12437212b7d",
   "metadata": {},
   "source": [
    "Whitespace 클래스를 이용해서 줄 바꿈, 공백을 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3866b537-b457-4d8f-98cb-fc4015fd4691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('안녕하세요', (0, 5)),\n",
       " ('.', (5, 6)),\n",
       " ('제대로', (7, 10)),\n",
       " ('인코딩이', (11, 15)),\n",
       " ('되는지', (16, 19)),\n",
       " ('확인', (20, 22)),\n",
       " ('중입니다', (23, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "pre_tokenizer = Whitespace()\n",
    "bert_tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# Whitespace 예시\n",
    "pre_tokenizer.pre_tokenize_str(\"안녕하세요. 제대로 인코딩이 되는지 확인 중입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c58039-e7e2-4cb6-8b37-d5eba967f5d3",
   "metadata": {},
   "source": [
    "`-` 문장이 인코딩되었을 때 기본적으로 어떤 형태를 취할지 양식 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d55889-8132-4f8c-b068-f991c8e57509",
   "metadata": {},
   "source": [
    "토큰화된 결과에 특별한 규칙을 적용하는 후처리(post-processing) 단계를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae07df7a-ab5d-4df6-bf15-7cadf9205901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(t, i) for i, t in enumerate(user_defined_symbols)]\n",
    ")\n",
    "\n",
    "bert_tokenizer.post_processor = post_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83dd676-61a9-4c34-a306-a3a192fcfbe9",
   "metadata": {},
   "source": [
    "**규칙**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d1c35-2c1b-43dc-a6b3-0b52b7a546db",
   "metadata": {},
   "source": [
    "1. 문장 가장 앞에 [CLS] 토큰이 있어야 하고 두 문장을 입력받았을 때 문장을 구별하기 위한 [SEP] 토큰으로 감싸져 있어야 한다.\n",
    "\n",
    "2. `single`과 `pair`는 문장이 한 개씩 들어오는지 혹은 두 개씩 들어오는지를 나타낸다.\n",
    "\n",
    "3. 하나만 주어졌을 때는 '[CLS] 문장 [SEP]' 형태를 나타내고 두 개가 들어왔을 때는 '[CLS] 문장1 [SEP] 문장2 [SEP]' 형태로 출력되어야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f92abf-aee0-4e79-b892-ffc87c40d586",
   "metadata": {},
   "source": [
    "`-` Vocab 구축을 위한 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eab4f3-1bce-4aea-917c-c47a24fb52c3",
   "metadata": {},
   "source": [
    "Vocab을 구축하려면 학습을 해야하므로 Trainer를 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23730f00-ce39-4536-953c-18180d32645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size 지정, trainer 생성\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "vocab_size = 24000\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size = vocab_size,\n",
    "    special_tokens = whole_user_defined_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92323df-aa93-4a37-86d5-127972ca7375",
   "metadata": {},
   "source": [
    "위에서 저장한 `txt 파일`을 통해 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f17cf513-9b2a-40c2-9572-2e8a71a35667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "bert_tokenizer.train(glob(f'/*.txt'),trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da6799-55ae-46c4-b7f9-68436c5f9f11",
   "metadata": {},
   "source": [
    "`-` 디코딩으로 학습이 잘 되었는지 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dedabf0-4e8b-488c-81d8-1cd945fdbadb",
   "metadata": {},
   "source": [
    "인코딩은 잘 되었지만 디코딩은 제대로 되지 않은 것을 알 수 있다.\n",
    "\n",
    "- 사실은 정상적인 작동이다..! 이상해보이지만 `##`이 띄어쓰기 없이 붙히는 의미라는 것을 안다면 정상적인 결과라는 것을 알 수 있다.\n",
    "- 아직 디코딩 방법이 적용되지 않았기 때문에 그런 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c510933-55e0-4b0b-afd2-414d246438b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 675, 906, 2220, 4518, 1240, 906, 2220, 569, 6727, 12916, 10780, 586, 1881, 16618, 10192, 106, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'인 ##코 ##딩 및 디 ##코 ##딩 ##이 제대로 이루 ##어지는 ##지 확인 중이 ##ᆸ니다 .'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bert_tokenizer.encode('인코딩 및 디코딩이 제대로 이루어지는지 확인 중입니다.')\n",
    "print(output.ids)\n",
    "\n",
    "bert_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a5d62-49d8-437b-b80d-0860a68f8d4c",
   "metadata": {},
   "source": [
    "`-` WordPiece 디코더를 할당"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645bfb5f-2bd1-4f21-999f-14165f2d1142",
   "metadata": {},
   "source": [
    "WordPiece 토크나이저에 맞는 디코더를 추가하니 이번에는 제대로 디코딩이 된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d201ac6e-03f0-4141-a557-35ad88ad6ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인코딩 및 디코딩이 제대로 이루어지는지 확인 중입니다.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import decoders\n",
    "\n",
    "bert_tokenizer.decoder = decoders.WordPiece()\n",
    "bert_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc00eb4-9e18-419c-bbbe-478435014e72",
   "metadata": {},
   "source": [
    "`-` 우리가 만든 토크나이저를 transformers 토크나이저로 대체해서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65432c-6583-464b-856a-16ade3b8c32e",
   "metadata": {},
   "source": [
    "잘 작동하는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3314eac-b0a9-4a38-b28f-8dd5f8c67a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 675, 906, 2220, 4518, 1240, 906, 2220, 569, 6727, 12916, 10780, 586, 1881, 16618, 10192, 106, 3]\n",
      "[CLS] 인코딩 및 디코딩이 제대로 이루어지는지 확인 중입니다. [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "fast_tokenizer = BertTokenizerFast(tokenizer_object=bert_tokenizer)\n",
    "encoded = fast_tokenizer.encode(\"인코딩 및 디코딩이 제대로 이루어지는지 확인 중입니다.\")\n",
    "decoded = fast_tokenizer.decode(encoded)\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b81eb9-d36e-4727-8af2-182c3f939c8c",
   "metadata": {},
   "source": [
    "`-` tokenizer 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69ba99-296a-4951-ad31-79cf692a031a",
   "metadata": {},
   "source": [
    "잘 작동하므로 tokenizer 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48de7183-7235-4293-aedd-4e3c15117fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./MyTokenizer/tokenizer_config.json',\n",
       " './MyTokenizer/special_tokens_map.json',\n",
       " './MyTokenizer/vocab.txt',\n",
       " './MyTokenizer/added_tokens.json',\n",
       " './MyTokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기본 작업환경이 /root/NLP이기에 output_dir는 저렇게 작성한다.\n",
    "output_dir = './MyTokenizer'\n",
    "fast_tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79b5b8-9c06-4d35-80a9-930b4d608358",
   "metadata": {},
   "source": [
    "`-` 저장한 tokenizer를 가져와서 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb63b940-5124-45bb-bc28-71f351402a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [[2, 675, 906, 2220, 1675, 6464, 586, 1881, 3], [2, 18632, 1594, 6985, 3782, 3]]\n",
      "token_type_ids [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
      "attention_mask [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]\n",
      "[CLS] 인코딩 잘 되는지 확인 [SEP]\n",
      "[CLS] 안되면 다시 학습하자 [SEP]\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer = BertTokenizerFast.from_pretrained(output_dir)\n",
    "\n",
    "encoded = new_tokenizer([\"인코딩 잘 되는지 확인\", \"안되면 다시 학습하자\"])\n",
    "\n",
    "for k, v in encoded.items():\n",
    "  print(k, v)\n",
    "\n",
    "print(new_tokenizer.decode(encoded[\"input_ids\"][0]))\n",
    "print(new_tokenizer.decode(encoded[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155500e2-95f0-491b-b5b6-d1e41268dcb3",
   "metadata": {},
   "source": [
    "**tokenizer 완성!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc1569-0a68-49e0-ae17-d330e93a954b",
   "metadata": {},
   "source": [
    "# BERT 초기화 후 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb1fa7-2472-4afc-b8b6-10995433d567",
   "metadata": {},
   "source": [
    "사전학습 모델은 그에 해당하는 토크나이저가 있다. 이것을 반대로 말하면 토크나이저를 사전학습해도 사전학습한 토크나이저를 쓰는 모델이 없다면 아무 의미가 없다는 의미이다.\n",
    "\n",
    "그래서 우리가 만든 토크나이저(WordPiece)에 대응하는 BERT모델을 초기화하여 처음부터 학습시키자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992c787-5c98-49d1-b228-5246607ea0c5",
   "metadata": {},
   "source": [
    "`-` Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5882596f-da20-4c6d-acf0-b515b4fb7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"ynat\")\n",
    "model_name = \"./MyTokenizer\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e120b-731f-4c1b-babf-978e7af0c579",
   "metadata": {},
   "source": [
    "`-` config 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d290d645-2b80-4b26-aab9-bd9c8b738105",
   "metadata": {},
   "source": [
    "모델의 최초 선언을 위해서는 해당 모델 `config`가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99462b07-823a-472d-858c-cd3a2dc76971",
   "metadata": {},
   "source": [
    "`config`는 `embedding size`, `hidden size`, `num layers`등 모델의 전반적인 구조 정보를 저장하고 있다.\n",
    "\n",
    "모델 `vocab size`는 토크나이저의 `vocab size`를 따라가야 하므로 `vocab size`만 따로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "438122a5-36e9-4c78-979b-7932786c31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=256,            \n",
    "    num_hidden_layers=4,        \n",
    "    num_attention_heads=4,      \n",
    "    intermediate_size=1024,     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede06ae-f07d-4b5d-978c-e12416ab1d22",
   "metadata": {},
   "source": [
    "`-` Model 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970776f-24bd-41c1-bed9-5ebd5582b878",
   "metadata": {},
   "source": [
    "config를 준비했으니 모델은 간단하게 `config` 인자를 넣으면 선언 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6f57984f-dc9c-4345-9a49-d5199232bf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(24000, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=256, out_features=24000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a69f4-c8e1-4e5f-8ae1-977f164e3119",
   "metadata": {},
   "source": [
    "model도 선언했으니 이제 학습만 하면 된다.\n",
    "\n",
    "학습 과정은 생략하겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
