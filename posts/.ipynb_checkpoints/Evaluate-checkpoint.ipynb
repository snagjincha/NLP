{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fe71f785-af9b-44cf-a45d-dcd8c69de7bd",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Evaluate\"\n",
    "author: \"차상진\"\n",
    "date: \"2025-03-16\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89082e32-dddf-4810-b13f-da18c451e197",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c948c00-dcac-4425-ac44-9a6c02fb6c18",
   "metadata": {},
   "source": [
    "Evaluate 클래스는 accuracy, F1 score, precision, recall, BLEU, ROUGE 등등 다양한 평가 지표를 간단하게 불러와 활용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec154206-1983-4f03-a5ba-c6a90da8132b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a694ba7bd0941029196bda121c9f151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "acc = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5064c8-2a1c-4669-8e14-64aa3d2a5c5e",
   "metadata": {},
   "source": [
    "**계산을 해보는 예시코드이다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ec0bb98-a6fe-4606-a87d-e2905e77dccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = evaluate.combine(['accuracy','f1','precision','recall'])\n",
    "metrics.compute(predictions=[1,0,0,1], references = [0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0c41e1a-2881-4248-ba54-caf2a1237ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for y,pred in zip([0,1,0,1],[1,0,0,1]):\n",
    "    metrics.add(predictions=pred, references=y)\n",
    "metrics.compute()\n",
    "\n",
    "# .add 메소드는 입력받는 값들이 스칼라 값이어야 한다. 한 번에 하나의 예측값과 정답을 추가."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c633b5a-6b61-414b-bffb-21e58bff8663",
   "metadata": {},
   "source": [
    "`-` zip()을 사용하여 references(정답)과 predictions(예측값)를 묶음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e19541f8-d0cb-4422-92b7-5c146a8339d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for y,preds in zip([[0,1],[0,1]],[[1,0],[0,1]]):\n",
    "    metrics.add_batch(predictions=preds, references=y)\n",
    "metrics.compute()\n",
    "\n",
    "# .add_batch 메소드는 입력받는 값들이 리스트(배치단위)여야 한다. 한 번에 여러 개의 예측값과 정답을 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a10571-5809-4005-8025-0f74daf4e3b7",
   "metadata": {},
   "source": [
    "## Create custom metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c2594-f48c-4fe1-901b-26cf3430f0d5",
   "metadata": {},
   "source": [
    "`-` 딕셔너리 형태로 반환되는 구조의 함수여야 Trainer 클래스의 매개변수인 compute_metrics에 입력하여 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dca42a16-4acd-4616-b82d-2ab861467ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도를 계산하는 간단한 함수\n",
    "def simple_accuracy(preds,labels):\n",
    "    return {'accuracy': (preds == labels).to(float).mean().item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa4c4a-07d5-46c6-9bba-08d7dc610ef7",
   "metadata": {},
   "source": [
    "## Trainer 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bbbf485-8a15-4f74-94f7-4defb40de8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro f1 score 사용\n",
    "import evaluate\n",
    "\n",
    "def custom_metrics(pred):\n",
    "    f1 = evaluate.load('f1')\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(axis = -1) # pred.predictions를 수행하면 logits값이 출력된다. 그 중 가장 큰 값을 가지는 인덱스를 반환하는 함수\n",
    "    return f1.compute(predictions = preds, references = labels, average = 'micro')\n",
    "          #.compute() 함수는 자동으로 결과를 딕셔너리형태로 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c018f-2ed4-4534-8a70-523e804c3fcb",
   "metadata": {},
   "source": [
    "`-` .argmax(?)\n",
    "\n",
    "    axis = -1을 쓰면 배열의 마지막 축에서 계산하는 것. axis=0은 열(세로), aixis= 1은 행(가로)이다.\n",
    "    위에서 pred.predcitions을 했을 때 나오는 결과가 2차원이기에 마지막 축이란 행이 되므로 axis = -1과 axis = 1은 같은 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fef103ee-4c46-4e4f-a3de-d43992b3fb5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa95956903724aa79ad934c45e058686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 45678\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 9107\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"ynat\")\n",
    "\n",
    "def tokenize_function(sample):\n",
    "    result = tokenizer(\n",
    "        sample[\"title\"],\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return result\n",
    "\n",
    "datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=[\"guid\", \"title\", \"url\", \"date\"]\n",
    ")\n",
    "print(datasets)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    max_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    logging_dir=\"/content/logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    output_dir=\"/content/ckpt\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=custom_metrics, # 이 부분을 바꿔준다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc0029ad-da75-4d14-94ce-ff65417cf988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 45678\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 18:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.082100</td>\n",
       "      <td>0.825163</td>\n",
       "      <td>0.702427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.571100</td>\n",
       "      <td>0.593032</td>\n",
       "      <td>0.810695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.570962</td>\n",
       "      <td>0.816844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.535641</td>\n",
       "      <td>0.815966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.501376</td>\n",
       "      <td>0.833535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.433800</td>\n",
       "      <td>0.479584</td>\n",
       "      <td>0.837158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.397700</td>\n",
       "      <td>0.483717</td>\n",
       "      <td>0.842868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.442900</td>\n",
       "      <td>0.449807</td>\n",
       "      <td>0.851104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.420800</td>\n",
       "      <td>0.434349</td>\n",
       "      <td>0.853300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.406100</td>\n",
       "      <td>0.438009</td>\n",
       "      <td>0.853080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /content/ckpt/checkpoint-50\n",
      "Configuration saved in /content/ckpt/checkpoint-50/config.json\n",
      "Model weights saved in /content/ckpt/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/ckpt/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/ckpt/checkpoint-50/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /content/ckpt/checkpoint-100\n",
      "Configuration saved in /content/ckpt/checkpoint-100/config.json\n",
      "Model weights saved in /content/ckpt/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/ckpt/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/ckpt/checkpoint-100/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /content/ckpt/checkpoint-150\n",
      "Configuration saved in /content/ckpt/checkpoint-150/config.json\n",
      "Model weights saved in /content/ckpt/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/ckpt/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/ckpt/checkpoint-150/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /content/ckpt/checkpoint-200\n",
      "Configuration saved in /content/ckpt/checkpoint-200/config.json\n",
      "Model weights saved in /content/ckpt/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/ckpt/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/ckpt/checkpoint-200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /content/ckpt/checkpoint-250\n",
      "Configuration saved in /content/ckpt/checkpoint-250/config.json\n",
      "Model weights saved in /content/ckpt/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/ckpt/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/ckpt/checkpoint-250/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /content/ckpt/checkpoint-300\n",
      "Configuration saved in /content/ckpt/checkpoint-300/config.json\n",
      "Model weights saved in /content/ckpt/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/ckpt/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/ckpt/checkpoint-300/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /content/ckpt/checkpoint-350\n",
      "Configuration saved in /content/ckpt/checkpoint-350/config.json\n",
      "Model weights saved in /content/ckpt/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/ckpt/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/ckpt/checkpoint-350/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /content/ckpt/checkpoint-400\n",
      "Configuration saved in /content/ckpt/checkpoint-400/config.json\n",
      "Model weights saved in /content/ckpt/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/ckpt/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/ckpt/checkpoint-400/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /content/ckpt/checkpoint-450\n",
      "Configuration saved in /content/ckpt/checkpoint-450/config.json\n",
      "Model weights saved in /content/ckpt/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/ckpt/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/ckpt/checkpoint-450/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /content/ckpt/checkpoint-500\n",
      "Configuration saved in /content/ckpt/checkpoint-500/config.json\n",
      "Model weights saved in /content/ckpt/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/ckpt/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/ckpt/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.5185918083190918, metrics={'train_runtime': 1125.3103, 'train_samples_per_second': 7.109, 'train_steps_per_second': 0.444, 'total_flos': 2104982937600000.0, 'train_loss': 0.5185918083190918, 'epoch': 0.18})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f87b43-99d7-46a8-8832-6751d2f66906",
   "metadata": {},
   "source": [
    "`-` 결과에서 이전에는 볼 수 없었던 F1 score이 보인다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
