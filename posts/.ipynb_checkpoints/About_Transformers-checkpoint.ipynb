{
 "cells": [
  {
   "cell_type": "raw",
   "id": "25982750-3c11-43d7-8f89-6b537a7acd56",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"About Transformers\"\n",
    "author: \"차상진\"\n",
    "date: \"2025-03-20\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e818860b-d9cc-4529-97a9-42d03250b6b1",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff8d20a-de8f-46a1-9dea-eb548c7d8c1f",
   "metadata": {},
   "source": [
    "## 1. 입력 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b315ea5-0b89-4ce1-9583-024bf3c9cc52",
   "metadata": {},
   "source": [
    "- 임베딩 레이어: 입력 데이터를 고정된 차원의 연속형 벡터로 변환합니다.\n",
    "- 위치 인코딩: 단어의 순서를 반영하기 위해 사인 및 코사인 함수를 이용한 위치 인코딩을 추가한다.\n",
    "- 최종 입력: 입력 임베딩과 위치 인코딩을 더한 값을 인코더에 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c94e47-4e92-4d29-af84-d94cbe4f1b90",
   "metadata": {},
   "source": [
    "## 2. 인코더 구조 (여러 개의 인코더 레이어 스택)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868ad79-3a0c-4fe2-b09f-316289295277",
   "metadata": {},
   "source": [
    "- 인코더는 여러 개의 인코더 레이어로 이루어지며, 각 인코더 레이어는 다음을 포함한다.\n",
    "1. 멀티-헤드 셀프 어텐션 (Multi-Head Self-Attention)\n",
    "    - 입력 문장에서 각 단어가 문맥을 반영해 서로 다른 중요도를 가질 수 있도록 어텐션을 적용합니다.\n",
    "    - (입력 임베딩 + 위치 인코딩) -> 쿼리(Q), 키(K), 밸류(V) 생성 -> 어텐션 결과 출력\n",
    "2. 잔차 연결 (Residual Connection) + Layer Normalization\n",
    "    - 입력 값(입력 임베딩 + 위치 인코딩)과 어텐션 결과를 더한 후 정규화 적용 (**이상하다고 생각 가능 -> 밑에서 설명**)\n",
    "3. 피드포워드 네트워크 (Feedforward Network, FFN)\n",
    "    - 비선형 변환을 수행하는 두 개의 완전연결층(FC)으로 구성된다.\n",
    "    - 비선형 변환이란 ReLU와 같은 활성화 함수를 의미한다.\n",
    "    - **비선형 변환을 이용하는 이유는 학습의 다양성과 안정성을 높이기 위해서이다. 선형 변환만으로는 곱셈과 덧셈으로만 나타내기에 함수의 형태가 단순한데 비선형 변환을 통해 더 복잡한 함수와 관계를 학습할 수 있다.**\n",
    "4. 잔차 연결 + Layer Normalization\n",
    "    - FFN의 출력을 다시 입력과 더한 후 정규화합니다. (2번과 비슷하게 입력 값을 다시 더해준다)\n",
    "    - 그 후 정규화를 한다. 즉, `LayerNorm(FFN Output + Attention Output)`\n",
    "      \n",
    "- 이러한 인코더 레이어를 여러 개 쌓아 깊은 표현 학습을 가능하게 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77fc49-8a0f-4841-b7cf-0c321b3fa22f",
   "metadata": {},
   "source": [
    "`-` 이상한 부분의 보충설명\n",
    "\n",
    "    Q. 어텐션 결과라는 것은 입력값을 멀티-헤드 셀프 어텐션을 거쳐서 나온 결과잖아요. 그런데 그 결과랑 입력 값을 또 더한다구요? 왜 또 더해요?\n",
    "\n",
    "    A. 어텐션의 출력은 입력을 바탕으로 생성된 값인데, 다시 입력과 더하는 것이 이상하게 보일 수 있음. 그 이유는 잔차 연결(Residual Connection)**은 신경망이 깊어질수록 발생하는 학습 어려움을 해결하기 위해 도입된 기법입니다. 만약 어텐션 출력만 다음 레이어로 넘긴다면, 원래 입력 정보가 손실될 수 있음. 따라서, 원래 입력 정보를 유지하면서도, 어텐션이 학습한 새로운 정보(출력)를 추가적으로 반영하기 위해 입력 + 어텐션 출력을 더함.\n",
    "\n",
    "    잔차 연결이 없다면? : 인코더/디코더가 깊어질수록 입력 정보가 왜곡될 가능성이 커진다. 그러므로 입력 정보(원래 값) + 어텐션 결과(새로운 정보)를 함께 유지하는 것이 학습 안정성 측면에서 유리함. 이것이 잔차 연결의 핵심 개념이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9a2b1f-71f9-4614-8902-ffd632c2609a",
   "metadata": {},
   "source": [
    "## 3. 디코더 구조 (여러 개의 디코더 레이어 스택)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d6cc6-7429-4805-b568-c6aad5bccdb4",
   "metadata": {},
   "source": [
    "- 디코더도 여러 개의 디코더 레이어로 구성되며, 인코더와 다른 점은 인코더-디코더 어텐션 레이어가 추가된다는 것입니다.\n",
    "- 각 디코더 레이어는 다음을 포함합니다.\n",
    "1. 마스크드 멀티-헤드 셀프 어텐션 (Masked Multi-Head Self-Attention)\n",
    "    - 디코더는 정답 데이터를 예측하는 과정에서 앞쪽 단어만 참고할 수 있도록 마스크를 적용하여 미래 정보를 차단합니다.\n",
    "    - 미래 정보(미래에 나올 단어)를 미리 안다면 정답지를 보고 컨닝을 하는 것과 같기에 올바론 학습이 되\n",
    "  지 않는다.\n",
    "    - 마스크드 멀티 헤드 어텐션도 인코더의 멀티 헤드 어텐션과 똑같이 Q,K,V 값 계산 -> 어텐션 결과 출력 \n",
    "2. 잔차 연결 + Layer Normalization\n",
    "    - 인코더 구조에서 설명한 것과 같음.\n",
    "3. 인코더-디코더 어텐션 (Encoder-Decoder Attention)\n",
    "    - 인코더에서 출력된 벡터를 사용하여 입력 문장과 현재 디코더 상태 간의 관계를 학습합니다.\n",
    "    - 디코더만 사용하는 모델(GPT)는 이 층을 사용할 수 없다. 인코더에서 나온 출력을 이용하기에 인코더와 디코더가 모두 있는 모델에서만 이 층을 사용한다.\n",
    "4. 잔차 연결 + Layer Normalization\n",
    "    - 3에서 추가된 인코더-디코더 어텐션 층이 있기에 잔차 연결 + Layer Normalizaiton 층이 추가된다.\n",
    "6. 피드포워드 네트워크 (FFN)\n",
    "7. 잔차 연결 + Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a5bf29-8a50-4ec7-8077-9d2282549eab",
   "metadata": {},
   "source": [
    "## 4. 출력 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc72d2-d394-4505-ad9e-1e54f06c9cca",
   "metadata": {},
   "source": [
    "- 선형 변환 (Linear Layer): 디코더에서 나온 출력을 단어 집합 크기만큼의 차원으로 변환합니다.\n",
    "- 소프트맥스 (Softmax): 확률 분포를 구해 최종적으로 가장 확률이 높은 단어를 예측합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817d6c1-9a5f-49f6-9328-7b1594acb8d2",
   "metadata": {},
   "source": [
    "## 요약 (순서 정리)\n",
    "1. 입력 데이터 처리 (임베딩 + 위치 인코딩)\n",
    "2. 인코더 (여러 개의 인코더 레이어)\n",
    "    -멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화 → FFN → 잔차 연결 + 정규화\n",
    "3. 디코더 (여러 개의 디코더 레이어)\n",
    "    -마스크드 멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화\n",
    "    -인코더-디코더 어텐션 → 잔차 연결 + 정규화\n",
    "    -FFN → 잔차 연결 + 정규화\n",
    "4. 출력 변환 (선형 레이어 → 소프트맥스 → 예측)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
