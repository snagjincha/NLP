{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d2b150de-508e-4f8d-823b-54c8c160aa5a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Model fine tuning\"\n",
    "author: \"ì°¨ìƒì§„\"\n",
    "date: \"2025-04-15\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6cd58-926c-43f7-bf9b-4812209c8c94",
   "metadata": {},
   "source": [
    "# 1. Install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a586e-2107-43af-a68b-c4e58fb5f7c8",
   "metadata": {},
   "source": [
    "ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ëŸ°íƒ€ì„ ì¬ì‹œì‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c391486f-fd95-4011-8114-79ad7bbba18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install datasets==2.20.0 transformers==4.41.2 peft==0.10.0 evaluate==0.4.2 scikit-learn==1.4.2 accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ac826-71e3-474a-8b55-9e2fe064b260",
   "metadata": {},
   "source": [
    "ì´ì „ chapterì—ì„œëŠ” ì „ì²´ì ìœ¼ë¡œ ë¯¸ì„¸ì¡°ì •ì„ í•˜ì§€ ì•Šê³  ëª¨ë¸ê³¼ ê° íƒœìŠ¤í¬ì˜ ëŒ€ëµì ì¸ êµ¬ì¡°ë§Œ í•™ìŠµí–ˆë‹¤.\n",
    "\n",
    "ì•„ë¬´ë˜ë„ í•™ìŠµì„ í•˜ì§€ ì•Šê³  ë°”ë¡œ predictë¥¼ í•˜ë‹ˆ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šì•˜ë‹¤.\n",
    "\n",
    "ì´ë²ˆ chapterì—ì„œëŠ” ë¯¸ì„¸ì¡°ì •ì„ í•´ë³´ëŠ” ì½”ë“œë¥¼ ë°°ì›Œë³¼ ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e567a31-ac81-4220-85e3-679b31f2be54",
   "metadata": {},
   "source": [
    "# 2. Encoder Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e994cf-07cb-49da-ab9a-baee8f790d3c",
   "metadata": {},
   "source": [
    "## 2-1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2887df-6bda-46fd-809e-42a8d5203836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d512fc4c4e4c028529d5034aebc6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/asdf/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc478ce8816f4e1088544d7470120106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966ae83583c745d180cafd4c834e9c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cc7068f3044c0b99d02fb093d7fa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d10d5ccae54c4dad3894faacd7b647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7fa05ca09c41a484eb7d10d1a9865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe189d-ce90-4bcb-9abb-ac66c4706088",
   "metadata": {},
   "source": [
    "## 2-2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411709ee-c803-4ed4-90ed-c7f81df4170b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b841fbfed9e94cda863b0a7dd3586abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9becfb5bcac84d8091faef2eba64fdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"sts\")\n",
    "\n",
    "def process_data(batch):\n",
    "  result = tokenizer(\n",
    "      batch[\"sentence1\"],\n",
    "      text_pair=batch[\"sentence2\"],\n",
    "      max_length=128,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_tensors=\"np\",\n",
    "  )\n",
    "  result[\"labels\"] = [x[\"binary-label\"] for x in batch[\"labels\"]]\n",
    "  return result\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    process_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "tokenized_dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2181644-6aec-4f05-b84b-b5cb0093dabf",
   "metadata": {},
   "source": [
    "## 2-3 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dc2ee1d-cd30-4fec-b3dc-f5096fffa584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/asdf/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='1830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/1830 04:27 < 16:01, 1.49 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.930365</td>\n",
       "      <td>0.786127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.718717</td>\n",
       "      <td>0.805395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.800480</td>\n",
       "      <td>0.786127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.828407</td>\n",
       "      <td>0.799615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.06518504738807679, metrics={'train_runtime': 268.0555, 'train_samples_per_second': 435.283, 'train_steps_per_second': 6.827, 'total_flos': 1678122311086080.0, 'train_loss': 0.06518504738807679, 'epoch': 2.185792349726776})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    Trainer, # ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ í›ˆë ¨ ë„êµ¬\n",
    "    TrainingArguments, # í•™ìŠµì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì •ì„ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤\n",
    "    default_data_collator, # ì½œë ˆì´í„°\n",
    "    EarlyStoppingCallback # early stop í•¨ìˆ˜\n",
    ")\n",
    "import evaluate \n",
    "\n",
    "\n",
    "def custom_metrics(pred): # micro f1 score í‰ê°€ ì§€í‘œë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n",
    "  f1 = evaluate.load(\"f1\")\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "\n",
    "  return f1.compute(predictions=preds, references=labels, average=\"micro\")\n",
    "\n",
    "training_args = TrainingArguments( # í•™ìŠµ argument ì„¤ì •\n",
    "    per_device_train_batch_size=64, # í•™ìŠµí•  ë•Œ ë°°ì¹˜ í¬ê¸°\n",
    "    per_device_eval_batch_size=64, # í‰ê°€í•  ë•Œ ë°°ì¹˜ í¬ê¸°\n",
    "    learning_rate=5e-6,\n",
    "    max_grad_norm=1, # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€)\n",
    "    num_train_epochs=10, \n",
    "    evaluation_strategy=\"steps\", # ì¼ì • stepë§ˆë‹¤ ê²€ì¦ ì‹¤í–‰\n",
    "    logging_strategy=\"steps\", # ì¼ì • ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì €ì¥\n",
    "    logging_steps=100, # 100 stepë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥\n",
    "    logging_dir=\"data/logs\", # ë¡œê·¸ ì €ì¥ ê²½ë¡œ (í˜„ì¬ ì‹¤í–‰ì¤‘ì¸ í´ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ í˜„ì¬ í´ë”ë¥¼ ì˜ í™•ì¸í•´ì•¼í•¨)\n",
    "    save_strategy=\"steps\", # ì¼ì • stepë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "    save_steps=100, # 100 stepë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "    output_dir=\"data/ckpt\", # ë¡œê·¸ ì €ì¥ ê²½ë¡œ (í˜„ì¬ ì‹¤í–‰ì¤‘ì¸ í´ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ í˜„ì¬ í´ë”ë¥¼ ì˜ í™•ì¸í•´ì•¼í•¨)\n",
    "    load_best_model_at_end = True, # í•™ìŠµ ì¢…ë£Œ í›„ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ë¡œë“œ\n",
    "    report_to='tensorboard', # tensorboardì— í•™ìŠµ ë¡œê·¸ ê¸°ë¡\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=custom_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)] # 2ë²ˆ ì—°ì†ìœ¼ë¡œ ê²€ì¦ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56037db6-07b6-46d5-907b-c5a2989a321e",
   "metadata": {},
   "source": [
    "## 2-4. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dd2a5-c743-469d-bda3-3af3f8fa96ea",
   "metadata": {},
   "source": [
    "ë¯¸ì„¸ì¡°ì •ìœ¼ë¡œ 400 stepì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¡œ ì €ì¥í•œ ê²½ë¡œì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì™€ ê²€ì¦ ê²Œì´í„° ì¤‘ 10ê°œ ìƒ˜í”Œì„ ì…ë ¥í•´ ê°„ë‹¨í•œ ì¶”ë¡ ì„ ì§„í–‰í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a9ead28-81b6-40bf-a570-6a3ac8a97dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8785,  3.4420],\n",
       "        [ 3.2443, -2.9451],\n",
       "        [ 1.9444, -1.9935],\n",
       "        [-3.5267,  3.0033],\n",
       "        [ 0.2386, -0.7375],\n",
       "        [ 3.3531, -2.7962],\n",
       "        [-3.2954,  2.9353],\n",
       "        [ 3.8899, -3.2844],\n",
       "        [ 4.2035, -3.6925],\n",
       "        [ 4.1651, -3.5504]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# tokenizer, model\n",
    "model_name = \"data/ckpt/checkpoint-400\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer) # ì½œë ˆì´í„° ì„¤ì •\n",
    "batch = collator([tokenized_dataset[\"validation\"][i] for i in range(10)])\n",
    "\n",
    "# inference\n",
    "with torch.no_grad():\n",
    "  logits = model(**batch).logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73347731-7400-42ba-89d9-25899221bfcf",
   "metadata": {},
   "source": [
    "## 2-5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01ec3d2d-9a90-41ac-872f-aef939bf9b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Tue Apr  1 08:33:18 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 1.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "pred_labels = logits.argmax(dim=1).cpu().numpy()\n",
    "true_labels = batch[\"labels\"].numpy()\n",
    "\n",
    "f1 = evaluate.load(\"f1\")\n",
    "f1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de7502-c616-4a6e-9402-e93769cfe72c",
   "metadata": {},
   "source": [
    "'klue/bert-base' ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ `AutoModelForSequenceClassification` í—¤ë“œë¥¼ ë¶™í˜”ë‹¤.\n",
    "\n",
    "ê·¸ í›„ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ê³  í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì˜ˆì¸¡ì„ í•˜ì˜€ìœ¼ë¯€ë¡œ ë¯¸ì„¸ì¡°ì •(fine tuning)ì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94948362-ad3b-4421-a299-03cc385882e5",
   "metadata": {},
   "source": [
    "# 3. Decoder Causal LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff9821-d8b2-42bb-ba8b-f8e9d10d32d9",
   "metadata": {},
   "source": [
    "## 3-1. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "098b12f1-fecf-4452-92a5-06bd1ca187d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a394ac0e-d005-4aa2-a385-becbabe58a01",
   "metadata": {},
   "source": [
    "## 3-2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac586a64-25fd-49fc-aa6f-2c7186160a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff6a34318c8438e8ef0ff494cc622d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3151e59e084421eb32795dd5920435a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ë°ì´í„°ì…‹ êµ¬ì¡°í™”\n",
    "split_dict = {\n",
    "    \"train\": \"train[:8000]\",\n",
    "    \"test\": \"train[8000:10000]\",\n",
    "    \"unused\": \"train[10000:]\",\n",
    "}\n",
    "dataset = load_dataset(\"heegyu/kowikitext\", split=split_dict)\n",
    "del dataset[\"unused\"] # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” unused ë°ì´í„°ë¥¼ ì‚­ì œ\n",
    "\n",
    "# í† í°í™”\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda batch: tokenizer([f\"{ti}\\n{te}\" for ti, te in zip(batch[\"title\"], batch[\"text\"])]),\n",
    "    batched=True, # ë°°ì¹˜ë¡œ ë¬¶ì–´ì„œ ì²˜ë¦¬\n",
    "    num_proc=2, # 2ê°œ í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ì‹¤í–‰\n",
    "    remove_columns=dataset[\"train\"].column_names, # ê¸°ì¡´ì˜ 'title','text' ì»¬ëŸ¼ì„ ì‚­ì œ í›„ í† í°í™” ê²°ê³¼ë§Œ ë‚¨ê¹€\n",
    ")\n",
    "\n",
    "# ìµœëŒ€ ê¸¸ì´ë¡œ ê·¸ë£¹í™”\n",
    "max_length = 512 \n",
    "def group_texts(batched_sample):\n",
    "    sample = {k: v[0] for k, v in batched_sample.items()} # ë°ì´í„°ì…‹ì—ì„œ ê° keyì˜ ì²« ë²ˆì§¸ ê°’ë§Œ ê°€ì ¸ì˜¤ê¸°.\n",
    "\n",
    "    if sample[\"input_ids\"][-1] != tokenizer.eos_token_id: # ë§ˆì§€ë§‰ í† í°ì´ <eos>ê°€ ì•„ë‹ˆë¼ë©´ <eos> ì¶”ê°€.\n",
    "        for k in sample.keys():\n",
    "            sample[k].append(\n",
    "                tokenizer.eos_token_id if k == \"input_ids\" else sample[k][-1]\n",
    "                # sample['input_ids']ë¼ë©´ <eos> í† í°ì„ ì¶”ê°€. / sample['input_ids']ê°€ ì•„ë‹ˆë¼ë©´ ê¸°ì¡´ ê°’ ìœ ì§€.(sample[k][-1])\n",
    "            )\n",
    "\n",
    "    result = {\n",
    "        k: [v[i: i + max_length] for i in range(0, len(v), max_length)] # ë¬¸ì¥ì´ ê¸¸ë©´ ì—¬ëŸ¬ ê°œì˜ ìƒ˜í”Œë¡œ ë¶„í• \n",
    "        for k, v in sample.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "grouped_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    num_proc=2,\n",
    ")\n",
    "grouped_dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c400903-c32a-4d0a-9111-d2c09d769f2e",
   "metadata": {},
   "source": [
    "## 3-3. Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b28117af-be9c-42da-8708-c4bfedc42501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/asdf/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13776' max='13776' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13776/13776 1:59:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.284300</td>\n",
       "      <td>4.471501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.219000</td>\n",
       "      <td>3.416660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.206900</td>\n",
       "      <td>2.995688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.185800</td>\n",
       "      <td>2.903164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.168300</td>\n",
       "      <td>2.876508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.151000</td>\n",
       "      <td>2.805254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.151500</td>\n",
       "      <td>2.801232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.160500</td>\n",
       "      <td>2.774476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.133300</td>\n",
       "      <td>2.774085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.085200</td>\n",
       "      <td>2.745091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.062300</td>\n",
       "      <td>2.747127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.066100</td>\n",
       "      <td>2.767898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.037000</td>\n",
       "      <td>2.726495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.048100</td>\n",
       "      <td>2.735216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.060500</td>\n",
       "      <td>2.741515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.031300</td>\n",
       "      <td>2.703305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.050100</td>\n",
       "      <td>2.725095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.062700</td>\n",
       "      <td>2.712202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.020300</td>\n",
       "      <td>2.688988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.996800</td>\n",
       "      <td>2.699451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>4.008100</td>\n",
       "      <td>2.701477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.989600</td>\n",
       "      <td>2.698209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.984000</td>\n",
       "      <td>2.686813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.007600</td>\n",
       "      <td>2.688273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.992500</td>\n",
       "      <td>2.678961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.982600</td>\n",
       "      <td>2.687267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>4.009200</td>\n",
       "      <td>2.686797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-6,\n",
    "    max_grad_norm=1,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    logging_dir=\"data/logs\",\n",
    "    output_dir=\"data/ckpt\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=grouped_dataset[\"train\"],\n",
    "    eval_dataset=grouped_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"data/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12c37d-fea1-4d74-98e6-dcd36316d90b",
   "metadata": {},
   "source": [
    "## 3-4. Predict by before fine tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10646ccb-26f1-4b2c-a219-c8e59c0c446e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/asdf/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìš°ë¦¬ëŠ” ëˆ„êµ¬ë‚˜ í¬ë§ì„ ê°€ì§€ê³  ì‚´ì•„ê°ˆ ìˆ˜ ìˆëŠ” ì‚¬íšŒë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤\"ê³  ê°•ì¡°í–ˆë‹¤.\n",
      "ì´ë‚  í–‰ì‚¬ì—ëŠ” ë°•ê·¼í˜œ ëŒ€í†µë ¹, í™©ìš°ì—¬ ìƒˆëˆ„ë¦¬ë‹¹ ëŒ€í‘œ ë“± ì—¬ê¶Œ ì§€ë„ë¶€ì™€ ê¹€ë¬´ì„± ì „ ëŒ€í‘œê°€ ì°¸ì„í•´ ì¶•ì‚¬ë¥¼ í–ˆë‹¤.\n",
      "ê¹€ì˜ì‚¼ ì •ë¶€ ì‹œì ˆì¸ ì§€ë‚œ 2007ë…„ ëŒ€ì„  ë‹¹ì‹œ ì´ëª…ë°• í›„ë³´ì˜ ë‹¹ì„ ì„ ìœ„í•´ 'êµ­ë¯¼í†µí•©21'ì„ ì´ëŒì—ˆë˜ ì´ í›„ë³´ëŠ” \"ìš°ë¦¬ë‚˜ë¼ì—ì„œ ê°€ì¥ í° ë¬¸ì œëŠ” ê²½ì œ\"ë¼ë©° \"ì´ëª…ë°•ì€ ê²½ì œë¥¼ ì‚´ë¦¬ê³  ì„œë¯¼ì„ ìœ„í•œ ì •ì¹˜ë¥¼ í•˜ê² ë‹¤ê³  ì•½ì†í–ˆì§€ë§Œ í˜„ì‹¤ì€ ê·¸ë ‡ì§€ ëª»í–ˆë‹¤\"ë©° ì´ê°™ì´ ë§í–ˆë‹¤.\n",
      "ê·¸ëŠ” ì´ì–´ \"ë‚˜ëŠ” ì§€ê¸ˆ ëŒ€í•œë¯¼êµ­ì„ ê±±ì •í•˜ê³  ìˆë‹¤, ê²½ì œê°€ ì–´ë µë‹¤ë©´ ìš°ë¦¬ ëª¨ë‘ í˜ì„ ëª¨ì•„ ìœ„ê¸°ë¥¼ ê·¹ë³µí•´ì•¼ í•œë‹¤ê³  ìƒê°í•œë‹¤\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "# ë¯¸ì„¸ì¡°ì • ì´ì „\n",
    "origin_name = \"skt/kogpt2-base-v2\"\n",
    "origin_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    origin_name,\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\"\n",
    ")\n",
    "origin_model = GPT2LMHeadModel.from_pretrained(origin_name)\n",
    "\n",
    "inputs1 = origin_tokenizer(\n",
    "    \"ìš°ë¦¬ëŠ” ëˆ„êµ¬ë‚˜ í¬ë§ì„ ê°€ì§€ê³ \",\n",
    "    return_tensors=\"pt\"\n",
    ").to(origin_model.device)\n",
    "outputs1 = origin_model.generate(inputs1.input_ids, max_length=128, repetition_penalty=2.0)\n",
    "result1 = origin_tokenizer.batch_decode(outputs1, skip_special_tokens=True)\n",
    "print(result1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c92fd2-b3f6-430e-8b4e-cf4ad3237740",
   "metadata": {},
   "source": [
    "## 3-5. Predict by after fine tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6946fdb4-07cb-4cdf-bcf6-66be8997677f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìš°ë¦¬ëŠ” ëˆ„êµ¬ë‚˜ í¬ë§ì„ ê°€ì§€ê³  ì‚´ì•„ê°ˆ ìˆ˜ ìˆëŠ” ì‚¬íšŒë¥¼ ë§Œë“¤ìëŠ” ê²ƒì´ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ í¬ë§ì€ ê²°êµ­ ì¢Œì ˆë˜ê³  ë§ì•˜ë‹¤. ì´ ì ˆë§ì ì¸ ìƒí™© ì†ì—ì„œ, ì‚¬ëŒë“¤ì€ ìì‹ ë“¤ì˜ ì‚¶ì„ í¬ê¸°í•˜ê³  ë‹¤ë¥¸ ì‚¬ëŒë“¤ì˜ ì‚¶ìœ¼ë¡œ ëŒì•„ê°€ê³ ì í•˜ì˜€ë‹¤. ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ê·¸ë“¤ì€ ìì‹ ì˜ ì‚¶ì— ëŒ€í•œ ì±…ì„ì„ íšŒí”¼í•˜ê³  ìê¸° ìì‹ ì„ í¬ìƒí•˜ëŠ” ì„ íƒì„ í•˜ê²Œ ë˜ì—ˆë‹¤. ê·¸ë¦¬í•˜ì—¬ ë§ì€ ì‚¬ëŒë“¤ì´ ìì‚´ì„ ì„ íƒí•˜ê²Œ ë˜ì—ˆê³ , ì´ëŠ” ê³§ ìì‚´ë¡œ ì´ì–´ì§€ê²Œ ë˜ì—ˆë‹¤.\n",
      "ì´ëŸ¬í•œ ìƒí™©ì— ëŒ€í•´ì„œ, í˜„ëŒ€ ì‚¬íšŒëŠ” ê°œì¸ì˜ ì¡´ì—„ì„±ì„ ì¡´ì¤‘í•˜ì§€ ì•ŠëŠ” ì‚¬íšŒë¼ê³  ë¹„íŒí•˜ì˜€ë‹¤. ë˜í•œ ê°œì¸ë“¤ì´ ìŠ¤ìŠ¤ë¡œ ëª©ìˆ¨ì„ ëŠëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê¸°ë„ í–ˆë‹¤. í•˜ì§€ë§Œ ê·¸ëŸ¬í•œ ê·¹ë‹¨ì ì¸ í–‰ë™ë“¤ì€ ì˜¤íˆë ¤ ì‚¬ëŒë“¤ì„ ì£½ìŒìœ¼ë¡œ ë‚´ëª°ê²Œ ë§Œë“œëŠ” ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•œ ê°œì¸ì´ ì‚¬ë§í•  ê²½ìš° ê°€ì¡±ë“¤ì˜ ë™ì˜ ì—†ì´ ê°•ì œë¡œ ì£½ìŒì„\n"
     ]
    }
   ],
   "source": [
    "# ë¯¸ì„¸ì¡°ì • ì´í›„\n",
    "finetuned_name = \"data/model\"\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_name)\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_name)\n",
    "\n",
    "inputs2 = finetuned_tokenizer(\n",
    "    \"ìš°ë¦¬ëŠ” ëˆ„êµ¬ë‚˜ í¬ë§ì„ ê°€ì§€ê³ \",\n",
    "    return_tensors=\"pt\").to(finetuned_model.device)\n",
    "outputs2 = finetuned_model.generate(\n",
    "    inputs2.input_ids,\n",
    "    max_length=128,\n",
    "    repetition_penalty=2.0\n",
    ")\n",
    "result2 = finetuned_tokenizer.batch_decode(outputs2, skip_special_tokens=True)\n",
    "print(result2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e8f02-f1fc-462b-b9de-3a040648ecf5",
   "metadata": {},
   "source": [
    "# 4. Encoder-Decoder Conditional Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b5b99-eb5f-4e50-9af7-658d7c34db62",
   "metadata": {},
   "source": [
    "ì–´ë–¤ ë¬¸ì¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ì…ë ¥í•œ ë¬¸ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ì‘ì„±í•˜ëŠ” ì¡°ê±´ë¶€ ìƒì„± íƒœìŠ¤í¬ë¡œ ë¯¸ì„¸ì¡°ì •í•œë‹¤.\n",
    "\n",
    "**ë²ˆì—­ Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a552f9-35ff-4735-8df9-81d87abab214",
   "metadata": {},
   "source": [
    "## 4-1. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2aee3412-081f-40da-b3db-0c16183fae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = 'hyunwoongko/kobart'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d35ec-7989-4e1e-a51e-51feda1d0c74",
   "metadata": {},
   "source": [
    "## 4-2. Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5eb60-cb7a-44e0-95a2-050d654c0b4a",
   "metadata": {},
   "source": [
    "ë²ˆì—­ ëª¨ë¸ì—ì„œëŠ” text_targetì´ë¼ëŠ” ì¸ìë¥¼ ì‚¬ìš©í•˜ê³  í•œêµ­ì–´ë¥¼ ì…ë ¥í–ˆì„ ë•Œ ì˜ì–´ë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ëª©ì ìœ¼ë¡œ `text_target = batch['english']`ë¥¼ ì‚¬ìš©í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0a9b359-a2a2-4785-b1c8-23e0e438fe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919bac73eef44b6793e09bfbaa172763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/166215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687bbe4a8e3b416faf845fcf539866db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1958 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb3267a04854c6a995f491a37cd767b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('msarmi9/korean-english-multitarget-ted-talks-task')\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda batch: (tokenizer(batch['korean'], text_target = batch['english'], max_length=128, truncation = True)\n",
    "                  ),batched = True, batch_size = 1000, num_proc = 2, remove_columns = dataset['train'].column_names)\n",
    "\n",
    "tokenized_dataset['train'].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca1ec5-adc5-468e-b136-67d355f4509c",
   "metadata": {},
   "source": [
    "`input_ids` : ì…ë ¥ ë¬¸ì¥ì´ í† í°í™” ë˜ì–´ì„œ ì‚¬ì „ì— ë§¤í•‘ëœ ìˆ«ìë¡œ ë³€í™˜ëœë‹¤.\n",
    "\n",
    "`attention mask`: ì§„ì§œ ì…ë ¥ê³¼ íŒ¨ë”©ì„ êµ¬ë³„í•˜ë„ë¡ ë„ì™€ì¤€ë‹¤. `input_ids`ì˜ ê¸¸ì´ê°€ ëª¨ë‘ ê°™ì•„ì•¼ ëª¨ë¸ì´ í•œ êº¼ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆê¸°ì— ë¬¸ì¥ì´ ì§§ì€ ê²½ìš°ì—” ë’¤ì— `0` ë˜ëŠ” `[PAD]`ë¥¼ ë„£ì–´ì„œ ê¸¸ì´ë¥¼ ë§ì¶˜ë‹¤. ì´ë•Œ, ì–´ë–¤ ë¶€ë¶„ì´ ì‹¤ì œ ë¬¸ì¥ì´ê³  ì–´ë–¤ ë¶€ë¶„ì´ íŒ¨ë”©ì¸ì§€ ì•Œë ¤ì£¼ëŠ” ê²Œ ë°”ë¡œ **attention mask**ì´ë‹¤.\n",
    "\n",
    "`labels`: ì •ë‹µ í† í° ì‹œí€€ìŠ¤ì´ë‹¤. Seq2Seq ëª¨ë¸ì—ì„œ ëª¨ë¸ì´ ì˜ˆì¸¡í•´ì•¼ í•  ëª©í‘œ ë¬¸ì¥ì„ ì˜ë¯¸í•œë‹¤. labelsë„ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ paddingì„ ë„£ì„ ìˆ˜ ìˆëŠ”ë° padding ë¶€ë¶„ì€ Loss ê³„ì‚°ì—ì„œ ë¬´ì‹œí•´ì•¼í•˜ë¯€ë¡œ ë³´í†µ -100ìœ¼ë¡œ ì±„ìš´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b5605-4bf9-4e8b-9867-3b90dd700d85",
   "metadata": {},
   "source": [
    "labelsê°€ ìˆë‹¤ëŠ” ê±´ ì •ë‹µì´ ìˆë‹¤ëŠ” ì˜ë¯¸!\n",
    "\n",
    "ë²ˆì—­ Taskì—ì„œ ì •ë‹µì€ ë²ˆì—­í•˜ê³ ì í•˜ëŠ” ì–¸ì–´ë¡œ ì“°ì—¬ì§„ ë¬¸ì¥ì´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì˜ì–´ ë¬¸ì¥ì´ë‹¤.\n",
    "\n",
    "ì˜ì–´ ë¬¸ì¥ì€ Datasetì— ì´ë¯¸ í¬í•¨ë˜ì–´ìˆê³  í•™ìŠµì„ ìœ„í•´ Loss ê³„ì‚°ì„ í•  ë•Œ ì‚¬ìš©ëœë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da18ca-4e89-46fb-99cc-2a9ed41f86e4",
   "metadata": {},
   "source": [
    "## 4-3. fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4684ac12-c678-4f29-8415-fc978c8c9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=1,\n",
    "    num_train_epochs=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    logging_dir=\"data/logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    output_dir=\"data/ckpt\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"data/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9250d0-a766-4456-bcbe-4f98108609d1",
   "metadata": {},
   "source": [
    "`-` GPU ì‚¬ìš©í•  ë•Œ í•™ìŠµ ì‹œê°„ì€ 2ì‹œê°„ì´ ì¡°ê¸ˆ ë„˜ê¸°ì— í•™ìŠµì€ ìƒëµí–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90578805-7d75-493d-a2a7-31789b146c42",
   "metadata": {},
   "source": [
    "## 4-4. predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a549d-66a3-4a7f-9cd7-314e6b087a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "model_name = \"data/model\" # ë¯¸ì„¸ì¡°ì •í•œ model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    ")\n",
    "batch = collator([tokenized_dataset[\"test\"][i] for i in range(2)])\n",
    "\n",
    "outputs = model.generate(batch[\"input_ids\"], max_length=128, do_sample=False)\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "origin = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
    "print(f\"ì›ë³¸ : {origin[0]} -> ì˜ì–´ : {result[0]}\")\n",
    "print(f\"ì›ë³¸ : {origin[1]} -> ì˜ì–´ : {result[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
